{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "stop_word_list = []\n",
    "\n",
    "pos_cut_list = []\n",
    "neg_cut_list = []\n",
    "\n",
    "pos_path = './data/pos.txt'\n",
    "neg_path = './data/neg.txt'\n",
    "stop_word = './data/stoplist.txt'\n",
    "\n",
    "with open(stop_word,'r') as fs:\n",
    "    for line in fs.readlines():\n",
    "        stop_word_list.append(line.strip())                               #停用词列表\n",
    "\n",
    "with open(pos_path,'r') as fp:\n",
    "    for line in fp.readlines():\n",
    "        line = re.sub(r'[a-zA-Z0-9]','',line)                             #清楚评论数据中的数字和字母\n",
    "        pos_list.append(line)\n",
    "    \n",
    "pos_list = np.array(pos_list)\n",
    "pos_list = np.unique(pos_list)                                           #评论数据集去重\n",
    "\n",
    "for comment in pos_list:\n",
    "    comment_cut = jieba.cut(comment)                                     #将每一条评论进行分词\n",
    "    comment_cut_str = []\n",
    "    for word in comment_cut:\n",
    "        if word not in stop_word_list:\n",
    "            comment_cut.append(word)\n",
    "#     comment_cut_s = ' '.join(comment_cut_str)\n",
    "#     comment_cut_s = re.sub(r'\\s+',' ',comment_cut_s).strip()\n",
    "    pos_cut_list.append(comment_cut)\n",
    "            \n",
    "print('len(pos_cut_list):',len(pos_cut_list))\n",
    "# print(pos_cut_list)\n",
    "\n",
    "pos_cut_path = './data/pos_cut.pkl'\n",
    "# with open(pos_cut_path,'w') as fpo:\n",
    "#     for comment_cut in pos_cut_list:\n",
    "#         fpo.write(comment_cut)\n",
    "#         fpo.write('\\n')\n",
    "pickle.dump(pos_cut_list,open(pos_cut_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import  FreqDist,ConditionalFreqDist \n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "#把单个词作为特征\n",
    "def bag_of_words(words):\n",
    "    return dict([(word,True) for word in words])\n",
    "\n",
    "#把双个词作为特征--使用卡方统计的方法，选择排名前1000的双词\n",
    "def bigram(words,n=1000):\n",
    "    score_fn = BigramAssocMeasures.chi_sq\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)      #把文本变成双词搭配的形式\n",
    "    bigrams = bigram_finder.nbest(score_fn,n)                    #使用卡方统计的方法，选择排名前1000的双词 \n",
    "    newBigrams = [u+v for (u,v) in bigrams]\n",
    "    return bag_of_words(newBigrams)\n",
    "\n",
    "#把单个词和双个词一起作为特征\n",
    "def bigram_words(words,n=1000):  \n",
    "    score_fn = BigramAssocMeasures.chi_sq\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    bigrams = bigram_finder.nbest(score_fn,n) \n",
    "    newBigrams = [u+v for (u,v) in bigrams]  \n",
    "    a = bag_of_words(words)\n",
    "    b = bag_of_words(newBigrams) \n",
    "    a.update(b)                                                 #把字典b合并到字典a中 \n",
    "    return a                                                    #所有单个词和双个词一起作为特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38596 5414\n"
     ]
    }
   ],
   "source": [
    "#获取信息量较高(前number个)的特征(卡方统计) \n",
    "def jieba_feature(number):     \n",
    "    pos_words = []  \n",
    "    neg_words = [] \n",
    "    for items in pickle.load(open('./data/pos_cut.pkl','rb')):                     #把集合的集合变成集合 \n",
    "        for item in items:\n",
    "            pos_words.append(item)\n",
    "    for items in pickle.load(open('./data/neg_cut.pkl','rb')):\n",
    "        for item in items:\n",
    "            neg_words.append(item)\n",
    "  \n",
    "    word_fd = FreqDist()                                       #可统计所有词的词频\n",
    "  \n",
    "    cond_word_fd = ConditionalFreqDist()                       #可统计积极文本中的词频和消极文本中的词频\n",
    "  \n",
    "    for word in pos_words:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['pos'][word] += 1\n",
    "  \n",
    "    for word in neg_words:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] += 1\n",
    "  \n",
    "    pos_word_count = cond_word_fd['pos'].N()                    #积极词的数量\n",
    "  \n",
    "    neg_word_count = cond_word_fd['neg'].N()                    #消极词的数量\n",
    "  \n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "  \n",
    "    word_scores = {}                                            #包括了每个词和这个词的信息量\n",
    "  \n",
    "    for word, freq in word_fd.items():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word],  (freq, pos_word_count), total_word_count) #计算积极词的卡方统计量，这里也可以计算互信息等其它统计量\n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word],  (freq, neg_word_count), total_word_count) #同理\n",
    "\n",
    "        word_scores[word] = pos_score + neg_score               #一个词的信息量等于积极卡方统计量加上消极卡方统计量\n",
    "  \n",
    "    best_vals = sorted(word_scores.items(), key=lambda item:item[1],  reverse=True)[:number] #把词按信息量倒序排序。number是特征的维度，是可以不断调整直至最优的\n",
    "  \n",
    "    best_words = set([w for w,s in best_vals])\n",
    "  \n",
    "    return dict([(word, True) for word in best_words])\n",
    "  \n",
    "    \n",
    "#构建训练需要的数据格式：\n",
    "  \n",
    "#[[{'买': 'True', '京东': 'True', '物流': 'True', '包装': 'True', '\\n': 'True', '很快': 'True', '不错': 'True', '酒': 'True', '正品': 'True', '感觉': 'True'},  'pos'],\n",
    "  \n",
    "# [{'买': 'True', '\\n':  'True', '葡萄酒': 'True', '活动': 'True', '澳洲': 'True'}, 'pos'],\n",
    "  \n",
    "# [{'\\n': 'True', '价格': 'True'}, 'pos']]\n",
    "  \n",
    "def build_features(dimension = 300):\n",
    "    #四种特征选取方式，越来越好\n",
    "  \n",
    "    #feature = bag_of_words(text())#单个词  \n",
    "    #feature = bigram(text(),score_fn=BigramAssocMeasures.chi_sq,n=dimension)#双个词\n",
    "#     feature =  bigram_words(text(),score_fn=BigramAssocMeasures.chi_sq,n=dimension)#单个词和双个词\n",
    "  \n",
    "    feature = jieba_feature(dimension)                            #结巴分词\n",
    "    posFeatures = []\n",
    "  \n",
    "    for items in pickle.load(open('./data/pos_cut.pkl','rb')):\n",
    "        a = {}\n",
    "        for item in items:\n",
    "            if item in feature.keys() and item != ' ':\n",
    "                a[item]='True'\n",
    "  \n",
    "        posWords = [a,'pos']                                        #为积极文本赋予\"pos\"\n",
    "        posFeatures.append(posWords)\n",
    "  \n",
    "    negFeatures = []\n",
    "  \n",
    "    for items in pickle.load(open('./data/neg_cut.pkl','rb')):\n",
    "        a = {}\n",
    "        for item in items:\n",
    "            if item in feature.keys() and item != ' ':\n",
    "                a[item]='True'\n",
    "  \n",
    "        negWords = [a,'neg']                                       #为消极文本赋予\"neg\"\n",
    "        negFeatures.append(negWords)\n",
    "  \n",
    "    return posFeatures,negFeatures\n",
    "  \n",
    "posFeatures,negFeatures =  build_features()                       #获得训练数据\n",
    "print(len(posFeatures),len(negFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(posFeatures)                                             #把文本的排列随机化  \n",
    "shuffle(negFeatures)                                             #把文本的排列随机化\n",
    "\n",
    "train =  posFeatures[1500:5500]+negFeatures[1500:]                     #训练集6000条 \n",
    "test = posFeatures[:1500]+negFeatures[:1500]                       #预测集(验证集)1500条\n",
    "shuffle(train)\n",
    "shuffle(test)\n",
    "\n",
    "# print(train[:5])\n",
    "# print(test[:5])\n",
    "  \n",
    "data,tag = zip(*test)                                            #分离测试集合的数据和标签，便于验证和测试\n",
    "# print(data[5])\n",
    "def score(classifier):\n",
    "    classifier = SklearnClassifier(classifier)                   #在nltk中使用scikit-learn的接口\n",
    "    classifier.train(train)                                      #训练分类器\n",
    "\n",
    "    pred = classifier.classify_many(data)                        #对测试集的数据进行分类，给出预测的标签\n",
    "    n = 0\n",
    "    s = len(pred)\n",
    "    for i in range(0,s):\n",
    "        if pred[i] == tag[i]:\n",
    "            n = n+1\n",
    "#         else:\n",
    "#             print(data[i],tag[i])\n",
    "  \n",
    "    return n/s                                                  #对比分类预测结果和人工标注的正确结果，给出分类器准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB`s accuracy is 0.755333\n",
      "MultinomiaNB`s accuracy is 0.760000\n",
      "LogisticRegression`s accuracy is  0.772667\n",
      "SVC`s accuracy is 0.681000\n",
      "LinearSVC`s accuracy is 0.768000\n",
      "NuSVC`s accuracy is 0.767333\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from nltk.classify.scikitlearn import  SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC,  NuSVC\n",
    "from sklearn.naive_bayes import  MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score\n",
    "  \n",
    "print('BernoulliNB`s accuracy is %f'  %score(BernoulliNB()))\n",
    "  \n",
    "print('MultinomiaNB`s accuracy is %f'  %score(MultinomialNB()))\n",
    "  \n",
    "print('LogisticRegression`s accuracy is  %f' %score(LogisticRegression()))\n",
    "  \n",
    "print('SVC`s accuracy is %f'  %score(SVC()))\n",
    "  \n",
    "print('LinearSVC`s accuracy is %f'  %score(LinearSVC()))\n",
    "  \n",
    "print('NuSVC`s accuracy is %f'  %score(NuSVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "dimension = 200\n",
      "\n",
      "BernoulliNB`s accuracy is 0.754000\n",
      "MultinomiaNB`s accuracy is 0.751000\n",
      "LogisticRegression`s accuracy is  0.772333\n",
      "SVC`s accuracy is 0.715667\n",
      "LinearSVC`s accuracy is 0.776667\n",
      "NuSVC`s accuracy is 0.769667\n",
      "---------------------\n",
      "dimension = 500\n",
      "\n",
      "BernoulliNB`s accuracy is 0.746333\n",
      "MultinomiaNB`s accuracy is 0.750000\n",
      "LogisticRegression`s accuracy is  0.768000\n",
      "SVC`s accuracy is 0.672000\n",
      "LinearSVC`s accuracy is 0.769333\n",
      "NuSVC`s accuracy is 0.770000\n",
      "---------------------\n",
      "dimension = 1000\n",
      "\n",
      "BernoulliNB`s accuracy is 0.734333\n",
      "MultinomiaNB`s accuracy is 0.743333\n",
      "LogisticRegression`s accuracy is  0.772333\n",
      "SVC`s accuracy is 0.701333\n",
      "LinearSVC`s accuracy is 0.767333\n",
      "NuSVC`s accuracy is 0.778333\n",
      "---------------------\n",
      "dimension = 1500\n",
      "\n",
      "BernoulliNB`s accuracy is 0.741333\n",
      "MultinomiaNB`s accuracy is 0.754667\n",
      "LogisticRegression`s accuracy is  0.771333\n",
      "SVC`s accuracy is 0.686000\n",
      "LinearSVC`s accuracy is 0.767667\n",
      "NuSVC`s accuracy is 0.773000\n",
      "---------------------\n",
      "dimension = 2000\n",
      "\n",
      "BernoulliNB`s accuracy is 0.725667\n",
      "MultinomiaNB`s accuracy is 0.747000\n",
      "LogisticRegression`s accuracy is  0.767000\n",
      "SVC`s accuracy is 0.681000\n",
      "LinearSVC`s accuracy is 0.766667\n",
      "NuSVC`s accuracy is 0.766667\n",
      "---------------------\n",
      "dimension = 2500\n",
      "\n",
      "BernoulliNB`s accuracy is 0.745667\n",
      "MultinomiaNB`s accuracy is 0.760333\n",
      "LogisticRegression`s accuracy is  0.772333\n",
      "SVC`s accuracy is 0.699333\n",
      "LinearSVC`s accuracy is 0.762333\n",
      "NuSVC`s accuracy is 0.773667\n",
      "---------------------\n",
      "dimension = 3000\n",
      "\n",
      "BernoulliNB`s accuracy is 0.718333\n",
      "MultinomiaNB`s accuracy is 0.748333\n",
      "LogisticRegression`s accuracy is  0.769667\n",
      "SVC`s accuracy is 0.699333\n",
      "LinearSVC`s accuracy is 0.765000\n",
      "NuSVC`s accuracy is 0.768333\n"
     ]
    }
   ],
   "source": [
    "#设置不同维度特征，寻找最佳的特征数量和最佳的分类器\n",
    "for dim in [200,500,1000,1500,2000,2500,3000]:\n",
    "    posFeatures,negFeatures =  build_features(dim)\n",
    "    shuffle(posFeatures)  \n",
    "    shuffle(negFeatures) \n",
    "\n",
    "    train =  posFeatures[1500:5500]+negFeatures[1500:] \n",
    "    test = posFeatures[:1500]+negFeatures[:1500]  \n",
    "    shuffle(train)\n",
    "    shuffle(test)\n",
    "\n",
    "    data,tag = zip(*test)  \n",
    "    \n",
    "    print('---------------------\\ndimension = %d\\n' %dim)\n",
    "\n",
    "    print('BernoulliNB`s accuracy is %f'  %score(BernoulliNB()))  \n",
    "    print('MultinomiaNB`s accuracy is %f'  %score(MultinomialNB()))\n",
    "    print('LogisticRegression`s accuracy is  %f' %score(LogisticRegression()))\n",
    "    print('SVC`s accuracy is %f'  %score(SVC()))\n",
    "    print('LinearSVC`s accuracy is %f'  %score(LinearSVC()))\n",
    "    print('NuSVC`s accuracy is %f'  %score(NuSVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#通过调参获得最好的模型参数之后，将其进行存储以便下次直接使用\n",
    "posFeatures,negFeatures =  build_features(1000)\n",
    "shuffle(posFeatures)  \n",
    "shuffle(negFeatures) \n",
    "\n",
    "train =  posFeatures[1500:5500]+negFeatures[1500:] \n",
    "test = posFeatures[:1500]+negFeatures[:1500]  \n",
    "shuffle(train)\n",
    "shuffle(test)\n",
    "\n",
    "data,tag = zip(*test)\n",
    "\n",
    "LR_classifier = SklearnClassifier(LogisticRegression())\n",
    "LR_classifier.train(train)\n",
    "\n",
    "pickle.dump(LR_classifier,open('./model/LR_classifier_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811\n"
     ]
    }
   ],
   "source": [
    "#读取存储的模型对新测试数据集进行测试\n",
    "classifier = pickle.load(open('./model/LR_classifier_model.pkl','rb'))\n",
    "\n",
    "test_set = posFeatures[6000:6500] + negFeatures[2000:2500]\n",
    "data,tag = zip(*test_set)\n",
    "\n",
    "pred = classifier.classify_many(data)\n",
    "print(accuracy_score(tag,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
