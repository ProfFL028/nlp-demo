Xi sends condolences to Trump over Las Vegas shooting
BEIJING - President Xi Jinping on Monday sent a condolence message to his US counterpart, Donald Trump, over the mass shooting in Las Vegas in the US state of Nevada.

The shooting caused heavy casualties, Xi said as he extended deep sympathy to the US government and people, profound condolences to the victims, and sincere solicitude to those wounded.

The Chinese president also wished a quick recovery of the injured.

A gunman opened fire on a country music concert in Las Vegas in the US state of Nevada on Sunday night, killing at least 59 people and injuring more than 500 others in the deadliest mass shooting in modern US history, police said on Monday.

More than 22,000 people were attending the outdoor music festival when the gunman rained bullets from a high-floor hotel room of the Mandalay Bay Hotel and Casino on the Las Vegas Strip, the police said.

The Islamic State group claimed responsibility for the attack, saying the shooter had converted to Islam months ago, Reuters reported. The group provided no evidence.

The death toll, which police emphasized was preliminary, would make the attack the deadliest mass shooting in modern US history, eclipsing last year's massacre of 49 people at an Orlando nightclub.

Thousands of panicked people fled, in some cases trampling one another as law enforcement officers scrambled to locate and stop the gunman. Shocked concertgoers, some with blood on their clothes, wandered the streets.

Police identified the gunman as area resident Stephen Paddock, 64, but said they had no immediate information about his motive.


President Xi meets US secretary of state
BEIJING - President Xi Jinping met with visiting US Secretary of State Rex Tillerson Saturday, discussing US President Donald Trump's state visit to China later this year.

"Currently the most important event in our bilateral relations is President Trump's China visit in November," Xi said. "His visit will be a major opportunity for the development of China-US relations."

Xi said China-US ties have been generally stable, and that he has maintained sound communication with President Trump.

Chinese and US teams should implement consensus reached by the two heads of state, grasp the direction of bilateral relationship, respect each other, and focus on cooperation while dealing with differences properly, Xi said.

"China attaches great importance to President Trump's visit, and I look forward to working with him to outline and advance our bilateral relations in the years to come," Xi said.

He said the teams on both sides should work closely to make the visit "successful and special."

China and the US are respectively the largest developing country and the largest developed one as well as two leading economies of the world, Xi said, stressing both sides need to and can cooperate on the bilateral, regional and global levels.

"The common interests of our two countries far exceed our differences, and cooperation is the only correct choice," Xi said.

The two sides should expand mutually beneficial cooperation and strengthen communication and coordination on major international and regional issues, he said.

Xi called on the two sides to handle their differences and sensitive issues through dialogue and consultation, on the basis of respecting each other's core interests and major concerns, so as to maintain stability of bilateral relations.

The two sides should continue to encourage and expand people-to-people exchanges and strengthen friendship between the people of the two countries, he said.

Tillerson conveyed Trump's greetings to Xi, saying the US president looks forward to his visit to China.

Hailing the progress in US-China relations under the auspices of the two presidents, Tillerson said the United States values its relations with China and hopes to increase mutual trust and practical cooperation in various areas as well as to jointly tackle international and regional challenges.


Stampede on crowded Indian pedestrian bridge leaves 22 dead
MUMBAI, India - A stampede broke out on a crowded pedestrian bridge connecting two railway stations in Mumbai during the Friday morning rush, killing at least 22 people and injuring 32 others, Indian officials said.

Police were investigating what caused the stampede on the bridge, which led some commuters to leap over the railing. Others were crushed or fell underfoot and were trampled.

"There were too many people on the bridge, and the people were in hurry and wanted to move out," said Brijesh Upadhyay, one of the many caught in the crowd. "There was nobody helping, it was very suffocating, and we just wanted to get out of there - and fell on each other."One rescuer told Indian broadcaster NDTV that the stampede trapped dozens in the narrow passage, forcing rescuers to break the railing to pull people out.

Mumbai police official Gansham Patel said some falling concrete had hit part of the bridge railing, leading people to surge forward out of panic at the thought that the bridge was collapsing.

Commuters also often complain about hawkers selling their wares on the narrow overpass, which connects two commuter railway stations, Elphinstone and Parel.

Heavy rains meant the bridge was even more crowded than usual, as some sought shelter from the downpour under the canopy covering the bridge, said lawmaker Shaina Nana Chudasama of the governing Bhartiya Janata Party.

Hospitals were treating 32 injured people, including 19 women and 13 men.

As Mumbai police appealed to citizens to donate blood to help the injured, Prime Minister Narendra Modi expressed his condolences to the families of those killed.

"Prayers with those who are injured," Modi tweeted.

Kishore Thakkar, another witness, said the bridge became overcrowded as people stopped, waiting for the rain to ease. "But then came a heavy push by people, causing some people to fall down and get crushed by the surge of passengers."He complained that government rescuers did not respond quickly to alerts sent by phone. "Local people had pulled out most of the victims by the time the police and government rescuers arrived," Thakkar told TV news channel ABP.

Tabrez Mukadam, a relative of a day worker who died in the stampede, said such accidents happen too often in India.

"These were all common people, laborers , day workers. There has been talk about this bridge for a long time now as it is crowded during non-peak times also. All this time the government ignored it, and today we see this accident.

"Separately in the southern city of Banglaore, two people were killed in another stampede by hundreds of people jostling to obtain coupons for free food offered by a local philanthropist, police said. The philanthropist has been detained for questioning.

Deadly stampedes are fairly common in densely populated India, where many cities are unequipped to deal with large crowds gathering in small areas, with few safety or crowd control measures.

In October 2013, a stampede in Madhya Pradesh state in central India killed more than 110 people, mostly women and children.


Abe joins gala by Chinese embassy
Abe told a full house of nearly 2,000 people that he will work hard to make the trilateral summit meeting for leaders of China, Japan and the Republic of Korea possible in Japan this year to improve his country's relations with Beijing.

Thursday's event also was for observing the 45th anniversary of the normalization of diplomatic relations between China and Japan on Friday.

Abe, the first Japanese prime minister to attend such a ceremony in 15 years, was accompanied by other Japanese officials and politicians, such as Foreign Minister Taro Kono and the ruling Liberal Democratic Party's secretary-general, Toshihiro Nikai.

Chinese Ambassador to Japan Cheng Yonghua said China attaches great importance to its relationship with Japan, persistently appealing to push the ties forward on the basis of four political documents and the consensus on improving the ties.

The four documents include the China-Japan Joint Statement signed in 1972, the China-Japan Treaty of Peace and Friendship of 1978, the China-Japan Joint Declaration of 1998 and a joint statement on advancing strategic and mutually beneficial relations in a comprehensive way that was signed in 2008.

He said the two countries need to carefully maintain the political foundation for their relations. The China-Japan relationship is improving, though many complicated and sensitive issues remain, he said.

The two countries should take concrete actions to implement the consensus that they are partners that do not pose a threat to each other. They need to try to build political and strategic trust.

Japanese Education Minister Yoshimasa Hayashi said he took Abe's presence at the event as a positive signal for bilateral ties.

Gao Hong, a Japan expert at the Chinese Academy of Social Sciences, agreed that Abe's willingness to participate sent a good message. "As Abe dissolved the Lower House of Japan's parliament on Thursday to call a snap election in October, he wants to build good relations with Japan's neighbors," the scholar said. "Abe may use the positive diplomatic approach to help his election campaign."

In May, the Abe administration said Japan is willing to cooperate with China on President Xi Jinping's Belt and Road Initiative and other business projects, which China has welcomed, Gao said.

Michio Ito, general manager of the China Business Office of Takenaka Corp, was glad that the Japanese prime minister came to the Chinese embassy's celebration, which he believed would help improve ties.



DPRK joint ventures ordered to shut down
China has ordered companies and joint ventures from the Democratic People's Republic of Korea to shut down within 120 days as it applies UN sanctions imposed following Pyongyang's sixth nuclear test, the Ministry of Commerce and the State Administration for Industry and Commerce said on Thursday.

Overseas Chinese joint ventures launched with DPRK firms or individuals should also be closed according to the sanctions from the United Nations, which did not give a deadline in a joint statement on its website.

The UN Security Council unanimously adopted a resolution on Sept 12 imposing new sanctions on Pyongyang, including limiting its oil imports, banning its textile exports and restricting overseas firms from hiring DPRK citizens.

Shi Yongming, a researcher at the China Institute of International Relations, said what China is doing is just implementing the UN resolution and Pyongyang should be ready for it.

"Sanctions are by no means the ultimate goal," he said.

The implementation of the sanctions is to make Pyongyang understand that the goal is always to bring the Korean Peninsula nuclear issue back to the negotiation table, Shi added.

Lu Kang, the Foreign Ministry spokesman, said at a daily news briefing on Thursday that "the Korean Peninsula nuclear issue is related to regional peace and stability as well as vital interests of all parties concerned. Breaking the deadlock requires all relevant parties to show their sincerity."

He said that both sanctions and talks are requirements of UN Security Council resolutions, and that it is not in accord with the spirit of UN resolutions to uphold one at the expense of the other.


Two giant pandas begin journey to Indonesia
Workers at the China Conservation and Research Center for the Giant Panda in Wolong, Southwest China's Sichuan province, help a panda named Huchun begin her journey to a zoo in Indonesia on Sept 27, 2017. She will be joined by another panda, Caitao. The two seven-year-old pandas will be flown from Chengdu to Jakarta by Indonesian state airline Garuda Indonesia at 4:10 am Thursday. The pandas were born at the Ya'an Bifengxia base, part of the China Conservation and Research Center for the Giant Panda in Sichuan. Caitao and Huchun will live in a 4,800-square-meter panda park at Taman Safari Indonesia for 10 years. [Photo provided to chinadaily.com.cn]


Thousands of commuters stranded in Sri Lanka due to train strike


COLOMBO - Thousands of commuters were left stranded in Sri Lanka on Thursday as railway unions launched an indefinite strike urging the government to fulfill their demands.

The unexpected strike was launched late on Wednesday evening, at a peak rush hour, leading to the cancellation of all trains throughout the island.

A tense situation erupted at the Fort Railway Station in capital Colombo on Wednesday when passengers were informed that trains had been cancelled.

The angry passengers launched a protest of their own against the railway unions, resulting in special police teams being deployed to calm the situation.

The Railway Unions launched the strike alleging that a group of junior train drivers, who were still under training, had been appointed by breaching the normal recruitment procedures.

The unions urged the government to look into the matter and revise the recruitment procedure of trainee.

Thousands of commuters who arrived at the railway stations on Thursday morning were left stranded.

Groups were seen get onto special buses. Special luxury buses had also been parked outside the stations to transport passengers to the north and south.

The strike caused severe traffic jams in Colombo and its outskirts as additional buses were deployed by the Transport Ministry.



Volcano erupts in Indonesia's North Sumatra


JAKARTA - Mount Sinabung volcano in Karo district, Indonesia's North Sumatra province erupted earlier Thursday, spewing a column of ash two km to the sky, a disaster agency senior official said.

The eruption took place at 02:45 am Jakarta time followed by tremors with ash sliding two km to the east and southeast, and 1.5 km to the south of the crater, said spokesman for the national disaster management Sutopo Purwo Nugroho.

"But the eruption did not leave casualty and trigger fresh evacuation," he told Xinhua in a text message.

On Wednesday, the volcano also erupted, spreading ash by up to 1.5 km high, said Sutopo.

Mount Sinabung has been on top alert since July 2, 2015 with no-go zone of seven km in the south, southeast and east of the crater, according to him.

Mount Sinabung is one of Indonesia's 129 active volcanoes.



12 confirmed dead after Rohingya boat capsizes off Bangladesh


DHAKA - Twelve bodies were recovered after a boat with scores of Rohingyas on board capsized at the mouth of the Naf river that divides Myanmar and Bangladesh, a police official said Monday.

"Bodies of 10 children, one man and one woman have so far been recovered," said Md Main Uddin, officer-in-charge of Teknaf police station under Cox's Bazar district, some 292 km southeast of capital Dhaka.

He could not tell immediately when the boat capsized but assumed that it may have capsized sometime late Sunday night.

Law enforcers Monday morning recovered the bodies which were floating in the Bay of Bengal near Shah Porir Island of Bangladesh's Cox's Bazar district which is now home to nearly 1 million Rohingya refugees from Myanmar, he added.

According to the official, there is no information about how many people from the sunken boat managed to swim ashore after it capsized.

The official feared the death toll will rise, as some of the 11 rescued Rohingyas claimed some people are still missing.

He said a search operation is underway.

The bodies of 132 Rohingyas and a Bangladeshi boat man were recovered from the Naf River since Aug. 29 in at least 25 boat capsizal incidents.

Over half a million Rohingya people have fled to Bangladesh from Myanmar's Rakhine state amid a fresh wave of violence in the region since Aug. 25.


6.0-magnitude quake strikes off coast of Japan's Fukushima Pref., no tsunami warning issued
TOKYO -- An earthquake with a preliminary magnitude of 6.0 struck off the coast of Fukushima Prefecture in Japan's northeast on Friday, the weather agency here said.

According to the Japan Meteorological Agency (JMA), the temblor, which struck at 5:00 p.m. local time, registered 2 on the Japanese seismic scale which peaks at 7.

No tsunami warning has been issued as a result of the quake, the JMA said.

The quake's epicenter was located at a latitude of 37.5 degrees north and a longitude of 144.0 degrees east, with a depth of 6.0 km, according to the weather agency.

The quake registered between 1 and 2 on Japan's seismic scale across wide swathes of Japan's north and northeasterly regions, including the prefectures of Fukushima, Hokkaido, Iwate, Miyagi, Akita, and Yamagata among others.

Local authorities have not reported any damage or injuries to persons as a result of the temblor and Japan's nuclear watchdog has not reported any irregularities at nuclear power stations within the vicinity of the quake's reach. 



NYC Empire State Building to shine for Chinese Mid-Autumn Festival


NEW YORK - Tower lights of New York City's landmark Empire State Building will shine in red and gold at sunset Wednesday in honor of Chinese traditional Mid-Autumn Festival, which falls Oct 4 this year.

Tonight, the Empire State Building's "tower light will shine in red and gold, with colorful moons and lanterns in the mast to celebrate this joyous holiday," said Anthony Malkin, CEO and Chairman of Empire State Realty Trust.

Lighting scheme is designed by world-renowned lighting designer Marc Brickman.

Before flipping the switch for the lighting, Zhang Qiyue, consul general of China in New York, said China-US relationship has maintained a generally positive momentum since the beginning of this year.

"The two sides have since this year set up four dialogue mechanisms, one of them called social, cultural, and people-to-people dialogue," said Zhang.

"I think this lighting ceremony, or event like this are part of this social, cultural dialogue and events," the Chinese diplomat added.

For years, this New York landmark has been celebrating Chinese Lunar New Year in lights. Zhang said this year's Mid-Autumn Festival event marks a new initiative.

"We hope this tradition could be kept," she said.

The Mid-Autumn Festival is one of key traditional festivals in China which falls on the 15th day of the eighth lunar month. It is the time when the moon is said to be at its brightest and fullest.

In western calendar, the day usually occurs sometime between the third week of September and the second week of October.

This day is also considered a harvest festival, a time for relaxation and family reunion.

Since 1976, the Empire State Building's tower lights have maintained a tradition of changing color to recognize various occasions and organizations throughout the year.


Experts: Shooting shows urgent need for gun control


On Sunday, a gunman in a hotel overlooking the Las Vegas Strip opened fire on the crowd at a country music festival, killing at least 59 and injuring at least another 527, making it the deadliest mass shooting incident in US history. Three experts shared their views on the incident with China Daily's Zhang Zhouxiang and Wang Han:

Deep split in US society curbs gun control

Da Wei, director of the International Strategy and Security Research Center, University of International Relations

Discussions over stricter gun control have been onging in the United States for a long time, but there is hardly any progress; That's because it is increasingly more difficult for different groups in the US to reach consensus.

Decades ago, different groups with different interests in the US were discussing and debating with each other in constructive ways, but today they are quarrelling with each other. The result is endless mutual attacks without any attempts to find a solution to the problem.

Especially, the issue of gun control involves conflicts between elites and the ordinary people, between the urban and rural residents, even among different states. With more than 50 killed and more than 500 injured, the price is rather heavy and the US must find ways to mend its disintegrating society.

Lack of serious debate In US about gun control

Gong Honglie, an associate professor of international studies at Nanjing University

At a news conference, the US police said they did not find any clues linking the shooter and any terrorist organization. There are also suggestions that the suspect had mental problems. Therefore, it is highly possible that the Las Vegas shooting, the most deadly of its kind, is not a terrorist attack or an attack with a political background, but rather an attack launched by someone extremely disturbed who hated society.

Yet for the public, there is no difference between one kind of attack and another. They only care about how to stop such attacks and ensure their safety. That's a common challenge for all modern societies, especially the United States where gun control is rather loose and there are so many organizations and individuals that are out of control.

Concerning gun control, US politicians always rush to discuss it only after a major attack happens. When the attacks are no longer in the media spotlight, they drop the issue again. We hope they will change this time, or it will be too late.

Europe does better job at gun control

Zhu Sumei, a professor at the International Politics Department of the University of International Relations

After the tragedy in Las Vegas, US President Donald Trump expressed his "warmest condolences and sympathies" to the victims and their families. However, in order to prevent such tragedies from happening again, Trump needs to push through gun control. Do not forget that the police had found 17 firearms in the suite of the suspect, an astonishingly high number.

For historical and practical reasons, guns are a very serious problem in the US. As the chief supporter of the Second Amendment to the US Constitution, which grants US residents the right to bear arms, the National Rifle Association of America has about 5 million members nationwide, among which there are some quite powerful figures that are mostly Republicans. This group is a big obstacle to gun control in the US.

Besides that, many Democrats avoid promoting gun control because they do not want to offend the NRA. That's why former US president Barack Obama tried hard to push it, but the legislation on gun control still failed to get the approval of the US Congress. In 2015, Obama even openly admitted being depressed about this situation.

In gun control, Europe does a much better job than the US. It is so strict in Europe that terrorists even have few channels to get hold of them, so they have resorted to trucks and knives for their attacks. If the terrorists in Europe had guns, they would have caused much heavier casualties. It is time for US society to face this challenge.


Trump calls Las Vegas shooter 'sick' and 'demented man'
US President Donald Trump on Tuesday called the gunman who killed 59 people and wounded hundreds others at a music festival in Las Vegas a "very, very sick individual".

Trump spoke to reporters as he departed for a trip to hurricane-ravaged Puerto Rico. He called the gunman "demented" and said "we're looking into him very seriously", the Associated Press reported on Tuesday.

Trump also praised Las Vegas police, saying they had done an "incredible job".

Asked about gun laws, the president said, "We'll be talking about gun laws as time goes by," the AP reported.

Trump has labeled the Sunday night shooting at an outdoor music festival an "act of pure evil" and declared the nation would unite behind the survivors.



Putin advocates 'civilized' solution to Korean Peninsula impasse


SOCHI, Russia - Russia has always favored a civilized way to settle all disputes, including the Korean Peninsula nuclear issue, said President Vladimir Putin on Thursday.

 
The Democratic People's Republic of Korea (DPRK) should not be cornered or intimidated, he said at a meeting of Russian thinktank Valdai Discussion Club here, although Russia condemns Pyongyang's nuclear tests and adheres to UN sanctions.

 
Whether someone likes or dislikes DPRK, it is a sovereign country, Putin noted, according to an official transcript of his speech.

 
"We are firmly convinced that even the most complex knots - be it the crisis in Syria or Libya, the Korean Peninsula or Ukraine - must be disentangled rather than cut," he said.


Deal or No Deal, advice from all corners directed at May at crucial EU summit
LONDON - Co-founder and former leader of the pro-Brexit United Kingdom Independence Party (UKIP), Nigel Farage, accused Britain's main opposition leader Jeremy Corbyn of playing "divide and rule" by visiting Brussels Thursday.

The Labour politician was in the Belgian capital where he spent an hour talking with Michel Barnier, the chief negotiator tasked with brokering a European Union (EU) divorce deal with Britain.

The leaders of the bloc's 28 member states gathered in Brussels for a two-day summit, with British Prime Minister Theresa May among them to press home her bid for a good Brexit deal.

In London, some political commentators accused Corbyn's team of virtually gatecrashing the summit to spell out that the Labour party was against a "no deal" scenario, and waiting in the wings to take over the talks.

The Daily Telegraph said in a commentary: "If the timing of Corbyn's visit is designed as a deliberate snub to May, as a warning to her that an alternative negotiating team waits in the wings should she fail, then she should not allow such discourtesies to throw her off course."

The London-based Leave Means Leave campaign said a meeting between Corbyn and EU leaders before their audience with the prime minister was another calculated snub to Theresa May.

In a round of media interviews, Corbyn said Britain failing to reach a trade agreement with the EU would be "catastrophic" for British jobs.

He said in an interview with Sky News: "We cannot countenance the idea that we just rush headlong into no deal with Europe. No deal with Europe would be very dangerous for employment and jobs in Britain. The idea of no deal would mean that the World Trade Organisation rules would be implemented straight away in March 2019."

The newly-elected UKIP leader Henry Bolton accused the European Commission of having no intention of conducting reasonable negotiations with the British prime minister.

In London, meanwhile, there was support for the prospect of a "no deal" with a call to May from the Leave Means Leave campaign to walk away from the talks and quit Europe next March with no deal if trade talks stall.

"In the event of no progress at the European Council, the UK should formally declare that it is assuming that we will be subject to World Trade Organisation (WTO) rules from March 30, 2019," said the letter to May.

Senior figures from politics, including four former cabinet members, business, economics, law, science and the military have called in a letter for decisive action to dispel the highly damaging levels of uncertainty facing businesses across the country.

Foreign Secretary Boris Johnson said in a media briefing Thursday that Britain would do "very well", even if it has to leave the EU with no trade deal.

Speaking in London after a meeting with Mexican Foreign Minister Luis Videgaray Caso, Johnson predicted Britain would get a "great deal" in its Brexit negotiations.

But he added: "As with any negotiation you've got to be prepared to walk away. We have to prepare for every eventuality... I think we'll do fine."


US, EU at odds over uncertain Iran deal


WASHINGTON - European Union leaders on Thursday said they'd re-affirm the Iran nuke deal, despite US President Donald Trump's refusal to recertify the accord. But if Washington pulls out, Iran said it would shred the deal. That leaves the deal in limbo, experts told Xinhua.

According to European press reports on Thursday, EU leaders will re-affirm that they are committed to the international accord of world powers.

That sits in sharp contrast to Trump's stance on the international agreement. Earlier this month, Trump failed to certify that Iran was playing by the rules stipulated in the international accord on Iran's nuclear program, and contended that the Islamic republic is in defiance of the agreement.

Trump has given Congress 60 days to decide whether to reinstate the sanctions on Iran that were lifted under the 2015 accord.

Trump's move neither scraps the deal not saves it, and there's a chance the international accord may remain intact. But the deal's future remains uncertain, and some believe it risks falling apart.

"The nuclear agreement is in danger of collapsing," Jim Phillips, senior Middle East research fellow at the Heritage Foundation, told Xinhua

"The Obama Administration promised too much and delivered too little," he said of the previous administration, which put together the deal.

"Then the Democrats in the Senate blocked debate on the merits of the deal and prevented a resolution of disapproval from being voted on, despite the fact that 58 of the 100 senators were opposed to it," he said.

Phillips' outlook for the deal's survival is dim.


EU states to pump 'sufficient' money to stem illegal migration: Tusk
BRUSSELS - Member states of the European Union (EU) will pump "sufficient" money into a trust fund to stem illegal migration from North Africa, European Council President Donald Tusk said here on Thursday.

He made the remarks at a joint press conference with his European Commission counterpart Jean-Claude Juncker, following a first day meeting of the two-day EU summit, which focused on migration, digital Europe, as well as security and defense.

He said EU leaders have agreed on the need to help Italy manage the Central Mediterranean route, which links Libya to Italy and was named as the deadliest route to Europe by the International Organization for Migration (IOM) in a study published in September 2017.

"We have a real chance of closing the Central Mediterranean route. That is why we decided that member states will provide sufficient finances for the North Africa window of the Trust Fund for Africa, while the Commission ensures that this money is channeled to stem illegal migration," Tusk said.

"We should see concrete results within the next few weeks," he added.

As Libya is at the forefront of EU's efforts to stem illegal migration in the central Mediterranean route, EU leaders reiterated "the importance of working with the Libyan authorities and all neighbors of Libya to enhance border management capacity," according to the EU summit.

EU leaders also underscored the urgency of supporting the development of the local communities in Libya along the migratory routes."

To consolidate the declining trend of migrant inflow, the European Council also called on all member states to fully abide by the deal with Turkey.

Since the summer of 2015, an unprecedented refugee crisis has been a tough nut to crack for the EU.

Thanks to an "aid for return" deal with Turkey in March 2016, the EU boxed in the inflow of refugees via the eastern Mediterranean route, which saw a 98 percent plunge of arrivals.

However, the EU still bears the brunt of migratory pressure, particularly from the central Mediterranean route.

The IOM, the United Nations Migration Agency, reported Tuesday that 145,355 migrants and refugees had entered Europe by sea up to Oct. 15 this year, with over 75 percent arriving in Italy and the rest landing in Greece, Cyprus, and Spain.

Meanwhile, 2,776 people have died trying to cross the Mediterranean this year, according to the IOM.

A total of 387,895 migrants and refugees reached Europe in 2016, with a record high fatality of 5,143 in the Mediterranean Sea.



Spain's cabinet to meet Saturday to discuss suspending Catalan autonomy


MADRID -- Spain's cabinet will meet on Saturday to discuss suspending the Catalonia region's autonomy and imposing central rule in response to Catalan leader Carles Puigdemont's threat of secession unless talks were held on the issue.

About 2.26 million people took part in a referendum on Catalan independence on Oct. 1 despite a government ban on it, with 2.02 million casting "yes" votes.

Subsequently, Prime Minister Mariano Rajoy last week gave Puigdemont a Thursday 10 a.m. (0800 GMT) deadline, asking for a clear response on whether he had declared Catalan independence on Oct. 10.

Puigdemont's reaction was to write to Rajoy Thursday, proposing dialogue. The letter said, "Despite all of these efforts and our will for dialogue, the fact the only reply is the suspension of our autonomy implies that (the government) is not aware of the problem and does not want to talk."

"If the central government persists in impeding dialogue and continuing its repression, Catalonia's parliament will, f it considers it appropriate, vote on a formal declaration of independence," he said.

The wealthy Catalonia region, with a population of 7.5 million, accounts for about a fifth of Spain's economic output. Residents mostly speak Catalan, a language that some believe is a Spanish dialect but some argue to be a totally different language.


Finland trip warmed business, sports links
President says China has important role in balancing global development

Sauli Niinisto, the president of Finland, has wished President Xi Jinping ahead of the 19th CPC National Congress "all the strength and courage to go further" in his mission to rejuvenate China.

Finland trip warmed business, sports links

The Finnish people are closely following discussions leading up to the congress, which opens on Wednesday, and hope China will continue on its path of economic development, Niinisto said in an exclusive interview.

He said he is confident the two countries will enhance cooperation in the next five years, both politically and economically, and predicted that "China will play a very important role in balancing global development, which is much needed".

President Xi, who is also general secretary the Communist Party of China Central Committee, made a three-day state visit to Finland in early April. It was the first by a Chinese head of state in 22 years.

Niinisto described the visit as historic, adding that he was touched by the fact it came as Finland celebrated 100 years of independence.

"It's very valuable for a small country, and the leader of a small country, to hear what the big ones are thinking," said the Finnish president, who remembers Xi as being "a gentleman, and a very friendly personality".

During the visit, Finland and China signed an agreement on a future strategic partnership, which Niinisto said will open many new doors. "It seems that trade is enhanced, for example, and surely that also strengthens our relations," he added.

With Beijing to co-host the 2022 Winter Olympics with Zhangjiakou, Hebei province, another key area for cooperation is winter sports.

Xi's visit was "a good starting point for potential Finnish-China Olympic cooperation, laying the foundation to start building the required relationships and contacts", said Mikko Salonen, secretary-general of Finland's Olympic committee.

Dialogue and exchanges are already taking place between governments, businesses and sports clubs, he said, adding that a delegation from the Cross-Country Skiing Federation of Finland is planning to soon visit the Winter Games venues in Beijing and Zhangjiakou.

"I also know companies involved in making artificial snow have been in contact with their Chinese counterparts, and there have been lots of discussions between educational parties and organizations," Salonen said.

Chinese businesses also took the opportunity during Xi's visit to expand cooperation with Finnish companies in renewable energy, as Beijing works to boost green development.

China CAMC Engineering Co, an arm of the State-owned Sinomach, signed a contract with Finland's Boreal Bioref that will facilitate an investment of 800 million euros ($942.8 million) in a new bio-refinery in Kemijarvi.

"President Xi's visit has had a strong impact on the speed at which we have been able to proceed with the project," said Heikki Nivala, CEO and chairman of Boreal Bioref.

"Renewable energy is at the top of the agenda for Finland and China. The potential scale offered by the Chinese market creates a natural breeding ground for Finnish innovation in this field."

China also agreed to loan Finland two giant pandas during Xi's state visit. The endangered animals are expected to arrive this year and will remain at Ahtari Zoo for 15 years.

Juhani Haapaniemi, the zoo's CEO, said Finns nationwide are excited about the arrival of the pandas. Students at local schools have planted bamboo, while others have made drawings and videos to greet the pandas.

Zhang Yangfei contributed to this story. 


Kurdish-led fighters to continue battle against foreign IS militants in Syria's Raqqa
 DAMASCUS -- The Kurdish-led Syrian Democratic Forces (SDF) will continue to fight against the Islamic State (IS) militants who refused to surrender in Syria's northern city of Raqqa, a Kurdish spokeswoman was cited by Kurdish activists as saying Monday.

Jihan Sheikh Ahmad was cited as saying that the SDF, which is led by the Kurdish YPG and heavily backed by the US-led coalition, launched a day earlier the last stage of the battle in Raqqa, which is to eradicate the foreign IS militants, who refused to surrender, unlike the local militants who surrendered themselves to the SDF.

This comes a day after a deal was reached between the SDF-led Raqqa Civil Council and tribesman in Raqqa for the evacuation of civilians and local IS militants from Raqqa.

Kurdish activists said a total of 275 IS fighters surrendered over the past 24 hours to the SDF, while as many as 3,500 civilians have evacuated Raqqa over the last week.

Now, an estimated number of 300 foreign IS militants and few members of their families are still in Raqqa, with the SDF pushing over to eradicate them before declaring the city IS-free.

Jihan, the spokeswoman, said the SDF fighters are continuing to advance in Raqqa, mopping up the city from six directions. She added that the SDF fighters are expected to capture the neighborhoods of Andalus and Matar in the city.

Meanwhile, the US-led coalition said the aerial bombardment of Raqqa will continue on the areas that are still controlled by IS in the city.

IS militants declared Raqqa as their capital in 2014, after announcing their self-styled caliphate.

Tens of civilians have died during the intense battles as well as in the airstrikes of the US-led anti-terror coalition since the operation against IS in Raqqa started four months ago.

On Sept. 7, the Syrian Observatory for Human Rights said as many as 978 civilians had been killed in three months by the US-led airstrike and the shelling on Raqqa.



UAE official says Qatar giving up World Cup may end 'crisis'
DUBAI, United Arab Emirates - A top Emirati security official has said the only way for "Qatar's crisis" to end is if Doha gave up hosting the 2022 FIFA World Cup, his comments coming amid the ongoing diplomatic dispute between the energy-rich nation and four Arab countries.

Dubai security Lt. Gen. Dhahi Khalfan, known for being outspoken on Twitter, later wrote Monday his "personal analysis" of what he described as the financial pressure Doha faces in hosting the games had been misunderstood.

But his remarks came as lobbying firms backed by the four nations opposing Qatar in the diplomatic dispute increasingly target the upcoming soccer competition in their criticism.

The tournament has not come up in the demands previously made by the boycotting countries, though losing the World Cup would represent a bitter defeat for the tiny peninsular nation that's pushed itself onto the world stage with its bid.

Qatari officials did not respond to requests for comment on Monday. However, the 2022 tournament's head in Qatar said on Friday the boycott poses "no risk" to the competition being held.

Bahrain, Egypt, Saudi Arabia and the United Arab Emirates all cut diplomatic ties and began a boycott of Qatar on June 5, in part over allegations that Doha supports extremists and has overly warm ties to Iran.

Qatar has long denied funding extremists and restored full diplomatic ties to Iran amid the dispute. Doha shares a massive offshore natural gas field with Iran that makes its citizens incredibly wealthy.

On Sunday night, Khalfan targeted the FIFA tournament in his tweets.

"If the World Cup leaves Qatar, Qatar's crisis will be over ... because the crisis is created to get away from it," he wrote.

By Monday night, Khalfan returned to Twitter to write that his tweets were his "personal analysis.""I said Qatar is faking a crisis and claims it's besieged so it could get away from the burdens of building expensive sports facilities for the World Cup," he tweeted.

"That's why Qatar isn't ready and can't host the next World Cup," he added.

As the crisis has dragged on despite mediation by Kuwait, the United States and European nations, Qatar's opponents have begun targeting its hosting of the FIFA cup. They've pointed to allegations of corruption surrounding Qatar's winning bid, as well as the conditions that laborers working in Qatar face in building infrastructure for the games.

While FIFA ethics investigators found that the Qataris used a full range of lavishly funded state and sports agencies to win the 2010 vote to host the tournament, authorities concluded there was no "evidence of any improper activity by the bid team.

"When Qatar's sole land border with Saudi Arabia was closed and sea traffic cut off by the boycott, World Cup organizers were forced to instigate a "Plan B," including bringing in supplies from Turkey.

Asked about Khalfan's comments, FIFA said Monday: "We do not comment on speculation." Hassan al-Thawadi, Qatar World Cup supreme committee secretary-general, said on Friday that the project remained on time despite that.


Iran jails nuclear negotiator on spying charges


TEHRAN - Iran's judiciary spokesman on Sunday confirmed five-year jail sentence for an Iranian nuclear negotiator, Tasnim news agency reported.

A member of Iran's nuclear negotiating team arrested recently by the country's intelligence forces has been sentenced to five years in jail on espionage charges, Iranian Judiciary's Spokesman Gholam Hossein Mohseni Ejei was quoted as saying.

"The conviction of (Abdol-Rasool) Dorri Najaf Abadi on charges of spying for foreign intelligence services and leaking information have been upheld," Ejei said speaking at a press conference in Tehran.

Back in May, the judiciary spokesman had announced that Dorri Najaf Abadi had been sentenced to a prison term, but added that he could not provide details since the verdict could be appealed.

He added that Dorri Najaf Abadi holds a dual citizenship and is convicted on charges of spying for two foreign intelligence services, without further elaboration.

The final verdict was upheld after his case was reviewed by an appeals court, Ejei added.

The news of arrest of the former member of the negotiating team was first announced by the judiciary spokesperson in August 2016.


Palestinian gunman kills three Israeli security officers in West Bank


JERUSALEM - A Palestinian gunman killed three Israeli security officers and wounded another outside a West Bank settlement on Tuesday morning before he was shot dead, Israeli police said.

The shootout took place outside Har Adar, an upscale Jewish settlement northwest of Jerusalem, near the seam zone between Israel and the Israeli-occupied West Bank.

Police spokeswoman Luba Samri identified the assailant as a 37-year-old man from Beit Surik, a Palestinian village in the Jerusalem district that was rarely involved in a violent protest against Israel. The father of four children held an Israeli work permit, which allowed him to enter Israeli settlements.

At around 7:00 am local time (0400 GMT), he arrived with a group of Palestinian workers at the back gate of the fenced settlement, from which Palestinian laborers cross into Israel to do their daily work.

He raised the suspicion of border police officers who were guarding the settlement and they called him to stop, Samri said.

When the assailant realized that he was exposed, he pulled out a gun hidden under his shirt and opened fire towards the officers, killing three and wounding another, said Samri.

After the gag-order was lifted, it is revealed that the three killed were two security personnel from a private security company and a border policeman.

Another border police officer was wounded and taken to the Hadassah Ein Kerem hospital in Jerusalem with serious injuries, said a spokesperson of the hospital.

Other border police officers opened fire at the shooter, who died of his wounds at the scene.

The assailant planned to carry out a larger attack against civilians at the settlement. "The quick action by the security forces stopped him from entering into Har Adar," Police chief Roni Alsheikh told journalists near the site.

Commenting on the attack, Israeli President Reuven Rivlin said the "brutal terror attack once again shows the daily frontline on which our security forces stand."

Israel "will continue to confront terror, and will reach all its perpetrators and supporters," the president said in a statement.

Since September 2015, violent clashes between Palestinians and Israelis have claimed the lives of at least 293 Palestinians, 48 Jewish Israelis, two US nationals, a British tourist and two African asylum seekers.

Israel accuses the Palestinian National Authority of "inciting" the unrest. Palestinians say it is the result of 50 years of Israeli occupation of the West Bank and the Gaza Strip, home to more than 5 million Palestinians.



Kurdish parliament approves independence referendum on Sept 25


ERBIL, Iraq - The parliament of the semi-autonomous region of Kurdistan on Friday approved the independence referendum to be held on Sept 25.

All the 65 lawmakers attending the session of the 111-seat regional parliament voted in favor of holding the referendum on the independence of the Kurdish region and the disputed areas.

The parliament also decided to invite relevant parties, including Kurdistan's Independent High Elections and Referendum Commission, to conduct the referendum.

According to Tariq Jowahar, parliament media advisor, the session was chaired by the deputy speaker, as Speaker Yousif Mohammed Sadiq did not attend the session since his Gorran Movement is boycotting the session along with the Kurdistan Islamic Group.

Kaka Bashar, a Kurdish lawmaker from the Patriotic Union of Kurdistan (PUK), said he regretted that some of the lawmakers boycotted the parliament session, particularly the Gorran Movement, and hoped they will take part in the coming session.

Bahar Mahmoud, head of Gorran's parliamentary bloc, said the parliamentary session is "illegal."

"The session is illegal and a violation to the internal parliament law, as the session must be approved by the parliament's presidency which comprises the speaker and his deputy, in addition to the secretary of the parliament," she told Xinhua.

Mahmoud also revealed that Sadiq was prevented from entering the regional capital city of Erbil.

"This session was held according to the wishes of the two main parties and not according to the desire of parliament," she said, referring to the Kurdistan Democratic Party (KDP) headed by the regional President Masoud Barzani, and the PUK with Iraqi President Fuad Masoum as one of its leading figures.

Horman Hama Sharif, a spokesman for the Kurdistan Islamic Group, echoed Mahmoud's opinion in a statement.

"We do not participate in this session because we consider it illegal, because the legitimate speaker is not aware of the session, as well as the parliament secretary, and this is an exploitation of the parliament as an institution for the parties and personal purposes," he said.

"We believe that the session will not solve our political and economic problems, but it could be the reason for further crises," Sharif concluded.

Fala Farid, head of the parliamentary legal committee, maintained that the parliamentary session is legal.

"The session is legal and was attended by the majority of the lawmakers, who expressed their opinions in supporting the referendum," he told Xinhua.

In addition to the Gorran Movement, which has 24 lawmakers out of the 111-seat parliament, and the Kurdistan Islamic Group with six seats, the Turkoman Front and the Rafidain bloc also boycotted the session.

Earlier in the day, the parliament reconvened for the first time after two years of suspension and discussed the independence referendum of the region.

Barzani earlier vowed to go ahead with the independence referendum, ignoring calls to postpone the controversial move.

"So far, we haven't seen an alternative that can take the place of the referendum. Do not listen to anyone, we are going to a referendum," Barzani said during a campaign for the Kurdish referendum in Duhok Province.

On June 7, Barzani announced his intention to hold a referendum on the independence of the Kurdish region from Iraq on Sept. 25.

The referendum has been opposed by Baghdad because it would threaten the integrity of Iraq and would distract the ongoing fight against Islamic State militants by Iraqi forces.

The neighboring countries of Turkey, Iran and Syria also feel that the move would threaten their territorial integrity, as large numbers of Kurdish population live in those countries.



US says Russia offers to send monitors for Syria cease-fire


WASHINGTON - Russia has offered to send its own monitors to prevent violations of a new cease-fire in southwest Syria by the Syrian government, a senior US official said on Thursday.

"The Russians have made clear they are very serious about this and willing to put some of their people on the ground to help monitor from the regime side," said Brett McGurk, US special presidential envoy to the coalition against the Islamic State, here at a press conference.

"So far we're very encouraged by the progress over the last five days," McGurk added.

Russia, the United States and Jordan reached a deal last week to support a cease-fire in southwest Syria, setting up a de-escalation zone to create environment for reaching a political solution to the crisis in Syria.

At a press conference on Thursday in Paris, France, US President Donald Trump said the United States was working with Russia to establish a second cease-fire in "a very rough part of Syria."

"I think the President's referring to a very constructive discussion that he had with the Russians and building from this southwest agreement," said McGurk when asked about Trump's remarks.

Meanwhile, the seventh round of Syrian peace talks, attended by the Syrian government and opposition officials, started on Monday in Geneva, Switzerland, in an attempt to end the six-year-long civil war in Syria. But expectations for a breakthrough are low as the two sides are still far apart over a number of issues.


Matlosana as a competitive investment destination


South Africa's Statistician-General Pali Lehohla in September released a report entitled "Whither the Demographic Dividend". In his preface, he says: "Decisions we take in the course of political office often have effects, impacts and consequences long after we leave office - the 'construction sites' which require solid foundations, strong walls and impermeable roofs."

It is therefore of great concern that in the recently released statistics by Statistics SA on poverty and inequality in South Africa, the North West province remains among the most affected. Between 2001 and 2016, the province ranked between No 4 and No 7 in terms of levels of equality and development.

The leadership of the province has decided to act decisively to change this situation. We cannot continue to lament weak economic growth, lack of jobs, poor levels of education or the fact that our young people are becoming increasingly restless due to the lack of opportunities.

We have looked at our competitive advantages to put together a plan for development, job creation and investment. Our competitive offerings to domestic and international investors include our easy access to Gauteng, as well as regional markets of Botswana and Namiba. As a province, we also contribute 5.7 percent of national outputs to key economic sectors including mining, agriculture and manufacturing.

Yet at the same time, we cannot ignore the deep levels of poverty and inequality in the province, including the fact that 65 percent of it is rural with a low economic base.

Matlosana as a competitive investment destination

Our citizens are emphatic in expressing their dissatisfaction with the current state of affairs in the province through actions including increasingly frequent and violent service delivery protests.

We in the City of Matlosana, the largest city in the province, have assembled a team to put together bankable projects which can attract domestic and international investment. Through this we are confident we will be able to elevate the levels of development in the province while creating jobs which will directly impact on the socio-economic conditions of our people.

These projects was presented at the inaugural Matlosana Trade and Investment Conference from Oct 11 to 13 at the Rio Hotel Casino and Convention Resort Klerksdorp, North West province.

In looking to revitalize our socioeconomic levels, and as we collectively grapple with the as-yet-unknown impact of the fourth industrial revolution which will mean more mechanization, we must begin to look at economic development through a much broader lens.

We are therefore looking to position the province differently in the areas of manufacturing and mining, infrastructure, green economy, agriculture and agro-processing, tourism, and transport infrastructure development opportunities.

With this forward-looking lens in mind, we have made available land to develop an industrial park which aims to become a hub offering affordable industrial space for small and medium-sized enterprises. In addition to traditional areas of economic activity, including mining and manufacturing, we would like to see the park host companies who are able to contribute to metal beneficiation (ferrous and non-ferrous metals), agro-processing and solar geyser manufacturing, manufacturing of mining tools and apparel, and machinery in support of the mining companies in the area.

We have also identified the opportunity to develop the N12 Treasure Route, which would connect communities to the economic hub of Matlosana City. Among other benefits, better road infrastructure would make it easier for citizens to access employment opportunities and other services in Matlosana City.

Housing is also crucial the wellbeing of our people and the Isago Development aims to address the current housing backlog while providing wide-ranging business opportunities in the area.

To enhance our connectivity to the country, we have also identified the upgrading of the airport infrastructure. This will also support the transportation of cargo produced at the industrial park and elsewhere in the province.

With a view to leveraging existing opportunities for the future, we have put together a project producing biodiesel from sunflowers, 79 percent of which come from in the North West province and the Free State. Sunflowers have been identified as ideal for the South African climate and biodiesel is a cost-effective alternative to fossil fuel.

Heritage tourism in the province can build on South Africa's strong tourist appeal. Identified projects include the development of the Goudkoppie Theme Park at the site where gold was first discovered. Another exciting project would include the reconstruction of the house where Bishop Desmond Tutu was born, as well as the building of a shelter and home for the destitute. Through this we can also contribute to honoring our living heritage.

The need to ensure our young people are drawn into the mainstream of society and the economy has never been as important. We have therefore included in our plans to revitalize the economy of the City of Matlosana in particular, and the province in general, the establishment of a tertiary institution.

The magnitude of the sentiment expressed by Lehohla should inspire us as leaders who are tasked with ensuring the implementation of the National Development Plan, which seeks to create conditions for South Africa's development and prosperity.

We must put in place the right foundations, strong walls and impermeable roofs to ensure our efforts yield results which can be built upon long after we leave office. This is the only way to ensure we begin to build confidence in the citizens of our country that we will act in their best interests.

We are confident that our plans, aimed at the invigorating the economy of the North West province, will contribute to our competitive advantages which will add value for investors and our citizens alike. It is time to harness the collective power of our citizens to drive development and, at the same time, listen more to our people. We have heard the people of the North West province and we are prepared to act.

The author is the executive mayor of the City of Matlosana.



China's 'soft power' benefits Kenya
By Nancy Okang'a | China Daily Africa | Updated: 2017-10-15 10:56

For many years, the Western world has been looked up to as an attractive pacesetter for African countries in many ways. However, the tide is swiftly changing, as China increasingly becomes attractive to Africa.

It would appear that China's growing economic presence in Africa is shaping Chinese "soft power" on the continent. In Kenya, as in many other African countries, China has largely become the "talk of the town".

The fact that China has built a strong trade partnership with Kenya and, by extension, Africa has been acknowledged numerous times. Building on the strong economic engagements, Africa and China have sought to improve interactive relations under the rubric of soft power.

Therefore, even as the media coverage of the Chinese presence in the country focuses on the economic aspects of the relationship that involve trade, investments and tourism, China's soft power is a force that cannot be ignored. The increasing use of soft power has been manifested in different ways as China's engagements with Kenya take various platforms, mainly education, training, media, culture and infrastructure construction.

China's 'soft power' benefits Kenya

Indeed, more Africans are now studying in China compared with Western powers such as the United States, the United Kingdom, France and Germany, thanks to the scholarship opportunities offered by the Chinese government. Notably, scholarships from the West have dwindled recently, while Chinese scholarships have consistently shot up over the past one and a half decades. Young Africans yearning to fulfill their aims of attaining education and career dreams are now "looking East".

Large numbers of Kenyan students are being awarded scholarships funded by the People's Republic of China to study in universities in China at doctoral, master's and undergraduate levels. In short order, African graduates who studied or are studying in China will run into the thousands, bringing back with them deeper and experiential understanding of China over and above the much-needed knowledge and skills.

Interestingly, the scholarships are also backed by Chinese companies. This speaks to a direct link between economic engagements and soft power. For instance, China Road and Bridge Corp, which has been an active contractor in Kenya's infrastructure development, has been supporting tertiary education for a number of students. It will be remembered for building the Kenyan Standard Gauge Railway, a major economic booster since it facilitates the movement of goods and people. But in addition to the purely economic significance of the infrastructure built by CRBC, the company is now investing in the "soft" aspects by offering full scholarships to Kenyan students to undertake studies in the crucial field of engineering in China.

The introduction of Confucius Institutes in Kenya has also been a good strategy toward attainment of China's soft power, since soft power is usually largely about the cultural attraction rather than the economic. Therefore, the presence of numerous Confucius Institutes in Kenyan universities such as the University of Nairobi, Kenyatta University, Egerton University and Moi University - where students are given an opportunity to learn the Chinese language and thereby interact with Chinese culture - has accelerated the growth of China's soft power in Kenya.

The media, too, have been an avenue, such as China Central Television, which in 2012 launched a broadcasting center in Nairobi. The television network airs African news as well as Chinese TV dramas and documentaries. Additionally, China Daily newspaper circulates in many parts of Africa. All these will hopefully function to promote mutual understanding between Africa and China.

Some may argue that investment in the soft aspects of education, culture and media is aimed at colonizing African people. The argument may be that Africans will abandon the West and embrace an Oriental ideology.

On the contrary, beyond the trade and the general economic relationship between Kenya and China, investment in the soft dimensions of the Africa-China relations has far greater benefits. While economic ties are important, investing in people has long-lasting ramifications, especially when one considers the fact that the Africa-China relationship is a long-term one.

Additionally, investment in soft power works to counterbalance the dominance of the West in Africa, thus offering Africans alternative viewpoints of the world. In essence, African, Chinese and Western perspectives can all co-exist, with the people picking the aspects that they consider best from the different cultures.

The author has a master's degree in African literature from the University of Witwatersrand, Johannesburg. The views do not necessarily reflect those of China Daily.



276 killed in deadliest single attack in Somalia's history
MOGADISHU - The most powerful bomb blast ever witnessed in Somalia's capital killed 276 people with around 300 others injured, the country's information minister said early Monday, making it the deadliest single attack in this Horn of Africa nation. The toll was expected to rise.

In a tweet, Abdirahman Osman called the attack "barbaric" and said countries including Turkey and Kenya had already offered to send medical aid. Hospitals were overwhelmed a day after a truck bomb targeted a crowded street near key government ministries, including foreign affairs.

As angry protesters gathered near the scene of the attack, Somalia's government blamed the al-Qaida-linked al-Shabab extremist group for what it called a "national disaster". However, Africa's deadliest Islamic extremist group, which often targets high-profile areas of the capital, had yet to comment.

Al-Shabab earlier this year vowed to step up attacks after both the Trump administration and Somalia's recently elected president announced new military efforts against the group.

The Mogadishu bombing is one of the deadliest attacks in sub-Saharan Africa, larger than the Garissa University attack in Kenya in 2015 and the US Embassy bombings in Kenya and Tanzania in 1998.

Doctors at Mogadishu hospitals struggled to assist badly wounded victims, many burned beyond recognition. "This is really horrendous, unlike any other time in the past," said Dr. Mohamed Yusuf, the director of Medina hospital.

Inside, bleary-eyed nurses transported a man whose legs had been blown off. He waited as surgeons attended to another badly injured patient. Exhausted doctors struggled to keep their eyes open, while screams from victims and newly bereaved families echoed through the halls.

"Nearly all of the wounded victims have serious wounds," said nurse Samir Abdi. "Unspeakable horrors." The smell of blood was strong.

A teary-eyed Hawo Yusuf looked at her husband's badly burned body. "He may die waiting," she said. "We need help.

"Ambulance sirens echoed across the city as bewildered families wandered in the rubble of buildings, looking for missing relatives. "In our 10 year experience as the first responder in #Mogadishu, we haven't seen anything like this," the Aamin Ambulance service tweeted.

Grief overwhelmed many.

"There's nothing I can say. We have lost everything," wept Zainab Sharif, a mother of four who lost her husband. She sat outside a hospital where he was pronounced dead after hours of efforts by doctors to save him.

The country's Somali-American leader, President Mohamed Abdullahi Mohamed, declared three days of mourning and joined thousands of people who responded to a desperate plea by hospitals to donate blood. "I am appealing all Somali people to come forward and donate," he said.

Mogadishu, a city long accustomed to deadly bombings by al-Shabab, was stunned by the force of Saturday's blast. The explosion shattered hopes of recovery in an impoverished country left fragile by decades of conflict, and it again raised doubts over the government's ability to secure the seaside city of more than 2 million people.

"They don't care about the lives of Somali people, mothers, fathers and children," Prime Minister Hassan Ali Khaire said of the attackers. "They have targeted the most populated area in Mogadishu, killing only civilians."

Rescue workers searched for survivors trapped under the rubble of the largely destroyed Safari Hotel, which is close to Somalia's foreign ministry. The explosion blew off metal gates and blast walls erected outside the hotel.

The United States condemned the bombing, saying "such cowardly attacks reinvigorate the commitment of the United States to assist our Somali and African Union partners to combat the scourge of terrorism." It tweeted a photo of its charge d'affaires in Somalia donating blood.

The United Nations special envoy to Somalia called the attack "revolting", saying an unprecedented number of civilians had been killed. Michael Keating said the UN and African Union were supporting the Somali government's response with "logistical support, medical supplies and expertise."

In a tweet, UN Secretary-General Antonio Guterres said he was "sickened" by the attack, and his spokesman urged all Somalis to unite against extremism and work together to build a "functional" federal state.


Death toll in Somalia blast rises to 231
Agencies | Updated: 2017-10-15 22:31

MOGADISHU — The Latest on explosion in Somalia's capital (all times local):5:20 p.m.
A senator says the death toll from a massive truck bomb blast in Somalia's capital has risen to 231.
Abshir Abdi Ahmed says 275 others were injured. He cites doctors at hospitals he has visited in Mogadishu.
Saturday's blast is the single deadliest attack ever in the Horn of Africa nation.
Many of the bodies in hospital mortuaries are yet to be identified.

Death toll in Somalia blast rises to 231

Somali government forces and civilians gather at the scene of an explosion in KM4 street in the Hodan district of Mogadishu, Somalia October 15, 2017. （Photo/Agencies)

3:05 p.m.
Local journalists say one freelance journalist was killed in Saturday's massive bombing in Somalia's capital and several were injured.
Voice of America says one of its reporters, Abdulkaidr Mohamed Abdulle, is among the injured.
Police and hospital sources say the death toll from the truck bomb in Mogadishu has risen to 189 in what is the single deadliest attack ever in the Horn of Africa nation.
— Abdi Guled in Mogadishu.
2:35 p.m.
The death toll from a massive explosion in Somalia's capital has risen to 189 with over 200 others injured, police and hospital sources say, making it the single deadliest attack ever in the Horn of Africa nation.
Doctors are struggling to assist hundreds of horrifically wounded victims, with many burnt beyond recognition.
Somalia's government has blamed Saturday's truck bombing in Mogadishu on the al-Shabab extremist group, which has not commented.
— Abdi Guled in Mogadishu.
1:25 p.m.
The United States is joining the condemnation of Saturday's massive truck bombing in Somalia's capital that left scores dead.
A statement by the U.S. mission to Somalia says that "such cowardly attacks reinvigorate the commitment of the United States to assist our Somali and African Union partners to combat the scourge of terrorism."The U.S. military this year has stepped up drone strikes and other efforts this year against the al-Qaida-linked al-Shabab, which is based in Somalia and often targets Mogadishu.
1:20 p.m.
The International Committee of the Red Cross says four volunteers with the Somali Red Crescent Society are among the dead after a huge truck bombing in Somalia's capital.
A statement Sunday says "this figure may rise as there are a number of volunteers still missing."Security and medical sources say at least 53 people are dead after what Mogadishu residents call the largest explosion they've ever witnessed.
Officials have pleaded for blood donations. More than 60 people are injured.
Somalia's government has blamed the al-Shabab extremist group, which has not commented.
10:45 a.m.
Security and medical sources say the death toll from Saturday's truck bomb blast in Somalia's capital has risen to 53 as hospitals struggle to cope with the high number of casualties. More than 60 others are injured.
Police Capt. Mohamed Hussein says many victims died at hospitals from their wounds.
Somalia's government has yet to release the exact death toll from an explosion many called the most powerful they had ever witnessed in Mogadishu.
Ambulance sirens still echo across the city as bewildered families wander in the rubble of buildings.
President Mohamed Abdullahi Mohamed has joined thousands of people who responded to a desperate plea by hospitals to donate blood for the wounded victims.
The al-Shabab extremist group often targets high-profile areas in the capital with bombings.




Ethiopian Airlines mulling flights to Hangzhou as 6th Chinese destination
 Updated: 2017-10-15 18:52

ADDIS ABABA - Ethiopia's national air carrier Ethiopian Airlines (ET) is mulling flights to the eastern Chinese city of Hangzhou.

Hangzhou, the capital of Zhejiang province, has in recent years been known as an emerging technology hub and home to e-commerce giant Alibaba.

Tewolde Gebremariam, CEO of ET, told Xinhua on Sunday the air carrier is mulling Hangzhou as its next destination as part of its plan to attract more Chinese tourists and business people.

Private investment from China to Ethiopia in 2017, up to September 5, has reached more than $680 million, outpacing the entire 2016 figures of $560 million.

China is the single largest source of foreign direct investment (FDI) to Ethiopia for the last several years, as Ethiopia bids to attract Chinese expertise and money for its industrialization ambitions.

Chinese tourists are also a rising demographics with the East African country attracting 41,660 Chinese tourists in 2015, a trend the Ethiopian government expects to grow in the coming years.

Ethiopia had earned $3.32 billion from 886,897 tourists that visited the nation during the Ethiopian Fiscal Year 2016/17 that ended on July 8.

The country plans to earn $4.5 billion from 1.2 million tourists during the 2017/18 Fiscal Year that started July 9.

Hangzhou however is not the only Chinese destination that Ethiopian Airlines is mulling starting flights to.

Gebremariam previously told Xinhua ET is considering flights to Shenzhen, a major innovation and entrepreneurship center, as another destination possibly bringing the number of flight destinations ET has to Chinese cities to seven in total.

Ethiopian Airlines currently flies to five destinations in China: Beijing, Chengdu, Shanghai, Hong Kong and Guangzhou.

	
Western world
From Wikipedia, the free encyclopedia
For other uses, see Western World (disambiguation).
"Westerners" and "Occident" redirect here. For historical politics in Korea, see Westerners (Korean political faction). For other uses, see Occident (disambiguation).
Not to be confused with Western Hemisphere or Western bloc.
See also: Western culture and Westernization
Western world according to Samuel P. Huntington
Western world defined as the Anglosphere and the European Single Market
The Parthenon, Athens, Greece
The Pantheon, Rome, Italy

The Western world, or simply the West, refers to various nations, depending on the context, most often including at least part of Europe. There are many accepted definitions based on commonalities.[1] The Western world is also known as the Occident (from Latin: occidens "sunset, West", as contrasted with Orient).

The concept of the Western part of the Earth has its roots in the Greco-Roman world in Europe, Judaism and the advent of Christianity in Ancient Israel.[2][3][4][5][6] In the modern era, Western culture has been heavily influenced by the traditions of the Renaissance, Protestant Reformation, and Age of Enlightenment – and shaped by the expansive imperialism and colonialism of the 15th to 20th centuries. Before the Cold War era, the traditional Western viewpoint identified Western Civilization with the Western Christian (Catholic-Protestant) countries and culture.[7] Its political usage was temporarily changed by the antagonism during the Cold War in the mid-to-late 20th century (1947–1991).

The term originally had a literal geographic meaning. It contrasted Europe with the cultures and civilisations of the Middle East and North Africa, Sub-Saharan Africa, South Asia, Southeast Asia, and the remote Far East which early-modern Europeans saw as the East. In the contemporary cultural meaning, the phrase Western world includes Europe, as well as many countries of European colonial origin with substantial European ancestral populations in the Americas and Oceania.[8]

Contents

    1 Introduction
    2 Western culture
    3 Historical divisions
        3.1 Hellenic
        3.2 Roman Empire
        3.3 Christian schism
        3.4 Colonial "West"
        3.5 Cold War context
    4 Modern definitions
        4.1 Cultural definition
        4.2 Modern political definition
        4.3 Economic definition
    5 Other views
        5.1 Definition by the statistics bureau of Norway
        5.2 Turkey
    6 Maps
        6.1 Europe
        6.2 World
    7 Gallery
    8 See also
    9 References
    10 Further reading

Introduction

Western culture was influenced by many older great civilizations of the ancient Near East, such as Phoenicia, Minoan Crete, Sumer, Babylonia, and also Ancient Egypt. It originated in the Mediterranean basin and its vicinity; Greece and Rome are often cited as its originators. Over time, their associated empires grew first to the east and west to include the rest of the Mediterranean and Black Sea coastal areas, conquering and absorbing. Later, they expanded to the north of the Mediterranean Sea to include Western, Central, and Southeastern Europe. Christianisation of Ireland (5th century), Bulgaria, Serbia (9th century), Kievan Rus' (Russia, Ukraine, Belarus; 10th century), Scandinavia (12th century) and Lithuania (14th century) brought the rest of present-day European territory into Western civilisation.

Historians, such as Carroll Quigley in The Evolution of Civilizations,[9] contend that Western civilization was born around 500 AD, after the total collapse of the Western Roman Empire, leaving a vacuum for new ideas to flourish that were impossible in Classical societies. In either view, between the fall of the Western Roman Empire and the Renaissance, the West (or those regions that would later become the heartland of the culturally "western sphere") experienced a period of first, considerable decline,[10] and then readaptation, reorientation and considerable renewed material, technological and political development. This whole period of roughly a millennium is known as the Middle Ages, its early part forming the "Dark Ages", designations that were created during the Renaissance and reflect the perspective on history, and the self-image, of the latter period.

The knowledge of the ancient Western world was partly preserved during this period due to the survival of the Eastern Roman Empire and the institutions of the Catholic Church; it was also greatly expanded by the Arab importation[11][12] of both the Ancient Greco-Roman and new technology through the Arabs from India and China to Europe.[13][14] Since the Renaissance, the West evolved beyond the influence of the ancient Greeks and Romans and the Islamic world due to the Commercial,[15] Scientific,[16] and Industrial Revolutions,[17] and the expansion of the peoples of Western and Central European empires, and particularly the globe-spanning empires of the 18th and 19th centuries.[18] Numerous times, this expansion was accompanied by Christian missionaries, who attempted to proselytize Christianity.

Generally speaking, the current consensus would locate the West, at the very least, in the cultures and peoples of Europe (at least the European Union member states, EFTA countries, European microstates),[19][20] the United States, Canada, Australia, New Zealand, and parts of Latin America. There is debate among some as to whether Latin America is in a category of its own.[7][21][22] The Russian culture (particularly literature, music, painting, philosophy and architecture) is classified as a part of the Western culture.[citation needed]
Western culture
Main articles: Western culture, Western literature, Western art history, and Classical music
Further information: History of Western civilization
History of
Western philosophy
Part of "School of Athens" by Raphael (Raffaelo Sanzio, 1483-1520)
Western philosophy

    By era

    Pre-Socratic Ancient Medieval Renaissance Modern Contemporary 

    By century

    16th 17th 18th 19th 20th 21st 

See also

    Religious philosophy

    Buddhist Christian Hindu Islamic Jewish Sikh 

    Eastern philosophy

    Chinese Indian Iranian Japanese Korean 

    Western culture Western world 

    v t e 

The term "Western culture" is used very broadly to refer to a heritage of social norms, ethical values, traditional customs, religious beliefs, political systems, and specific artifacts and technologies.

Specifically, Western culture may imply:

    a Biblical Christian cultural influence in spiritual thinking, customs and either ethic or moral traditions, around the Post-Classical Era and after.
    European cultural influences concerning artistic, musical, folkloric, ethic and oral traditions, whose themes have been further developed by Romanticism.
    a Graeco-Roman Classical and Renaissance cultural influence, concerning artistic, philosophic, literary, and legal themes and traditions, the cultural social effects of migration period and the heritages of Celtic, Germanic, Slavic and other ethnic groups, as well as a tradition of rationalism in various spheres of life, developed by Hellenistic philosophy, Scholasticism, Humanisms, the Scientific Revolution and Enlightenment.

The concept of Western culture is generally linked to the classical definition of the Western world. In this definition, Western culture is the set of literary, scientific, political, artistic and philosophical principles that set it apart from other civilizations. Much of this set of traditions and knowledge is collected in the Western canon.[23]

The term has come to apply to countries whose history is strongly marked by European immigration or settlement, such as the Americas, and Oceania, and is not restricted to Europe.

Some tendencies that define modern Western societies are the existence of political pluralism, laicism, generalization of middle class, prominent subcultures or countercultures (such as New Age movements), increasing cultural syncretism resulting from globalization and human migration. The modern shape of these societies is strongly based upon the Industrial Revolution and the societies' associated social and environmental problems, such as class and pollution, as well as reactions to them, such as syndicalism and environmentalism.
Historical divisions

The geopolitical divisions in Europe that created a concept of East and West originated in the Roman Empire.[24] The Eastern Mediterranean was home to the highly urbanized cultures that had Greek as their common language (owing to the older empire of Alexander the Great and of the Hellenistic successors), whereas the West was much more rural in its character and more readily adopted Latin as its common language. After the fall of the Western Roman Empire, Western and Central Europe were substantially cut off from the East where Byzantine Greek culture and Eastern Christianity became founding influences in the Arab/Muslim world and among the Eastern and Southern Slavic peoples. Roman Catholic Western and Central Europe, as such, maintained a distinct identity particularly as it began to redevelop during the Renaissance. Even following the Protestant Reformation, Protestant Europe continued to see itself as more tied to Roman Catholic Europe than other parts of the perceived civilized world.

Use of the term West as a specific cultural and geopolitical term developed over the course of the Age of Exploration as Europe spread its culture to other parts of the world. In the past two centuries the term Western world has sometimes been used synonymously with Christian world because of the numerical dominance of Roman Catholicism and Protestantism compared to other Christian traditions, ancient Roman ideas, and heresies. As secularism rose in Europe and elsewhere during the 19th and 20th centuries, the term West came to take on less religious connotations and more political connotations, especially during the Cold War. Additionally, closer contacts between the West and Asia and other parts of the world in recent times have continued to cloud the use and meaning of the term.
Hellenic
The Ancient Greek world, c. 550 BC

The Hellenic division between the barbarians and the Greeks contrasted in many societies the Greek-speaking culture of the Greek settlements around the Mediterranean to the surrounding non-Greek cultures. Herodotus considered the Persian Wars of the early 5th century BC a conflict of Europa versus Asia (which he considered all land north and east of the Sea of Marmara, respectively). The terms "West" and "East" were not used by any Greek author to describe that conflict. The anachronistic application of those terms to that division entails a stark logical contradiction, given that, when the term "West" appeared, it was used in opposition to the Greeks and Greek-speaking culture.

Western society traces its cultural origins, at least partially, to Greek thought and Christian religion, thus following an evolution that began in ancient Greece and the Levant, continued through the Roman Empire, and spread throughout Europe. The inherently "Greek" classical ideas of history (which one might easily say they invented) and art may, however, be considered almost inviolate in the West, as their original spread of influence survived the Hellenic period of Roman classical antiquity, the Dark Ages, its resurgence during the Western Renaissance, and has managed somehow to keep and exert its pervasive influence down into the present age, with every expectation[by whom?] of it continuing to dominate any secular Western cultural developments.

However, the conquest of the Western parts of the Roman Empire by Germanic peoples and the subsequent dominance by the Western Christian Papacy (which held combined political and spiritual authority, a state of affairs absent from Greek civilization in all its stages), resulted in a rupture of the previously existing ties between the Latin West and Greek thought,[25] including Christian Greek thought. The Great Schism and the Fourth Crusade confirmed this deviation.

On the other hand, the Modern West, emerging after the Renaissance as a new civilization, has been greatly influenced by (its own interpretation of) Greek thought, which was preserved in the Roman Empire and the medieval Islamic world during the Medieval West's Dark Ages and transmitted from there by emigration of Greek scholars, courtly marriages, and Latin translations. The Renaissance in the West emerged partly from currents within the Roman (Byzantine) Empire.
Roman Empire
Main article: Roman Empire
The Roman Empire in 210 AD, at its greatest extent under Septimius Severus (its vassals in pink)
The Roman Empire under Trajan in 117 AD

Ancient Rome (510 BC–AD 476) was a civilization that grew from a city-state founded on the Italian Peninsula about the 9th century BC to a massive empire straddling the Mediterranean Sea. In its 12-century existence, Roman civilization shifted from a monarchy, to a republic, to an autocratic empire. It came to dominate Western, Central and Southeastern Europe and the entire area surrounding the Mediterranean Sea through conquest using the Roman legions and then through cultural assimilation by giving Roman privileges and eventually citizenship to the whole empire. Nonetheless, despite its great legacy, a number of factors led to the eventual decline of the Roman Empire.

The Western Roman Empire provinces eventually were replaced by Germanic ruled kingdoms in the 5th century AD due to civil wars, corruption, and devastating Germanic invasions from such tribes as the Goths, the Franks and the Vandals.

The Eastern Roman Empire, governed from Constantinople, is usually referred to as the Byzantine Empire after 476, the traditional date for the "fall of the Western Roman Empire" and for the beginning of the Early Middle Ages. The Eastern Roman Empire survived the fall of the West, and protected Roman legal and cultural traditions, combining them with Greek and Christian elements, for another thousand years. The name Byzantine Empire was used after the Byzantine Empire ended, the inhabitants of the Byzantine Empire continued to call themselves Romans.
Plato, Seneca and Aristotle in a medieval manuscript illustration

The Roman Empire succeeded the approximately 500-year-old Roman Republic (510 BC – 1st century BC), which had been weakened by the conflict between Gaius Marius and Sulla and the civil war of Julius Caesar against Pompey and Marcus Brutus. During these struggles hundreds of senators were killed, and the Roman Senate had been refilled with loyalists of the First Triumvirate and later those of the Second Triumvirate.

Several dates are commonly proposed to mark the transition from Republic to Empire, including the date of Julius Caesar's appointment as perpetual Roman dictator (44 BC), the victory of Caesar's heir Octavian at the Battle of Actium ( 2, 31 September BC), and the Roman Senate's granting to Octavian the honorific Augustus. (16, 27 January BC). Octavian/Augustus officially proclaimed that he had saved the Roman Republic and carefully disguised his power under republican forms: Consuls continued to be elected, tribunes of the plebeians continued to offer legislation, and senators still debated in the Roman Curia. However, it was Octavian who influenced everything and controlled the final decisions, and in final analysis, had the legions to back him up, if it became necessary.

Roman expansion began long before the empire and reached its zenith under emperor Trajan with the conquest of Dacia in AD 106. During this territorial peak, the Roman Empire controlled about 5 900 000 km² (2,300,000 sq.mi.) of land surface and had a population of 100 million. From the time of Caesar to the Fall of the Western Roman Empire, Rome dominated Western Eurasia (as well as the Mediterranean coast of northern Africa) comprising the majority of its population, and trading with population living outside it through trade routes. Ancient Rome has contributed greatly to the development of law, war, art, literature, architecture, technology and language in the Western world, and its history continues to have a major influence on the world today. Latin language has been the base from which Roman languages evolved and it has been the official language of the Catholic Church and all Catholic religious ceremonies all over Europe until 1967, as well as an or the official language of countries such as Poland (9th–18th centuries).[26]

The Roman Empire is where the idea of the "West" began to emerge. Due to Rome's central location at the heart of the Empire, "West" and "East" were terms used to denote provinces West and east of the capital itself. Therefore, Iberia (Portugal and Spain), Gaul (France), Mediterranean coast of North Africa (Tunisia, Algeria, and Morocco) and Britannia were all part of the "West", while Greece, Cyprus, Anatolia, Lebanon, Syria, Israel, Palestine, Egypt, and Libya were part of the "East". Italy itself was considered central, until the reforms of Diocletian, with the idea of formally dividing the Empire into true two halves: Eastern and Western.

In 395, the Roman Empire formally split into a Western Roman Empire and an Eastern one, each with their own emperors, capitals, and governments, although ostensibly they still belonged to one formal Empire. The dissolution of the Western half (nominally in 476, but in truth a long process that ended by 500) left only the Eastern Roman Empire alive. For centuries, the East continued to call themselves Eastern Romans, while the West began to think in terms of Latins (those living in the old Western Empire) and Greeks (those inside the Roman remnant to the east).
Christian schism
Main article: East–West Schism
Crystal Clear app kedit.svg
	
This section may need to be rewritten entirely to comply with Wikipedia's quality standards. You can help. The discussion page may contain suggestions. (September 2015)
The religious distribution in 1054[27]

In the early 4th century, the Roman Emperor Constantine the Great established the city of Constantinople as the capital of the Eastern Roman Empire. The Eastern Roman Empire included lands east of the Adriatic Sea and bordering on the Eastern Mediterranean and parts of the Black Sea. This division into Eastern and Western Roman Empires was reflected in the administration of the Roman Catholic and Eastern Greek Orthodox churches, with Rome and Constantinople debating over whether either city was the capital of Western religion.

As the Eastern (Orthodox) and Western (firstly Catholic, then Protestant as well) churches spread their influence, the line between Eastern and Western Christianity was moving. Its movement was affected by the influence of the Byzantine empire and the fluctuating power and influence of the Catholic church in Rome. Beginning in the Middle Ages religious cultural hegemony slowly waned in Europe generally. This process may have prompted the geographic line of religious division to approximately follow a line of cultural divide.

The influential American conservative political scientist, adviser and academic Samuel P. Huntington argued that this cultural division still existed during the Cold War as the approximate Western boundary of those countries that were allied with the Soviet Union. Others have fiercely criticized these views arguing they confuse the Eastern Roman Empire with Russia, especially considering the fact that the country that had the most historical roots in Byzantium, Greece, expelled communists and was allied with the West during the Cold War. Still, Russia accepted Eastern Christianity from the Byzantine Empire (by the Patriarch of Constantinople: Photios I) linking Russia very close to the Eastern Roman Empire world. Later on, in 16th century Russia created its own religious centre in Moscow. Religion survived in Russia beside severe persecution carrying values alternative to the communist ideology.

Under Charlemagne, the Franks established an empire that was recognized as the Holy Roman Empire by the Pope in Rome, offending the Roman Emperor in Constantinople. The crowning of the Emperor by the Pope led to the assumption that the highest power was the papal hierarchy, establishing, until the Protestant Reformation, the civilization of West Christendom. The Latin Rite Catholic Church of western and central Europe headed by the Pope split with the eastern, Greek-speaking Patriarchates during the Great Schism. Meanwhile, the extent of each expanded, as British Isles, Germanic peoples, Bohemia, Poland, Hungary, Scandinavia, Baltic peoples and the other non-Christian lands of the northwest were converted by the Western Church, while Greece, Bulgaria, Romania, Serbia, Montenegro, Russia, Belarus, Ukraine, and Georgia were converted by the Eastern Church.

In this context, the Protestant reformation may be viewed as a schism within the Catholic Church. Martin Luther, in the wake of precursors, broke with the pope and with the emperor, backed by many of the German princes in an attempt to reform corruption within the church. These changes were adopted by the Scandinavian kings. Later, the commoner Jean Cauvin (John Calvin) assumed the religio-political leadership in Geneva, a former ecclesiastical city whose prior ruler had been the bishop. The English King later improvised on the Lutheran model, but subsequently many Calvinist doctrines were adopted by popular dissenters, leading to the English Civil War.

Both royalists and dissenters colonized North America, eventually resulting in an independent United States of America.
Colonial "West"

The Reformation, and consequent dissolution of West Christendom as even a theoretical unitary political body, resulted in the Thirty Years War. The war ended in the Peace of Westphalia, which enshrined the concept of the nation-state and the principle of absolute national sovereignty in international law.
The Industrial Revolution, which began in Great Britain in the mid 18th to early 19th century, forever modified the economy worldwide.

These concepts of a world of nation-states, coupled with the ideologies of the Enlightenment, the coming of modernity, the Scientific Revolution,[28] and the Industrial Revolution,[29] produced powerful political and economic institutions that have come to influence (or been imposed upon) most nations of the world today. Historians agree that the Industrial Revolution was one of the most important events in history.[30]

This process of influence (and imposition) began with the voyages of discovery, colonization, conquest, and exploitation of Portugal and Spain it continued with the rise of the Dutch East India Company, and the creation and expansion of the British and French colonial empires. Due to the reach of these empires, Western institutions expanded throughout the world. Even after demands for self-determination from subject peoples within Western empires were met with decolonization, these institutions persisted. One specific example was the requirement that post-colonial societies were made to form nation-states (in the Western tradition), which often created arbitrary boundaries and borders that did not necessarily represent a whole nation, people, or culture, and are often the cause of international conflicts and friction even to this day. Though the overt colonial era has passed, Western nations, as comparatively rich, well-armed, and culturally powerful states, still wield a large degree of influence throughout the world.

Although not part of Western colonization process proper, Western culture entered Japan primarily in the so-called Meiji period (1868–1912), though earlier contact with the Portuguese, the Spaniards and the Dutch were also present in the recognition of European nations as strategically important to the Japanese. The traditional Japanese society was virtually overturned into an industrial and militarist power like Western countries such as the United Kingdom, the French Third Republic, and the German Empire.
Cold War context

During the Cold War, a new definition emerged. Earth was divided into three "worlds". The First World, analogous in this context to what was called the West, was composed of NATO members and other countries aligned with the United States. The Second World was the Eastern bloc in the Soviet sphere of influence, including the Soviet Union (15 republics including presently independent Estonia, Latvia, Lithuania) and Warsaw Pact countries like Poland, Bulgaria, Hungary, Romania, East Germany (now united with Germany), Czechoslovakia (now split into the Czech Republic and Slovakia).

The Third World consisted of countries, many of which were unaligned with either, and important members included India, Yugoslavia, Finland (Finlandization) and Switzerland (Swiss Neutrality); some include the People's Republic of China, though this is disputed, since the People's Republic of China, as communist, had friendly relations – at certain times – with the Soviet bloc, and had a significant degree of importance in global geopolitics. Some Third World countries aligned themselves with either the US-led West or the Soviet-led Eastern bloc.
European trade blocs as of the late 1980s. EEC member states are marked in blue, EFTA – green, and Comecon – red.
	
East and West in 1980, as defined by the Cold War. The Cold War had divided Europe politically into East and West, with the Iron Curtain splitting Central Europe.

A number of countries did not fit comfortably into this neat definition of partition, including Switzerland, Sweden, Austria, and Ireland, which chose to be neutral. Finland was under the Soviet Union's military sphere of influence (see FCMA treaty) but remained neutral and was not communist, nor was it a member of the Warsaw Pact or Comecon but a member of the EFTA since 1986, and was west of the Iron Curtain. In 1955, when Austria again became a fully independent republic, it did so under the condition that it remain neutral, but as a country to the west of the Iron Curtain, it was in the United States' sphere of influence. Spain did not join the NATO until 1982, towards the end of the Cold War and after the death of the authoritarian Franco.
Modern definitions

The exact scope of the Western world is somewhat subjective in nature, depending on whether cultural, economic, spiritual or political criteria are employed.

Many anthropologists, sociologists and historians oppose "the West and the Rest" in a categorical manner.[31] The same has been done by Malthusian demographers with a sharp distinction between European and non-European family systems. Among anthropologists, this includes Durkheim, Dumont and Lévi-Strauss.[31]

As the term "Western world" does not have a strict international definition, governments do not use the term in legislation of international treaties and instead rely on other definitions.
Cultural definition
Further information: Western culture and Culture of Europe
Latin alphabet world distribution. The dark green areas shows the countries where this alphabet is the sole main script. The light green shows the countries where the alphabet co-exists with other scripts. In addition, nearly all languages which don't regularly use the Latin alphabet have romanization systems.

From a cultural and sociological approach the Western world is defined as including all cultures that are directly derived from and influenced by European cultures, i.e. Europe (at least the European Union member states, EFTA countries, European microstates);[19][20] in the Americas (e.g. Argentina, Brazil, Canada, Chile, Colombia, Costa Rica, Panama, Mexico, United States of America and Uruguay), in Africa (South Africa), and in Oceania (Australia and New Zealand). Together these countries constitute Western society.[8][32][33]

In the 20th century, Christianity declined in influence in many Western countries, mostly in the European Union where some member states have experienced falling church attendance and membership in recent years,[34] and also elsewhere. Secularism (separating religion from politics and science) increased. However, while church attendance is in decline, in some western countries (i.e. Italy, Poland and Portugal) more than half the people state that religion is important,[35] and most Westerners nominally identify themselves as Christians (e.g. 59% in the United Kingdom) and attend church on major occasions, such as Christmas and Easter. In the Americas, Christianity continues to play an important societal role, though in areas such as Canada, low level of religiosity is common as a result of experiencing processes of secularization similar to European ones. The official religions of the United Kingdom and some Nordic countries are forms of Christianity, even though the majority of European countries have no official religion. Despite this, Christianity, in its different forms, remains the largest faith in most Western countries.[36]

Christianity remains the dominant religion in the Western world, where 70% are Christians.[37] A 2011 Pew Research Center survey found that 76% of Europeans, 73% in Oceania, and about 86% in the Americas (90% in Latin America and 77% in North America) described themselves as Christians.[37][38][39][40]
Modern political definition
Legal systems of the world

Countries of the Western world are generally considered to share certain fundamental political ideologies, including those of liberal democracy, the rule of law, human rights and gender equality. All of these are prerequisites, for example, for a state to become a full member of the European Union and therefore from a modern political point of view all European Union member states from Western, Central and Eastern Europe are considered part of the Western world.
Economic definition

Though the Cold War has ended, and some members of the former Eastern Bloc make a general movement towards liberal democracy and other beliefs held in common by traditionally Western states, most of the former Soviet republics (except Baltic states) are not considered Western because of the small presence of social and political reform, as well as the significant cultural, economic and political differences to what is known today as described by the term "The West": United States of America and Canada, European Union and European Free Trade Association member states, Australia and New Zealand.

The term "Western world" is sometimes interchangeably used with the term First World or developed countries, stressing the difference between First World and the Third World or developing countries. This usage occurs despite the fact that many countries that may be culturally "Western" are developing countries – in fact, a significant percentage of the Americas are developing countries. It is also used despite many developed countries or regions not being Western (e.g., Japan, South Korea, Taiwan, Singapore, Hong Kong, Macao, Qatar, Israel), and therefore left out when "Western world" is used to denote developed countries.

The existence of "The North" implies the existence of "The South", and the socio-economic divide between North and South. The term "the North" has in some contexts replaced earlier usage of the term "the West", particularly in the critical sense, as a more robust demarcation than the terms "West" and "East". The North provides some absolute geographical indicators for the location of wealthy countries, most of which are physically situated in the Northern Hemisphere, although, as most countries are located in the northern hemisphere in general, some have considered this distinction equally unhelpful.

The 35 high-income countries in the Organisation for Economic Co-operation and Development (OECD), which include: Australia, Canada, Iceland, Israel, Japan, New Zealand, Norway, South Korea, Switzerland, the United States and the countries of the EU (except for: Bulgaria, Croatia, Cyprus, Lithuania, Malta and Romania), are generally included in what used to be called developed world, although the OECD includes countries, namely Chile, Mexico and Turkey, that are not yet fully industrial countries, but newly industrialised countries. Although Andorra, Cyprus, Hong Kong, Macau, Malta, Liechtenstein, Monaco, San Marino, Singapore, Taiwan and Vatican City, are not members of the OECD, they might also be regarded as developed countries, because of their high living standards, high per capita incomes, and their social, economic and political structure are quite similar to those of the high income OECD countries.
Other views
The clash of civilizations according to Samuel P. Huntington (1996), as presented in the book.[41] Huntington's map of major civilizations. What constitutes Western civilization in his view is coloured dark blue, although he considers that Latin America (shown in purple) might be either an additional part of the West or a separate civilization akin to the West. Turkey, Russia and Mexico[42] were considered "torn countries" that are either already part of the West or in the process of joining the West.

A series of scholars of civilization, including Arnold J. Toynbee, Alfred Kroeber, and Carroll Quigley have identified and analyzed "Western civilization" as one of the civilizations that have historically existed and still exist today. Toynbee entered into quite an expansive mode, including as candidates those countries or cultures who became so heavily influenced by the West as to adopt these borrowings into their very self-identity; carried to its limit, this would in practice include almost everyone within the West, in one way or another. In particular, Toynbee refers to the intelligentsia formed among the educated elite of countries impacted by the European expansion of centuries past. While often pointedly nationalist, these cultural and political leaders interacted within the West to such an extent as to change both themselves and the West.[21]

The theologian and paleontologist Pierre Teilhard de Chardin conceived of the West as the set of civilizations descended from the Nile Valley Civilization of Egypt.[43]

Palestinian-American literary critic Edward Said uses the term occident in his discussion of orientalism. According to his binary, the West, or Occident, created a romanticized vision of the East, or Orient to justify colonial and imperialist intentions. This Occident-Orient binary focuses on the Western vision of the East instead of any truths about the East. His theories are rooted in Hegel's Master-slave dialectic: The Occident would not exist without the Orient and vice versa. Further, Western writers created this irrational, feminine, weak "Other" to contrast with the rational, masculine, strong West because of a need to create a difference between the two that would justify imperialist ambitions, Said influenced Indian-American theorist Homi K. Bhabha.

The term the "West" may also be used pejoratively by certain tendencies and especially critical of the influence of the traditional West, due to the history of most[citation needed][vague] of the members of the traditional West being previously involved, at one time or another, in outright imperialism and colonialism. Some of these critics also claim that the traditional West has continued to engage in what might be viewed as modern implementations of imperialism and colonialism, such as neoliberalism and globalization. (It should be noted that many Westerners who subscribe to a positive view of the traditional West are also very critical of neoliberalism and globalization, for their allegedly negative effects on both the developed and developing world.)

Allegedly, definitions of the term "Western world" that some may consider "ethnocentric" others consider "constructed" around one or another Western culture.[citation needed] The British writer Rudyard Kipling wrote about this contrast: East is East and West is West and never the twain shall meet, expressing his belief that somebody from the West "can never understand the Asian cultures" as the latter "differ too much" from the Western cultures. Some may view this alleged incompatibility as a precursor to Huntington's "clash of civilizations" theory.

From a very different perspective, it has also been argued that the idea of the West is, in part, a non-Western invention, deployed in the non-West to shape and define non-Western pathways through or against modernity.[44]
Definition by the statistics bureau of Norway

The official statistics bureau of Norway, Statistics Norway, has used a definition of the "West" as "EU28/EEA, United States, Canada, Australia and New Zealand", and a definition of the "Rest of the World" as "Asia, Africa, Latin America, Oceania excluding Australia and New Zealand, and Europe outside EU/EEA", for the purpose of immigration statistics.[19][20]
Turkey
Turkey, an EU candidate, and member states of NATO, in the European Union–Turkey Customs Union

According to Samuel P. Huntington, Turkey, whose political leadership has systematically tried to Westernize the predominantly Muslim country with only 3% of its territory within Europe since the 1920s, is his chief example of a "torn country" that is attempting to join Western civilization. The country's elite started the Westernization efforts, beginning with Mustafa Kemal Atatürk, who took power as the first president of the modern Turkish nation-state in 1923, imposed western institutions and dress, removed the Arabic alphabet and embraced the Latin alphabet, joined NATO, and are seeking to join the European Union since the 1960s with very slow progress.[45]
Maps

The following maps aim to give a perspective of what separates the variously defined Western world from the rest of the world.
Europe

    Geopolitical Occident of Europe.

    European Union and its member candidate countries.
    European Union and European Free Trade Association.
      EU member states (as of 2013)
      EFTA member states

    NATO has added 13 new members since the German reunification and the end of the Cold War.

World

    World map by quartiles of Human Development Index in 2014.

    Western world defined by European Single Market and spread of the Romance languages (2017)

    Countries with 50% or more Christians are colored purple while countries with 10% to 50% Christians are colored pink

    The global distribution of Christians: countries coloured a darker shade have a higher proportion of Christians.[46]

    Distribution of Roman Catholics

    Distribution of Protestants

Gallery

    The Roman Forum (Rome)

    The Parliament of the United Kingdom (London)

    The National Assembly (Paris)

    The United States Congress (Washington, D.C.)

    The School of Athens depicts a fictional gathering of the most prominent thinkers of classical antiquity. Fresco by Raphael, 1510–1511

See also

    Americanization
    Anglicisation
    Anglophone
    Atlanticism
    Eastern world
    East-West dichotomy
    Europeanisation
    Far West
    Francophonie
    Golden billion
    Hispanophone
    Mid-Atlantic English
    Western esotericism
    Western philosophy

Organisations

    European Union
    European Economic Area
    Group of Eight (G8)

Representation in the UN

    Eastern European Group
    Western European and Others Group

References

Western Civilization, Our Tradition; James Kurth; accessed 30 August 2011
Religions in Global Society. p. 146, Peter Beyer (2006)
Cambridge University Historical Series, An Essay on Western Civilization in Its Economic Aspects, p. 40: Hebraism, like Hellenism, has been an all-important factor in the development of Western Civilization; Judaism, as the precursor of Christianity, has indirectly had had much to do with shaping the ideals and morality of western nations since the christian era.
Caltron J.H Hayas, Christianity and Western Civilization (1953), Stanford University Press, p. 2: That certain distinctive features of our Western civilization – the civilization of western Europe and of America – have been shaped chiefly by Judaeo–Graeco–Christianity, Catholic and Protestant.
Horst Hutter, University of New York, Shaping the Future: Nietzsche's New Regime of the Soul And Its Ascetic Practices (2004), p. 111: three mighty founders of Western culture, namely Socrates, Jesus, and Plato.
Fred Reinhard Dallmayr, Dialogue Among Civilizations: Some Exemplary Voices (2004), p. 22: Western civilization is also sometimes described as "Christian" or "Judaeo-Christian" civilization.
[1]|Google books results in English language between the 1800–1960 period
Thompson, William; Hickey, Joseph (2005). Society in Focus. Boston, MA: Pearson. 0-205-41365-X.
"The Evolution of Civilizations – An Introduction to Historical Analysis (1979)". Archive.org. 10 March 2001. p. 84. Retrieved 31 January 2014.
Middle Ages

    Of the three great civilizations of Western Eurasia and North Africa, Christian Europe began as the least developed in virtually all aspects of material and intellectual culture, well behind the Islamic states and Byzantium.

H. G. Wells, The Outline of History, Section 31.8, The Intellectual Life of Arab Islam

    For some generations before Muhammad, the Arab mind had been, as it were, smouldering, it had been producing poetry and much religious discussion; under the stimulus of the national and racial successes it presently blazed out with a brilliance second only to that of the Greeks during their best period. From a new angle and with a fresh vigour it took up that systematic development of positive knowledge, which the Greeks had begun and relinquished. It revived the human pursuit of science. If the Greek was the father, then the Arab was the foster-father of the scientific method of dealing with reality, that is to say, by absolute frankness, the utmost simplicity of statement and explanation, exact record, and exhaustive criticism. Through the Arabs it was and not by the Latin route that the modern world received that gift of light and power.

Lewis, Bernard (2002). What Went Wrong. Oxford University Press. p. 3. ISBN 978-0-06-051605-5.

    For many centuries the world of Islam was in the forefront of human civilization and achievement ... In the era between the decline of antiquity and the dawn of modernity, that is, in the centuries designated in European history as medieval, the Islamic claim was not without justification.

"Science, civilization and society". Es.flinders.edu.au. Retrieved 6 May 2011.
Richard J. Mayne, Jr. "Middle Ages". Britannica.com. Retrieved 6 May 2011.
InfoPlease.com, commercial revolution
"The Scientific Revolution". Wsu.edu. 6 June 1999. Archived from the original on 1 May 2011. Retrieved 6 May 2011.
Eric Bond; Sheena Gingerich; Oliver Archer-Antonsen; Liam Purcell; Elizabeth Macklem (17 February 2003). "Innovations". The Industrial Revolution. Retrieved 6 May 2011.
"How Islam Created Europe; In late antiquity, the religion split the Mediterranean world in two. Now it is remaking the Continent.". TheAtlantic.com. May 2016. Retrieved 25 April 2016.
Questions about immigrant-related statistics. "Key figures Immigration and immigrants – SSB". Ssb.no. Retrieved 15 December 2015.
Innvandrere og norskfødte med innvandrerforeldre, 1. januar 2015 Statistics Norway (in Norwegian) retrieved 15 December 2015
Cf., Arnold J. Toynbee, Change and Habit. The challenge of our time (Oxford 1966, 1969) at 153–56; also, Toynbee, A Study of History (10 volumes, 2 supplements).
Auster, Lawrence (3 April 2006). "Are Hispanics Westerners? The Debate Continues". Amnation.com. Retrieved 31 January 2014.
Duran 1995, p. 81
Bideleux, Robert; Jeffries, Ian. A history of eastern Europe: crisis and change. Routledge. p. 48. ISBN 978-0-415-16112-1.
Charles Freeman. The Closing of the Western Mind. Knopf, 2003. ISBN 1-4000-4085-X
Karin Friedrich et al., The Other Prussia: Royal Prussia, Poland and Liberty, 1569–1772, Cambridge University Press, 2000, ISBN 0-521-58335-7, Google Print, p. 88
Dragan Brujić (2005). "Vodič kroz svet Vizantije (Guide to the Byzantine World)". Beograd. p. 51.[dead link]
"Modern West Civ. 7: The Scientific Revolution of the 17 Cent". Fordham.edu. Retrieved 6 May 2011.
"The Industrial Revolution". Mars.wnec.edu. Archived from the original on 18 October 2000. Retrieved 6 May 2011.
Industrial Revolution and the Standard of Living: The Concise Encyclopedia of Economics, Library of Economics and Liberty
"New Left Review - Jack Goody: The Labyrinth of Kinship". Retrieved 24 July 2007.
"Embassy of Brazil – Ottawa". Brasembottawa.org. Retrieved 6 May 2011.
Falcoff, Mark. "Chile Moves On". AEI. Retrieved 6 May 2011.
Ford, Peter (22 February 2005). "What place for God in Europe". USA Today. Retrieved 24 July 2009.
Eurostat (2005). "Social values, Science and Technology" (PDF). Special Eurobarometer 225. Europa, web portal: 9. Retrieved 11 June 2009.
See ARDA data archives: http://www.thearda.com/internationalData/regions/index.asp
ANALYSIS (19 December 2011). "Global Christianity". Pewforum.org. Retrieved 17 August 2012.
ANALYSIS (19 December 2011). "Europe". Pewforum.org. Retrieved 17 August 2012.
ANALYSIS (19 December 2011). "Americas". Pewforum.org. Retrieved 17 August 2012.
ANALYSIS (19 December 2011). "Global religious landscape: Christians". Pewforum.org. Retrieved 17 August 2012.
The World of Civilizations: Post-1990 scanned image Archived 12 March 2007 at the Wayback Machine.
Huntington, Samuel P. (August 2, 2011). The Clash of Civilizations and the Remaking of World Order. Simon & Schuster. pp. 151–54. ISBN 978-1451628975.
Cf., Teilhard de Chardin, Le Phenomene Humain (1955), translated as The Phenomena of Man (New York 1959).
Bonnett, A. 2004. The Idea of the West
Samuel P. Huntington. The Clash of Civilizations and the Remaking of World Order. The Free Press. pp. 144–49.

    ANALYSIS (19 December 2011). "Table: Religious Composition by Country, in Percentages". Pewforum.org. Retrieved 17 August 2012.

Further reading

    Ankerl, Guy (2000). Coexisting contemporary civilizations: Arabo-Muslim, Bharati, Chinese, and West. INU societal research. Vol.1:. Global communication without universal civilization. Geneva: INU Press. ISBN 2881550045.
    Bavaj, Riccardo: "The West": A Conceptual Exploration , European History Online, Mainz: Institute of European History, 2011, retrieved: 28 November 2011.
    Daly, Jonathan. “The Rise of Western Power: A Comparative History of Western Civilization” (London and New York: Bloomsbury, 2014). ISBN 978-1441161314.
    Daly, Jonathan. "Historians Debate the Rise of the West" (London and New York: Routledge, 2015). ISBN 978-1138774810.
    The Western Tradition homepage at Annenberg/CPB - where you can watch each episode on demand free (Pop-ups required)
    J. F. C. Fuller. A Military History of the Western World. Three Volumes. New York: Da Capo Press, Inc., 1987 and 1988.

    V. 1. From the earliest times to the Battle of Lepanto; ISBN 0306803046.
    V. 2. From the defeat of the Spanish Armada to the Battle of Waterloo; ISBN 0306803054.
    V. 3. From the American Civil War to the end of World War II; ISBN 0306803062.

Political science
From Wikipedia, the free encyclopedia


Political science, also called government,[1][2] is a social science which deals with systems of governance, and the analysis of political activities, political thoughts and political behaviour.[3] It deals extensively with the theory and practice of politics which is commonly thought of as determining of the distribution of power and resources. Political scientists "see themselves engaged in revealing the relationships underlying political events and conditions, and from these revelations they attempt to construct general principles about the way the world of politics works."[4]

Political science comprises numerous subfields, including comparative politics, political economy, international relations, political theory, public administration, public policy and political methodology. Furthermore, political science is related to, and draws upon, the fields of economics, law, sociology, history, philosophy, geography, psychology, and anthropology.

Comparative politics is the science of comparison and teaching of different types of constitutions, political actors, legislature and associated fields, all of them from an intrastate perspective. International relations deals with the interaction between nation-states as well as intergovernmental and transnational organizations. Political theory is more concerned with contributions of various classical and contemporary thinkers and philosophers.

Political science is methodologically diverse and appropriates many methods originating in social research. Approaches include positivism, interpretivism, rational choice theory, behaviouralism, structuralism, post-structuralism, realism, institutionalism, and pluralism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents and official records, secondary sources such as scholarly journal articles, survey research, statistical analysis, case studies, experimental research and model building.

Contents

    1 Overview
        1.1 Behavioral revolution and new institutionalism
        1.2 Anticipating of crises
        1.3 Political science in the Soviet Union
        1.4 Recent developments
    2 Political science education
        2.1 Cognate fields
    3 History
    4 See also
    5 References
    6 Further reading
    7 External links
        7.1 Library Guides to Political Science

Overview
Globe icon.
	
The examples and perspective in this section deal primarily with the United States and do not represent a worldwide view of the subject. You may improve this article, discuss the issue on the talk page, or create a new article, as appropriate. (August 2016) (Learn how and when to remove this template message)

Political scientists study matters concerning the allocation and transfer of power in decision making, the roles and systems of governance including governments and international organizations, political behaviour and public policies. They measure the success of governance and specific policies by examining many factors, including stability, justice, material wealth, peace and public health. Some political scientists seek to advance positive (attempt to describe how things are, as opposed to how they should be) theses by analysing politics. Others advance normative theses, by making specific policy recommendations.

Political scientists provide the frameworks from which journalists, special interest groups, politicians, and the electorate analyse issues. According to Chaturvedy, "...Political scientists may serve as advisers to specific politicians, or even run for office as politicians themselves. Political scientists can be found working in governments, in political parties or as civil servants. They may be involved with non-governmental organizations (NGOs) or political movements. In a variety of capacities, people educated and trained in political science can add value and expertise to corporations. Private enterprises such as think tanks, research institutes, polling and public relations firms often employ political scientists." [5] In the United States, political scientists known as "Americanists" look at a variety of data including constitutional development, elections, public opinion and public policy such as Social Security reform, foreign policy, US Congressional committees, and the US Supreme Court — to name only a few issues.

Because political science is essentially a study of human behaviour, in all aspects of politics, observations in controlled environments are often challenging to reproduce or duplicate, though experimental methods are increasingly common (see experimental political science).[6] Citing this difficulty, former American Political Science Association President Lawrence Lowell once said "We are limited by the impossibility of experiment. Politics is an observational, not an experimental science."[7] Because of this, political scientists have historically observed political elites, institutions, and individual or group behaviour in order to identify patterns, draw generalizations, and build theories of politics.

Like all social sciences, political science faces the difficulty of observing human actors that can only be partially observed and who have the capacity for making conscious choices unlike other subjects such as non-human organisms in biology or inanimate objects as in physics. Despite the complexities, contemporary political science has progressed by adopting a variety of methods and theoretical approaches to understanding politics and methodological pluralism is a defining feature of contemporary political science.

The advent of political science as a university discipline was marked by the creation of university departments and chairs with the title of political science arising in the late 19th century. In fact, the designation "political scientist" is typically for those with a doctorate in the field, but can also apply to those with a master's in the subject.[8] Integrating political studies of the past into a unified discipline is ongoing, and the history of political science has provided a rich field for the growth of both normative and positive political science, with each part of the discipline sharing some historical predecessors. The American Political Science Association was founded in 1903 and the American Political Science Review was founded in 1906 in an effort to distinguish the study of politics from economics and other social phenomena. To date, the [9]American Political Science Review is the leading journal in Political Science research.
Behavioral revolution and new institutionalism

In the 1950s and the 1960s, a behavioural revolution stressing the systematic and rigorously scientific study of individual and group behaviour swept the discipline. A focus on studying political behaviour, rather than institutions or interpretation of legal texts, characterized early behavioural political science, including work by Robert Dahl, Philip Converse, and in the collaboration between sociologist Paul Lazarsfeld and public opinion scholar Bernard Berelson.

The late 1960s and early 1970s witnessed a take off in the use of deductive, game theoretic formal modelling techniques aimed at generating a more analytical corpus of knowledge in the discipline. This period saw a surge of research that borrowed theory and methods from economics to study political institutions, such as the United States Congress, as well as political behaviour, such as voting. William H. Riker and his colleagues and students at the University of Rochester were the main proponents of this shift.

Despite considerable research progress in the discipline based on all the kinds of scholarship discussed above, it has been observed that progress toward systematic theory has been modest and uneven.[10]
Anticipating of crises

The theory of political transitions,[11] and the methods of their analysis and anticipating of crises,[12] form an important part of political science. Several general indicators of crises and methods were proposed for anticipating critical transitions.[13] Among them, a statistical indicator of crisis, simultaneous increase of variance and correlations in large groups, was proposed for crises anticipation and successfully used in various areas.[14] Its applicability for early diagnosis of political crises was demonstrated by the analysis of the prolonged stress period preceding the 2014 Ukrainian economic and political crisis. There was a simultaneous increase in the total correlation between the 19 major public fears in the Ukrainian society (by about 64%) and also in their statistical dispersion (by 29%) during the pre-crisis years.[15] A feature shared by certain major revolutions is that they were not predicted. The theory of apparent inevitability of crises and revolutions was also developed.[16]
Political science in the Soviet Union

In the Soviet Union, political studies were carried out under the guise of some other disciplines like theory of state and law, area studies, international relations, studies of labor movement, "critique of bourgeois theories", etc. Soviet scholars were represented at the International Political Science Association (IPSA) since 1955 (since 1960 by the Soviet Association of Political and State Studies).

In 1979, the 11th World Congress of IPSA took place in Moscow. Until the late years of the Soviet Union, political science as a field was subjected to tight control of the Communist Party of the Soviet Union and was thus subjected to distrust. Anti-communists accused political scientists of being "false" scientists and of having served the old regime.[17]

After the fall of the Soviet Union, two of the major institutions dealing with political science, the Institute of Contemporary Social Theories and the Institute of International Affairs, were disbanded, and most of their members were left without jobs. These institutes were victims of the first wave of anticommunist opinion and ideological attacks. Today, the Russian Political Science Association unites professional political scientists from all around Russia.
Recent developments

In 2000, the Perestroika Movement in political science was introduced as a reaction against what supporters of the movement called the mathematicization of political science. Those who identified with the movement argued for a plurality of methodologies and approaches in political science and for more relevance of the discipline to those outside of it.[18]

Evolutionary psychology theories argue that humans have evolved a highly developed set of psychological mechanisms for dealing with politics. However, these mechanisms evolved for dealing with the small group politics that characterized the ancestral environment and not the much larger political structures in today's world. This is argued to explain many important features and systematic cognitive biases of current politics.[19]
Political science education

Political science, possibly like the social sciences as a whole, "as a discipline lives on the fault line between the 'two cultures' in the academy, the sciences and the humanities."[20] Thus, in some American colleges where there is no separate School or College of Arts and Sciences per se, political science may be a separate department housed as part of a division or school of Humanities or Liberal Arts.[21] Whereas classical political philosophy is primarily defined by a concern for Hellenic and Enlightenment thought, political scientists are also marked by a great concern for "modernity" and the contemporary nation state, along with the study of classical thought, and as such share a greater deal of terminology with sociologists (e.g. structure and agency).

Most United States colleges and universities offer B.A. programs in political science. M.A. or M.A.T. and Ph.D. or Ed.D. programs are common at larger universities. The term political science is more popular in North America than elsewhere; other institutions, especially those outside the United States, see political science as part of a broader discipline of political studies, politics, or government. While political science implies use of the scientific method, political studies implies a broader approach, although the naming of degree courses does not necessarily reflect their content.[22] Separate degree granting programs in international relations and public policy are not uncommon at both the undergraduate and graduate levels. Master's level programs in political science are common when political scientists engage in public administration.[23]

The national honor society for college and university students of government and politics in the United States is Pi Sigma Alpha.
Cognate fields

Most political scientists work broadly in one or more of the following five areas:

    Comparative politics, including area studies
    International relations
    Political philosophy or political theory
    Public administration
    Public law

Some political science departments also classify methodology as well as scholarship on the domestic politics of a particular country as distinct fields. In the United States, American politics is often treated as a separate subfield.

In contrast to this traditional classification, some academic departments organize scholarship into thematic categories, including political philosophy, political behaviour (including public opinion, collective action, and identity), and political institutions (including legislatures and international organizations). Political science conferences and journals often emphasize scholarship in more specific categories. The American Political Science Association, for example, has 42 organized sections that address various methods and topics of political inquiry.[24]
History
Main article: History of political science

As a social science, contemporary political science started to take shape in the latter half of the 19th century. At that time it began to separate itself from political philosophy, which traces its roots back to the works of Chanakya, Aristotle and Plato which were written nearly 2,500 years ago. The term "political science" was not always distinguished from political philosophy, and the modern discipline has a clear set of antecedents including also moral philosophy, political economy, political theology, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal state.
See also

    Political science portal 

    Outline of political science – structured list of political topics, arranged by subject area
    Index of politics articles – alphabetical list of political subjects
    Political lists – lists of political topics
    Political science terminology
    Outline of law
    Index of law articles
    Process tracing
	
Higher education in the United States
From Wikipedia, the free encyclopedia
Education in the United States

    By state + insular areas
    By subject area
    History of
    Issues: Finance – Law – Literacy – Reform
    Levels: Primary – Secondary – Higher
    Organizations

Nuvola apps bookcase.svg Education portal
Flag of the United States.svg United States portal

    v t e 

2008 - 2012 Bachelor's Degree or Higher (5-year estimate) by County (percent)
People 25 Years and Over Who Have Completed an Advanced Degree by State (percent)(2012)

Higher education in the United States is an optional final stage of formal learning following secondary education. Higher education, also referred to as post-secondary education, third stage, third level, or tertiary education occurs most commonly at one of the 4,726 Title IV degree-granting institutions, either colleges or universities in the country.[1] These may be public universities, private universities, liberal arts colleges, community colleges, or for-profit colleges. High visibility issues include greater use of the Internet, such as massive open online courses, competency-based education, sexual assault, cutbacks in state and local spending, rapidly rising tuition and increasing student loan debt[2][3]

According to the National Student Clearinghouse, US college enrollment has declined for five consecutive years and is projected to continue declining for the next two decades.[4] This decline amounts to a 2.5 million loss in enrollment from the peak year of 2010-2011.[5]

Strong research and funding have helped make America's elite colleges and universities among the world's most prestigious, making them particularly attractive to international students, professors and researchers in the pursuit of academic excellence.[6] Other countries, however, are now offering incentives to take away researchers [7] as funding is threatened.[8][9]

According to Pew Research Center and Gallup poll surveys conducted in 2017, public opinion about US colleges has been declining, especially among Republicans and the white working class.[10][11][12][13]

Several scholarly books critical of US higher education have also been published in the last decade. Suzanne Mettler's book, Degrees of Inequality has been particularly critical of higher education policy that reinforces growing class inequality.[14] Craig David Wilder has also written Ebony and Ivy, a scholarly analysis of early US universities and their enslavement of African Americans.[15]

Unlike the tertiary education system of the UK and Australia, American education is unique in the world to place strong emphasis on Liberal Arts education in its higher education curriculum.[16]

Contents

    1 Statistics
        1.1 Declining numbers
    2 Types of colleges and universities
        2.1 Community colleges
        2.2 Colleges
        2.3 Liberal arts colleges
        2.4 Technical Schools
        2.5 Universities
        2.6 For-Profit Colleges
    3 Student funding
        3.1 Grant, scholarship, and work study program facts
        3.2 Student loans
        3.3 Free College Tuition
    4 History
        4.1 Colonial era
            4.1.1 Enslavement and Exclusion
        4.2 19th century
        4.3 Impact of 19th-century colleges
        4.4 Law and medical schools
        4.5 20th century
            4.5.1 Catholic colleges and universities
            4.5.2 Community colleges
            4.5.3 Student activism and social movements
            4.5.4 For-profit colleges
        4.6 21st century
            4.6.1 Technology
            4.6.2 Protests and Political Clashes
    5 Funding of schools
        5.1 Concentration of wealth
    6 Admission process
    7 International study and student exchange
    8 Government coordination
        8.1 Coordination institutions
        8.2 Academic employment and adjunctification
    9 Selected issues
        9.1 Rankings of tertiary institutions
        9.2 Financial value of degrees
            9.2.1 College major by underemployment rate
            9.2.2 Financial value of skilled trades
        9.3 Socioeconomic status
        9.4 Race
        9.5 Gender
        9.6 Undocumented Students
        9.7 MOOC
    10 Criticism
        10.1 Cost and finances
        10.2 For-profit schools
        10.3 Student Loan Debt
        10.4 Graduation rates
        10.5 Academic standards
        10.6 Political views
            10.6.1 Professor Watchlist
        10.7 Geographic considerations
        10.8 Skepticism about higher education
        10.9 Declining accessibility and high cost
        10.10 Ending affirmative action
        10.11 Sexual assault
    11 See also
    12 References
    13 Further reading
    14 External links

Statistics

In 2012, the US peaked at a total of 4,726 Title IV-eligible, degree-granting institutions: 3,026 4-year institutions and 1,700 2-year institutions.[1] By 2014-2015, the total had declined to 4,627 institutions.[17]

In 2010, the US peaked at more than 21 million students in higher education, roughly 5.7% of the total population.[18] About 13 million of these students were enrolled full-time which was 81,000 students lower than 2010.[18]:table 224

A US Department of Education longitudinal survey of 15,000 high school students in 2002, and again in 2012 at age 27, found that 84% of the 27-year-old students had some college education, but only 34% achieved a bachelor's degree or higher; 79% owe some money for college and 55% owe more than $10,000; college dropouts were three times more likely to be unemployed than those who finished college; 40% spent some time unemployed and 23% were unemployed for six months or more; and 79% earned less than $40,000 per year.[19][20]
Declining numbers

Falling birth rates have meant fewer young people are graduating from high school. The number of high school graduates grew 30% from 1995 to 2013, then peaked at 3.5 million and projections show it holding at about that level in the next decade.[21] According to the National Student Clearinghouse and the National Center for Education Statistics, enrollment has been declining since 2010-2011. According to the National Student Clearinghouse, enrollment in 2016 was down about 2.4 million from the peak year.[22] The US Department of Education, National Center for Education Statistics, has also reported a loss of more than 800,000 students from 2010 to 2014.[23]

The number of Title IV eligible institutions has also declined by 17.8 percent since 2012-13.[24]
Types of colleges and universities
Harvard University: Harvard Yard with freshman dorms in the background.
See also: List of American institutions of higher education and List of state universities in the United States

Colleges and universities in the U.S. vary in terms of goals: some may emphasize a vocational, business, engineering, or technical curriculum (like polytechnic universities and land-grant universities) while others may emphasize a liberal arts curriculum. Many combine some or all of the above, being a comprehensive university. In the US, the term "college" refers to either one of three types of education institutions: stand-alone higher level education institutions that are not components of a university, including 1) community colleges, 2) liberal arts colleges, or 3) a college within a university, mostly the undergraduate institution of a university. Unlike colleges versus universities in other portions of the world, a stand-alone college is truly stand-alone and is not part of a university, and is also not affiliated with an affiliating university.
The Great Dome of Massachusetts Institute of Technology (MIT), a university adopting the polytechnic university model.

Almost all colleges and universities are coeducational. During a dramatic transition in the 1970s, all but a handful of men's colleges started accepting women. Over 80 percent of the women's colleges of the 1960s have closed or merged, leaving fewer than 50 in operation. Over 100 historically black colleges and universities (HBCUs) operate, both private (such as Morehouse College) and public (such as Florida A&M).

Higher education has led to the creation of accreditation organizations, independent of the government, to vouch for the quality of competing degrees. The accreditation agencies rate universities and colleges on criteria such as academic quality, the quality of their libraries, the publishing records of their faculty, the degrees which their faculty hold, and their financial solvency. Accrediting agencies have been criticized for possible conflicts of interest that lead to favorable results.[25] Non-accredited institutions exist, such as Bible colleges, but the students are not eligible for federal loans.
Community colleges

Community colleges are often, though not always, two-year colleges. They have open admissions, with generally lower tuition fees than other state or private schools.[citation needed] Graduates receive an associate's degree, such as an Associate of Arts (A.A.), upon graduating. Many students earn an associate degree at a two-year institution before transferring to a four-year institution to complete studies for a bachelor's degree.[26]

Four-year colleges usually have a larger number of students, offer a greater range of studies, and provide the bachelor's degree (most commonly the Bachelor of Arts (B.A.) or Bachelor of Science (B.S.)). They are primarily either undergraduate institutions (i.e. Liberal Arts Colleges), or the undergraduate institution of a university (such as Harvard College and Yale College).
The DeSeversky Mansion on the Old Westbury campus of New York Institute of Technology.

According to National Student Clearinghouse data, community college enrolment has dropped by 1.6 million students since its peak year of 2010-2011. According to one survey, 88% of community colleges were facing declining enrollments.[27] A report in the New York Times in 2017 suggested that of the nation's 18 million undergraduates, 40% were attending community college; of these students, 62% were attending community college full-time, and 40% of them worked at least 30 hours a week or more, and more than half lived at home as a way to save money.[28]
Colleges

Some U.S. states (such as Washington) now offer tertiary education at "colleges", many of which were formerly called "community colleges". The elevation in status comes from a cooperation between the community college and a local university. There are two primary distinctions between colleges and community colleges that arise from this arrangement.

The first is an increased standardization of curricula and adherence to some university guidelines at the colleges, thereby improving the chances that (former) community college credits are transferred to in-state universities. The aim is to maximize the number of transferred credits, as this has traditionally been a frequent issue that forces students to take redundant coursework, pay more tuition unnecessarily, as well as giving them unfair competitive advantage at university.

The second primary distinction is that the renamed colleges, in cooperation with a university, can offer courses that go beyond the 2-year-level of education that is typical of community colleges. Some colleges go so far as to offer particular, specialized 4-year bachelor's degrees on behalf of the university.
Liberal arts colleges

Four-year institutions in the U.S. emphasizing the liberal arts are liberal arts colleges, entirely undergraduate institutions and stand-alone. They traditionally emphasize interactive instruction, although student research projects are of growing importance. They are known for being residential and for having smaller enrollment, class size, and higher teacher-student ratios than universities. These colleges encourage a high level of teacher-student interaction, at the center of which are classes taught by full-time faculty, rather than graduate student teaching assistants (TAs), who often teach classes at some Research I universities and other universities. Most are private,[according to whom?] although there are public liberal arts colleges. Some offer experimental curricula, such as Hampshire College, Beloit College, Bard College at Simon's Rock, Pitzer College, Sarah Lawrence College, Grinnell College, Bennington College, New College of Florida, and Reed College.
Technical Schools

Technical schools are four-year institutions that emphasize a particular trade or set of technical skills, primarily for the sake of employability.
Universities
Saint Anselm College, a New England liberal arts college

Universities are research-oriented educational institutions which provide both undergraduate and graduate programs. For historical reasons, some universities such as Boston College, Dartmouth College, and The College of William & Mary and College of Charleston have retained the term "college" instead of "university" as their name. Graduate programs grant a variety of master's degrees (like the Master of Arts (M.A.), Master of Science (M.S.), Master of Business Administration (M.B.A.) or Master of Fine Arts (M.F.A.)) in addition to doctorates such as the Ph.D. The Carnegie Classification of Institutions of Higher Education distinguishes among institutions on the basis of the prevalence of degrees they grant and considers the granting of master's degrees necessary, though not sufficient, for an institution to be classified as a university.[29]
Public California State University, Office of the Chancellor in Long Beach, California

Some universities have professional schools. Examples include journalism school, business school, medical schools which award either the M.D. or D.O., law schools (J.D.), veterinary schools (D.V.M.), pharmacy schools (Pharm.D.), and dental schools. A common practice is to refer to different units within universities as colleges or schools, what is referred to outside the U.S. as faculties. Some colleges may be divided into departments, including an anthropology department within a college of liberal arts and sciences, within a larger university. Few universities adopt the term "college" as names of academic organizations. For example, Purdue University is composed of multiple colleges—among others, the College of Agriculture and the College of Engineering. Of these Purdue breaks the College of Agriculture down into departments, such as the Department of Agronomy or the Department of Entomology, whereas Purdue breaks down the College of Engineering into schools, such as the School of Electrical Engineering, which enrolls more students than some of its colleges do. As is common in this scheme, Purdue categorizes both its undergraduate students (and faculty and programs) and its post-graduate students (and faculty and programs) via this scheme of decomposition, being a topical decomposition that focuses on an academic sector of directly related academic disciplines.
Ezekiel W. Cullen Building on the campus of the University of Houston

The American university system is largely decentralized. Public universities are administered by the individual states and territories, usually as part of a state university system. Except for the United States service academies and staff colleges, the federal government does not directly regulate universities. However it can offer federal grants and any institution that receives federal funds must certify that it has adopted and implemented a drug prevention program that meets federal regulations.[30][31]

Each state supports at least one state university and several support many more. California, for example, has three public higher education systems: the 10-campus University of California, the 23-campus California State University, and the 112-campus California Community Colleges System. Public universities often have a large student body, with introductory classes numbering in the hundreds, and some undergraduate classes taught by graduate students. Tribal colleges operated on Indian reservations by some federally recognized tribes are also public institutions.

Many private universities also exist. Among these, some are secular while others are involved in religious education. Some are non-denominational and some are affiliated with a certain sect or church, such as Roman Catholicism (with different institutions often sponsored by particular religious institutes such as the Jesuits) or religions such as Lutheranism or Mormonism. Seminaries are private institutions for those preparing to become members of the clergy. Most private schools (like all public schools) are non-profit, although some are for-profit.
For-Profit Colleges

For-profit higher education (known as for-profit college or proprietary education in some instances) refers to higher education educational institutions operated by private, profit-seeking businesses. University of Phoenix has been the largest for-profit college in the US.[32]

Since 2010, for-profit colleges have received greater scrutiny and negative attention from the US government, state Attorneys General, the media, and scholars.[33]
Student funding
The Main Building on the campus of the University of Texas at Austin

Students often use scholarships, student loans, or grants to supplement their tuition costs, rather than paying all tuition out-of-pocket. Several states offer scholarships that allow students to attend free of tuition or at lower cost, for example the HOPE Scholarship in Georgia and the Bright Futures Scholarship Program in Florida. A considerable number of private liberal arts colleges and universities offer full need-based financial aid, which means that admitted students will only have to pay as much as their families can afford (based on the university's assessment of their income).[34][35] This can turn some of the most prestigious institutions into the cheapest options for low-income students.[36] In most cases, the barrier of entry for students who require financial aid is set higher, a practice called need-aware admissions. Universities with exceptionally large endowments may combine need-based financial aid with need-blind admission, in which students who require financial aid have equal chances to those who do not.[citation needed]

Financial assistance comes in two primary forms: Grant programs and loan programs. Grant programs consist of money the student receives to pay for higher education that does not need to be paid back, while loan programs consist of money the student receives to pay for higher education that must be paid back. Public higher education institutions (which are partially funded through state government appropriation) and private higher education institutions (which are funded exclusively through tuition and private donations) offer both grant and loan financial assistance programs. Grants to attend public schools are distributed through federal and state governments, as well as through the schools themselves; grants to attend private schools are distributed through the school itself (independent organizations, such as charities or corporations also offer grants that can be applied to both public and private higher education institutions).[37] Loans can be obtained publicly through government sponsored loan programs or privately through independent lending institutions.
Grant, scholarship, and work study program facts

Grant programs, as well as work study programs, can be divided into two primary categories: Need-based financial awards and merit-based financial awards. Most state governments provide need-based scholarship programs, while a few also offer merit-based aid.[38] Several need-based grants are provided through the federal government based on information provided on a student's Free Application for Federal Student Aid or FAFSA.[39] The federal Pell Grant is a need-based grant available from the federal government. The federal government also has two other grants that are a combination of need-based and merit-based: the Academic Competitiveness Grant, and the National SMART Grant, but the SMART grant was abolished in 2011 with the last grant awarded in June 2011. In order to receive one of these grants a student must be eligible for the Pell Grant, meet specific academic requirements, and be a US citizen.[37]

A student's eligibility for work study programs is also determined by information collected on the student's FAFSA.[37] Need-based financial awards are money or work study jobs provided to students who do not have the financial resources by themselves to pay for higher education. The intent of need-based financial aid is to close the gap between the required cost to pay for the higher education and the money that is available to pay for the education.[citation needed]

Merit-based financial awards are money given to a student based on a particular gift, talent, conditional situation, or ability that is worthy of the monetary award, regardless of economic standing. The intent of merit-based financial aid is to encourage and reward students who exhibit these qualities, in the hopes that they will attend the university providing the merit-based award or scholarship. Not only does merit-based assistance benefit the student, but the benefit is seen as reciprocal for the educational institution itself, as students who exhibit exceptional qualities are able to enhance the development of the school itself.

Financial aid has also been found to be linked to increased enrollment. A study conducted by the National Bureau of Economic Research found that an increased availability of any amount of financial aid leads to increased enrollment rates. Evidence also suggests that access to financial aid also increases both “persistence and competition”. Further benefit has been noted with academic-based scholarships, augmenting the effects of financial aid by incentivizing the scholarship with performance-based requirements.[40]

Many companies offer tuition reimbursement plans for their employees, in order to make the benefit package more attractive, to upgrade the skill levels and to increase retention.[41]
Student loans
Main article: Student loans in the United States

In 2012, student loan debt owed in the United States totaled more than $1 trillion.[42] In 2012, total student loans exceeded consumer credit card debt for the first time in history.[43]

In late 2016, the total estimated US student loan debt exceeds $1.4 trillion.[44]

Many different types of loans can be taken out by a student or the student's parents in order to pay for higher education. In general these can be divided into two categories: federal student loans and private student loans.

Federal student loans

There are four kinds of student loans available through the government: subsidized Stafford Loans, unsubsidized Stafford Loans, direct loans, and PLUS loans. A student's eligibility for any of these loans, as well as the amount of the loan itself is determined by information on the student's FAFSA. The interest rate and whether or not interest accrues on the loan while the student is in school depends of the type of federal loan. For example, subsidized Stafford Loans do not accrue interest while a student is enrolled in a university, whereas unsubsidized Stafford Loans accrue interest as soon as a student receives them.

In 2017, the Federal Perkins Loan program expired. [45]

Private student loans

Students can also acquire loans privately, through banks, credit unions, savings and loan associations, or other finance companies (ref. article pg. 3). Private loans are typically used to supplement federal student loans, which have a yearly borrowing limit. However, private loans typically have more rigid repayment policies.

Education tax credits

US tax payers may be eligible for tax credits designed to help make higher education more affordable. There are two different tax credits meant to help defray the costs of higher education: the Hope Tax Credit and the Lifetime Learning Tax Credit.
Free College Tuition

Rhode Island, Tennessee, Oregon and New York offer free tuition for community college students.[46] Community college tuition was free in California from 1960 to 1984. The City University of New York also offered free tuition from 1970 to 1976.[47]

Legislation for free community college has been proposed in 11 other states: Arizona, California, Hawaii, Illinois, Kentucky, Maryland, Massachusetts, Mississippi, Oklahoma, Washington, and Wisconsin.[48]
History
Main article: History of higher education in the United States
Accuracy dispute
	
This article appears to contradict the article History of higher education in the United States. Please see discussion on the linked talk page. (May 2015) (Learn how and when to remove this template message)
Colonial era

Religious denominations established most early colleges in order to train ministers. Harvard College was founded by the colonial legislature in 1636. In 1659, Polish Aleksander Karol Kurcjusz first established higher education in New Amsterdam (Bobr-Tylingo 1982, 145). Harvard initially focused on training young men for the ministry, and won general support from the Puritan government, some of whose leaders had attended either Oxford or Cambridge. The College of William & Mary was founded by Virginia government in 1693, with 20,000 acres (81 km2) of land for an endowment, and a penny tax on every pound of tobacco, together with an annual appropriation. James Blair, the leading Church of England minister in the colony, was president for 50 years, and the college won the broad support of the Virginia gentry. It trained many of the lawyers, politicians, and leading planters. Yale College was founded in 1701, and in 1716 was relocated to New Haven, Connecticut. The conservative Puritan ministers of Connecticut had grown dissatisfied with the more liberal theology of Harvard, and wanted their own school to train orthodox ministers. New Light Presbyterians in 1747 set up the College of New Jersey, in the town of Princeton, which later was renamed Princeton University.[49]
Enslavement and Exclusion

Several colleges, including Brown University, Harvard, Dartmouth, William and Mary, University of Virginia, Georgetown University, University of Alabama, University of South Carolina, Clemson University and Rutgers University held enslaved people—and relied on captives to operate.[15][50][51] [52][53][54][55][56]
19th century
Share of all bachelor's degrees awarded by field

Many Protestant denominations, as well as the Catholics, opened small colleges in the nineteenth century.[57][58] The Catholics, especially, opened a number of women's colleges in the early twentieth century.

All of the schools were small, with a limited undergraduate curriculum based on the liberal arts. Students were drilled in Greek, Latin, geometry, ancient history, logic, ethics and rhetoric, with few discussions and no lab sessions. Originality and creativity were not prized, but exact repetition was rewarded. The college president typically enforced strict discipline, and the upperclassman enjoyed hazing the freshman. Many students were younger than 17, and most of the colleges also operated a preparatory school. There were no organized sports, or Greek-letter fraternities, but literary societies were active. Tuition was very low and scholarships were few. Many of their students were sons of clergymen; most planned professional careers as ministers, lawyers or teachers.[59]
Impact of 19th-century colleges

Summarizing the research of Burke and Hall, Katz concludes that in the 19th century:.[60]

    The nation's many small colleges helped young men make the transition from rural farms to complex urban occupations.
    These colleges especially promoted upward mobility by preparing ministers and thereby provided towns across the country with a core of community leaders.
    The elite colleges became increasingly exclusive and contributed relatively little to upward social mobility. By concentrating on the offspring of wealthy families, ministers and a few others, the elite Eastern colleges, especially Harvard, played an important role in the formation of a Northeastern elite with great power.

Law and medical schools

There were no schools of law in the early British colonies, thus no schools of law were in America in colonial times. A few lawyers studied at the highly prestigious Inns of Court in London, while the majority served apprenticeships with established American lawyers.[61] Law was very well established in the colonies, compared to medicine, which was in rudimentary condition. In the 18th century, 117 Americans had graduated in medicine in Edinburgh, Scotland, but most physicians in the colonies learned as apprentices.[62] In Philadelphia, the Medical College of Philadelphia was founded in 1765, and became affiliated with the university in 1791. In New York, the medical department of King's College was established in 1767, and in 1770 awarded the first American M.D. degree.[63]
20th century

At the beginning of the 20th century, fewer than 1,000 colleges with 160,000 students existed in the United States. The number of colleges skyrocketed in waves, during the early and mid 20th century. State universities grew from small institutions of fewer than 1000 students to gigantic campuses with 40,000 more students, as well as a network of regional campuses around the state. In turn the regional campuses broke away and became separate universities. To handle the explosive growth of K-12 education, every state set up a network of teachers' colleges, beginning with Massachusetts in the 1830s. After 1950, they became state colleges and then state universities With a broad curriculum.
Catholic colleges and universities
Main article: History of Catholic education in the United States § Colleges and universities
The Main Building at the University of Notre Dame, the best known Catholic university in the United States

The Association of Catholic Colleges and Universities was founded in 1899 and continues to facilitate the exchange of information and methods.[64] Vigorous debate in recent decades has focused on how to balance Catholic and academic roles, with conservatives arguing that bishops should exert more control to guarantee orthodoxy.[65][66][67]
Community colleges

Major new trends included the development of the junior colleges. They were usually set up by City school systems starting in the 1920s.[68] By the 1960s they were renamed as "community colleges."

Junior colleges grew from 20 in number In 1909, to 170 in 1919. By 1922, 37 states had set up 70 junior colleges, enrolling about 150 students each. Meanwhile, another 137 were privately operated, with about 60 students each. Rapid expansion continued in the 1920s, with 440 junior colleges in 1930 enrolling about 70,000 students. The peak year for private institutions came in 1949, when there were 322 junior colleges in all; 180 were affiliated with churches, 108 were independent non-profit, and 34 were private Schools run for-profit.[69]

Many factors contributed to rapid growth of community colleges. Students parents and businessmen wanted nearby, low-cost schools to provide training for the growing white collar labor force, as well as for more advanced technical jobs in the blue collar sphere. Four years colleges were also growing, albeit not as fast; however many of them were located in rural or small-town areas away from the fast-growing metropolis. Community colleges continue as open enrollment, low-cost institutions with a strong component of vocational education, as well as a low-cost preparation for transfer students into four-year schools. They appeal to a poorer, older, less prepared element.[70][71]
Student activism and social movements

College students were involved in social movements long before the 20th century, but the most dramatic student movements rose in the 1960s. In the 1960s, students organized for civil rights and against the Vietnam War. In the 1970s, students led movements for women's rights and gay rights, as well as protests against South African apartheid [72]
For-profit colleges
Main article: For-profit higher education in the United States

While for-profit colleges originated during Colonial times, growth in these schools was most apparent from the 1980s to about 2011. For-profit college enrollment, however, has declined significantly since 2011, after several federal investigations. For-profit colleges were criticized for predatory marketing and sales practices.[73] The failures of Corinthian Colleges and ITT Technical Institute were the most remarkable closings.[74]
21st century
Students of a U.S. university with their professor on the far right, 2009

Changing technology and politics have resulted in dramatic changes in US higher education during the 21st century.
Technology

In 2017, about 15 percent of all students attend exclusively online, and competition for online students has been increasing [75]
Protests and Political Clashes

Student protests and clashes between left and right appeared on several US campuses in 2017.[76][77][78][79][80]

On August 11, 2017, White nationalists and members of the alt-right rallied at the University of Virginia, protesting the removal of a statue of Robert E. Lee.[81] The following day, one person died during protests in Charlottesville.[82] Following this tragedy, speaking engagements by Richard Spencer were canceled at Texas A&M and University of Florida.[83]
Funding of schools

According to the Center on Budget and Policy Priorities, "years of cuts in state funding for public colleges and universities" have made college less affordable and less accountable for students[84]. As a result, public colleges and universities must close campuses, retain fewer professors and staff, and drive up tuition costs. The decline in funding for these public institutions since the Great Recession is nearly "$9 billion below its 2008 level, after adjusting for inflation."[85]

The long-term consequences of a decline in state funding for public colleges and universities are fewer low-income students, more non-residents of the state (non-resident tuition is typically three times resident tuition), and higher tuition.[86]

Since the Great Recession, U.S. universities have transitioned from federal grants to corporate funds and have been "increasingly reliant on private philanthropy". At the University of Maryland, Northrop Grumman has funded a cybersecurity concentration, designs the curriculum in cybersecurity, provides computers and pays some cost of a new dorm. At Ohio State, IBM partnered to teach big data analytics. Murray State University's engineering program was supported by computer companies. The College of Nanoscale Science and Engineering at State University of New York in Albany, received billions of dollars in private sector investment.[87]

ITT Educational Services Inc, a big operator of For-Profit Schools, warned in July 2014 that it could face restricted funding from the U.S. government for failing to file timely financial reports.[88]
Concentration of wealth

The top 25 US schools have 52% of all endowment wealth.[89]
Admission process
Main article: College admissions in the United States

Students can apply to some colleges using the Common Application. With a few exceptions, most undergraduate colleges and universities maintain the policy that students are to be admitted to (or rejected from) the entire college, not to a particular department or major. (This is unlike college admissions in many European countries, as well as graduate admissions.) Some students, rather than being rejected, are "wait-listed" for a particular college and may be admitted if another student who was admitted decides not to attend the college or university. The five major parts of admission are ACT/SAT scores, GPA, College Application, Essay, and Letters of Recommendation.[90] Not all colleges require essays or letters of recommendation, though they are often proven to increase chances of acceptance.
Further information: Transfer admissions in the United States
International study and student exchange
Columbia University Low Memorial Library.

In 2007-8, American students numbering 262,416 studied outside the country with more than 140,000 of these studying in Europe.[91]

The US is the most popular country in the world in terms of attracting students from other countries, according to UNESCO, with 16% of all international students going to the US (the next highest is the UK with 11%).[92] 671,616 foreign students enrolled in American colleges in 2008-9.[92][93] This figure rose to 723,277 in 2010–2011. The largest number, 157,558, came from China.[94] According to Uni in the USA, despite "exorbitant" costs of US universities, higher education in America remains attractive to international students due to "generous subsidies and financial aid packages that enable students from even the most disadvantaged backgrounds to attend the college of their dreams".[95]
Government coordination
Coordination institutions

Every state has an entity designed to promote coordination and collaboration between higher education institutions. A few are listed:

    Alabama Commission on Higher Education
    California Postsecondary Education Commission
    Texas Higher Education Coordinating Board
    Washington State Higher Education Coordinating Board
    The Georgia Department of Technical and Adult Education

Academic employment and adjunctification

In the 1980s and 1990s significant changes in the economics of academic life began to be felt, identified by some as a catastrophe in the making and by others as a new era with potentially huge gains for the university. Some critics identified the changes as a new "corporatization of the university." Academic jobs have been traditionally viewed by many intellectuals as desirable, because of the autonomy and intellectual freedom they allow (especially because of the tenure system), despite their low pay compared to other professions requiring extensive education. And until the mid-1970s, when federal expenditures for higher education fell sharply, there were routinely more tenure-track jobs than Ph.D. graduates.

In 2012, by contrast, despite rising tuition rates and growing university revenues (especially in the U.S.) well-paid professorial positions were rarer, replaced with poorly paid adjunct positions and graduate-student labor.[96] People with doctorates in the sciences, and, to a lesser extent, mathematics, often found jobs outside of academia (or worked, part-time, in industry to supplement their incomes). A Ph.D. in the humanities and many social sciences prepared the student primarily for academic employment. However, a large proportion of such Ph.D.s—ranging from 30 percent to 60 percent—were unable to obtain tenure-track jobs. They chose between adjunct positions, which paid less and lacked job security; teaching jobs in community colleges or in high schools, where little research is done; the non-academic job market, where they will tend to be overqualified; or some other course of study, such as law or business.[citation needed]

In 2017, 17% of faculty was tenured. 89% of adjunct professors worked at more than one job. An adjunct was paid an average of $2,700 for a single course. 31% of the faculty lived below the poverty level. While student-faculty ratios remained the same since 1975, administrator-student ratio went from 1-84 to 1-68. student-professional staff ratios fell from 50:1 to 21:1. The money that colleges have been receiving have gone into administration and staff and not teaching.[97]

    Academics seem to think that the business world is in a feudal environment characterized by huge status differences and abusive treatment of underlings. They think that because, to be honest, that's a pretty good characterization of...the modern university, where serfs, in the form of adjunct professors toil in the vineyards

[97]

With academic institutions producing Ph.D.s in greater numbers than the number of tenure-track professorial positions they intended to create, there was little question that administrators were cognizant of the economic effects of this arrangement. The sociologist Stanley Aronowitz wrote: "Basking in the plenitude of qualified and credentialed instructors, many university administrators see the time when they can once again make tenure a rare privilege, awarded only to the most faithful and to those whose services are in great demand".[98]

In 2015, some[who?] believed that, as a number of Baby Boomer professors retired, the academic job market would rebound.[citation needed] However, others predicted that this would not result in an appreciable growth of tenure-track positions, as universities would fill their needs with low-paid adjunct positions. Aronowitz ascribed this problem to the economic restructuring of academia as a whole:

    In fact, the program of restructuring on university campuses, which entails reducing full-time tenure-track positions in favor of part-time, temporary, and contingent jobs, has literally "fabricated" this situation. The idea of an academic "job market" based on the balance of supply and demand in an open competitive arena is a fiction whose effect is to persuade the candidate that (he or she) simply lost out because of bad luck or lack of talent. The truth is otherwise.[99]

Selected issues
Rankings of tertiary institutions

Universitas 21 ranked the country as having the best higher education system in the world in 2012. Cost was not considered in the rankings.[100]

Numerous organizations produce rankings of universities in the United States each year. A 2010 University of Michigan study has confirmed that the rankings in the United States have significantly affected colleges' applications and admissions.[101] Referred to as the "granddaddy of the college rankings",[102] America's best–known American college and university rankings have been compiled since 1983 by U.S. News & World Report and are widely regarded as the most influential of all college rankings.[103]

2007 movement
Main article: Criticism of college and university rankings (2007 United States)

On 19 June 2007, during the annual meeting of the Annapolis Group, members discussed the letter to college presidents asking them not to participate in the "reputation survey" section of the U.S. News & World Report survey (this section comprises 25% of the ranking). As a result, "a majority of the approximately 80 presidents at the meeting said that they did not intend to participate in the U.S. News reputational rankings in the future." [104] However, the decision to fill out the reputational survey or not will be left up to each individual college as: "the Annapolis Group is not a legislative body and any decision about participating in the US News rankings rests with the individual institutions." [105] The statement also said that its members "have agreed to participate in the development of an alternative common format that presents information about their colleges for students and their families to use in the college search process." [105] This database will be web-based and developed in conjunction with higher education organizations including the National Association of Independent Colleges and Universities and the Council of Independent Colleges.

On 22 June 2007, U.S. News & World Report editor Robert Morse issued a response in which he argued, "in terms of the peer assessment survey, we at U.S. News firmly believe the survey has significant value because it allows us to measure the "intangibles" of a college that we can't measure through statistical data. Plus, the reputation of a school can help get that all-important first job and plays a key part in which grad school someone will be able to get into. The peer survey is by nature subjective, but the technique of asking industry leaders to rate their competitors is a commonly accepted practice. The results from the peer survey also can act to level the playing field between private and public colleges." [106] In reference to the alternative database discussed by the Annapolis Group, Morse also argued, "It's important to point out that the Annapolis Group's stated goal of presenting college data in a common format has been tried before [...] U.S. News has been supplying this exact college information for many years already. And it appears that NAICU will be doing it with significantly less comparability and functionality. U.S. News first collects all these data (using an agreed-upon set of definitions from the Common Data Set). Then we post the data on our website in easily accessible, comparable tables. In other words, the Annapolis Group and the others in the NAICU initiative actually are following the lead of U.S. News." [106]
Financial value of degrees

Studies have looked at the financial payoff to the large investment in time and money that is typically required to receive an academic degree. People with higher education have always tended to have higher salaries and less unemployment than people with less education. However, the type of degree has a large impact on future earnings.[107][108] By mid-career, the median annual earnings of undergraduates range from $76,000 in the STEM fields to $46,000 in education and social work. There is also a wide variation in the values of graduate degrees over undergraduate degrees. In medicine they increase earnings by an average of 137%, while in the liberal arts the increase is 23%.[109]

Selection of a four-year college as compared to a two-year junior college, even by marginal students such as those with a C+ grade average in high school and SAT scores in the mid 800s, increases the probability of graduation and confers substantial economic and social benefits for most undergraduates.[110][111][112] However, the admission of so many marginal students does impact graduation rates, partly due to the need for these students to take noncredit remedial courses in English, reading, math or science.[113][114]

Some fields of study produce many more graduates than the professions can take in. Due to the resulting higher education bubble, these graduates often have to consider jobs for which they are overqualified, or that have no academic requirements.[115][116][117] Employers have responded to the oversupply of graduates by raising the academic requirements of many occupations higher than is really necessary to perform the work.[118]

Although vocational education is usually less financially lucrative in the long term than a bachelor's degree, it can still provide a respectable income at much less cost in time and money, sometimes with the option of upgrading to a bachelor's degree at a later date. Even ten years after graduation, there are many people with a certificate or associate degree who earn more money than those with a B.A.[119][120][121] It can also benefit university graduates, since some four-year schools fail to prepare their graduates for the kinds of jobs that are available in their surrounding regions. Over seven percent of the nation's community college students already possess a bachelor's degree.[122]
College major by underemployment rate

College majors ranked in ascending order by the percentage of college graduates with degrees in those fields who are employed in jobs that do not require a college degree. Data is from the Federal Reserve Bank of New York, the United States Census Bureau, and the American Community Survey. Note: The unemployment and underemployment rates are for recent college graduates (between the ages 22 and 27).[123]
Rank 	College Major 	Unemployment
rate 	Underemployment
rate 	Median Wage
Early Career 	Median Wage
Mid-Career 	Share with
Graduate Degree
1 	Nursing 	2.0% 	13.4% 	$48,000 	$65,000 	26.5%
2 	Special education 	2.4% 	14.9% 	$33,000 	$43,000 	61.3%
3 	Chemical engineering 	5.2% 	17.0% 	$70,000 	$94,000 	49.8%
4 	Computer engineering 	4.6% 	19.1% 	$60,000 	$100,000 	37.8%
5 	Civil engineering 	2.8% 	19.8% 	$50,000 	$86,000 	37.3%
6 	Pharmacy 	4.1% 	20.3% 	$42,000 	$110,000 	54.4%
7 	Electrical engineering 	4.4% 	20.6% 	$61,000 	$95,000 	44.1%
8 	Mechanical engineering 	3.6% 	20.7% 	$60,000 	$91,000 	40.7%
9 	Aerospace engineering 	2.4% 	20.8% 	$60,000 	$90,000 	49.3%
10 	Primary education 	3.0% 	21.3% 	$32,000 	$42,000 	46.2%
11 	Miscellaneous education 	1.0% 	22.8% 	$34,000 	$43,000 	52.9%
12 	General education 	4.3% 	22.9% 	$33,500 	$44,000 	47.6%
13 	Computer science 	3.6% 	24.6% 	$54,000 	$86,000 	32.2%
14 	Industrial engineering 	6.2% 	25.6% 	$60,000 	$81,000 	41.3%
15 	Early childhood education 	3.8% 	26.2% 	$30,000 	$40,000 	38.6%
16 	Mathematics 	5.9% 	26.3% 	$42,000 	$77,000 	51.0%
17 	Secondary education 	2.6% 	26.7% 	$34,000 	$47,700 	48.3%
18 	Accounting 	4.0% 	26.8% 	$45,000 	$68,000 	27.1%
19 	General engineering 	3.6% 	27.6% 	$55,000 	$80,000 	35.4%
20 	Miscellaneous engineering 	3.1% 	29.9% 	$52,000 	$83,000 	44.4%
21 	Biochemistry 	4.2% 	32.2% 	$35,200 	$75,000 	74.1%
22 	Chemistry 	4.7% 	32.6% 	$39,000 	$69,000 	65.3%
23 	Treatment therapy 	2.3% 	32.8% 	$35,000 	$66,000 	42.6%
24 	Miscellaneous physical sciences 	5.5% 	33.1% 	$38,000 	$70,000 	54.0%
25 	Physics 	6.2% 	34.2% 	$50,000 	$80,000 	70.0%
26 	Construction services 	1.8% 	35.4% 	$50,000 	$80,000 	10.6%
27 	Social services 	4.6% 	35.7% 	$30,000 	$40,000 	47.6%
28 	Finance 	3.7% 	36.8% 	$47,000 	$80,000 	29.8%
29 	Architecture 	6.8% 	37.1% 	$40,000 	$68,000 	36.1%
30 	Business analytics 	3.7% 	38.8% 	$50,000 	$83,000 	23.1%
31 	Commercial art/Graphic design 	5.8% 	41.7% 	$35,000 	$52,000 	10.1%
32 	Earth science 	7.9% 	41.8% 	$30,300 	$63,000 	45.8%
33 	Economics 	5.1% 	42.9% 	$48,000 	$80,000 	40.6%
34 	Engineering technologies 	4.5% 	43.7% 	$42,000 	$75,000 	22.1%
35 	Journalism 	3.4% 	43.7% 	$35,000 	$64,000 	24.3%
36 	Biology 	5.1% 	44.8% 	$31,500 	$60,000 	63.7%
	Overall 	5.0% 	45.1% 	$38,000 	$62,000 	37.3%
37 	Information systems and management 	6.7% 	45.3% 	$43,000 	$70,000 	23.2%
38 	Advertising/Public relations 	4.9% 	46.1% 	$40,000 	$61,000 	16.3%
39 	Family and consumer sciences 	4.3% 	46.2% 	$28,400 	$48,000 	30.0%
40 	Miscellaneous biological sciences 	5.9% 	46.4% 	$35,000 	$57,000 	61.1%
41 	Health services 	3.8% 	47.2% 	$33,000 	$54,000 	53.0%
42 	International affairs 	6.8% 	47.4% 	$40,000 	$70,000 	42.8%
43 	Geography 	8.8% 	48.1% 	$34,500 	$60,000 	30.6%
44 	Political science 	7.2% 	49.7% 	$38,000 	$70,000 	51.9%
45 	General social sciences 	6.4% 	50.4% 	$33,000 	$50,000 	37.9%
46 	Theology/Religion 	6.8% 	50.8% 	$28,600 	$45,000 	43.6%
47 	Interdisciplinary studies 	5.9% 	50.9% 	$35,000 	$60,000 	36.9%
48 	Psychology 	5.9% 	51.3% 	$30,000 	$52,000 	49.9%
49 	English language 	7.5% 	51.7% 	$32,200 	$55,000 	46.5%
50 	Philosophy 	5.0% 	51.7% 	$35,000 	$60,000 	56.0%
51 	Foreign language 	5.7% 	51.8% 	$33,000 	$51,000 	49.8%
52 	Environmental studies 	8.5% 	53.7% 	$30,000 	$60,000 	30.4%
53 	Agriculture 	1.8% 	54.2% 	$36,000 	$55,000 	22.1%
54 	Nutrition sciences 	5.5% 	54.4% 	$36,000 	$50,000 	42.7%
55 	Marketing 	4.6% 	56.1% 	$40,000 	$70,000 	16.7%
56 	Mass media 	8.6% 	56.5% 	$31,200 	$56,000 	19.0%
57 	Sociology 	6.5% 	56.5% 	$33,000 	$54,000 	35.9%
58 	History 	5.9% 	56.6% 	$35,000 	$60,000 	48.4%
59 	Ethnic studies 	7.3% 	57.1% 	$32,000 	$60,000 	50.3%
60 	Communications 	5.4% 	58.1% 	$36,000 	$62,000 	22.7%
61 	General business 	4.9% 	58.2% 	$40,000 	$67,000 	23.9%
62 	Liberal arts 	5.8% 	58.3% 	$32,000 	$56,000 	28.4%
63 	Animal and plant sciences 	2.8% 	58.7% 	$32,000 	$52,000 	35.1%
64 	Art history 	7.8% 	58.9% 	$32,000 	$53,000 	42.6%
65 	Anthropology 	8.8% 	59.1% 	$30,000 	$50,000 	47.7%
66 	Medical technicians 	6.7% 	59.6% 	$44,000 	$60,000 	23.9%
67 	Business management 	4.8% 	61.4% 	$38,000 	$61,000 	22.1%
68 	Miscellaneous technologies 	5.4% 	62.0% 	$33,000 	$70,000 	17.4%
69 	Fine arts 	7.6% 	62.3% 	$29,000 	$50,000 	22.5%
70 	Leisure/Hospitality 	5.0% 	62.6% 	$32,000 	$52,000 	29.4%
71 	Public policy/Pre-law 	6.3% 	63.1% 	$32,000 	$60,000 	42.8%
72 	Performing arts 	6.8% 	66.5% 	$30,000 	$47,000 	38.5%
73 	Criminal justice 	5.5% 	74.4% 	$33,000 	$60,000 	21.8%
Financial value of skilled trades

Wages and expected job openings of skilled trades with educational requirements ranging from an associate degree to a high school diploma. Data is from the Bureau of Labor Statistics.[124]
Occupation 	Required education 	Median annual wage in May 2010
	Projected job openings from 2010 to 2020
	Required work experience in a related occupation
	On-the-job training
Air traffic controller 	Associate degree 	$108,040 	10,200 	None 	Long-term on-the-job training
General manager or Operations manager 	Associate degree 	$94,400 	410,100 	1 to 5 years 	None.
Construction management 	Associate degree 	$83,860 	120,400 	More than 5 years 	None
Radiation therapist 	Associate degree 	$74,980 	6,700 	None 	None
Nuclear medicine technologist 	Associate degree 	$68,560 	7,500 	None 	None
Dental hygienist 	Associate degree 	$68,250 	104,900 	None 	None
Nuclear technician 	Associate degree 	$68,090 	3,300 	None 	Moderate-term on-the-job training
Registered nurse 	Associate degree 	$64,690 	1,207,400 	None 	None
Diagnostic medical sonographer 	Associate degree 	$64,380 	31,700 	None 	None
Aerospace engineering technologist 	Associate degree 	$58,080 	1,700 	None 	None
Engineering technologist 	Associate degree 	$58,020 	16,800 	None 	None
Electrical engineering technician or Electronics engineering technician 	Associate degree 	$56,040 	31,800 	None 	None
Radiologic technician 	Associate degree 	$54,340 	95,100 	None 	None
Funeral service manager, director, mortician, or undertaker 	Associate degree 	$54,330 	10,700 	None 	Apprenticeship
Respiratory therapist 	Associate degree 	$54,280 	52,700 	None 	None
Geological and petroleum technician 	Associate degree 	$54,020 	7,000 	None 	Moderate-term on-the-job training.
Electrical and electronics drafter 	Associate degree 	$53,020 	7,200 	None 	None
Occupational therapy assistant 	Associate degree 	$51,010 	16,800 	None 	None
Precision instrument and equipment repairer 	Associate degree 	$50,910 	5,500 	None 	Long-term on-the-job training
Mechanical engineering technician 	Associate degree 	$50,110 	10,400 	None 	None
First-line supervisor of firefighters and fire prevention workers 	Postsecondary non-degree award 	$68,240 	33,100 	1 to 5 years 	None
Commercial pilot 	Postsecondary non-degree award 	$67,500 	19,300 	None 	None
Electrical and electronics repairer (powerhouse, substation, and relay) 	Postsecondary non-degree award 	$65,230 	6,900 	None 	Long-term on-the-job training
Insurance appraiser (automotive) 	Postsecondary non-degree award 	$56,230 	2,700 	None 	Moderate-term on-the-job training
Telecommunications equipment installer and repairer, except line installer 	Postsecondary non-degree award 	$54,710 	59,300 	None 	Moderate-term on-the-job training
Aircraft maintenance technician 	Postsecondary non-degree award 	$53,420 	45,200 	None 	None
Railroad switch and signal repairer 	Postsecondary non-degree award 	$53,230 	1,300 	None 	Moderate-term on-the-job training
First-line supervisor of production and operating workers 	Postsecondary non-degree award 	$53,090 	87,900 	1 to 5 years 	None
Avionics technician 	Postsecondary non-degree award 	$52,320 	5,800 	None 	None
Electrical and electronics repairer (commercial and industrial equipment) 	Postsecondary non-degree award 	$51,820 	17,700 	None 	Long-term on-the-job training
Commercial diver 	Postsecondary non-degree award 	$51,360 	1,300 	None 	Moderate-term on-the-job training
Manager 	High school diploma 	$96,450 	249,400 	1 to 5 years 	None
Transportation, storage, and distribution manager 	High school diploma 	$80,210 	33,700 	More than 5 years 	None
First-line supervisor of police and detectives 	High school diploma 	$78,260 	38,700 	1 to 5 years 	Moderate-term on-the-job training
Administrative services manager (payroll, benefits, etc.) 	High school diploma 	$77,890 	99,800 	1 to 5 years 	None
Nuclear power reactor operator 	High school diploma 	$75,650 	2,000 	None 	Long-term on-the-job training
Elevator mechanic 	High school diploma 	$70,910 	8,200 	None 	Apprenticeship
Power station distributor and dispatcher 	High school diploma 	$68,900 	3,600 	None 	Long-term on-the-job training
First-line supervisor of non-retail sales workers 	High school diploma 	$68,880 	123,500 	More than 5 years 	None
Detective 	High school diploma 	$68,820 	30,100 	1 to 5 years 	Moderate-term on-the-job training
Fashion designer 	High school diploma 	$64,530 	6,700 	None 	Long-term on-the-job training
Power station operator 	High school diploma 	$63,080 	14,400 	None 	Long-term on-the-job training
Business operations specialist 	High school diploma 	$62,450 	327,200 	Less than 1 year 	Long-term on-the-job training
Media and communication equipment worker 	High school diploma 	$61,680 	3,300 	None 	Moderate-term on-the-job training
Agriculture manager 	High school diploma 	$60,750 	234,500 	More than 5 years 	None
Postmaster or mail superintendent 	High school diploma 	$60,300 	4,800 	1 to 5 years 	Moderate-term on-the-job training
Oil refinery operator 	High school diploma 	$60,040 	14,400 	None 	Long-term on-the-job training
First-line supervisor of mechanics, installers, and repairers 	High school diploma 	$59,150 	164,900 	1 to 5 years 	None
Artists and related workers 	High school diploma 	$58,840 	4,800 	None 	Long-term on-the-job training
First-line supervisor of construction workers or natural resource extraction workers 	High school diploma 	$58,680 	259,700 	More than 5 years 	None
Claims adjuster, examiner or investigator 	High school diploma 	$58,620 	79,900 	None 	Long-term on-the-job training
Lineman 	High school diploma 	$58,030 	52,700 	None 	Long-term on-the-job training
Gas plant operator 	High school diploma 	$57,200 	4,500 	None 	Long-term on-the-job training
Rapid transit driver 	High school diploma 	$56,880 	2,800 	None 	Moderate-term on-the-job training
Purchasing manager 	High school diploma 	$56,580 	91,200 	None 	Long-term on-the-job training
Loan officer 	High school diploma 	$56,490 	115,200 	None 	Moderate-term on-the-job training
First-line supervisor of corrections officers 	High school diploma 	$55,910 	16,500 	1 to 5 years 	Moderate-term on-the-job training
Chemical plant operator 	High school diploma 	$55,490 	14,100 	None 	Long-term on-the-job training
Real estate broker 	High school diploma 	$54,910 	29,700 	1 to 5 years 	None
Boilermaker 	High school diploma 	$54,640 	11,800 	None 	Apprenticeship
Transit policeman or Railroad policeman 	High school diploma 	$54,330 	1,100 	None 	Short-term on-the-job training
Agricultural purchasing agent 	High school diploma 	$54,220 	3,200 	None 	Long-term on-the-job training
Mail carrier 	High school diploma 	$53,860 	103,400 	None 	Short-term on-the-job training
Police officer 	High school diploma 	$53,540 	249,400 	None 	Moderate-term on-the-job training
Indoor postal worker 	High school diploma 	$53,100 	15,500 	None 	Short-term on-the-job training
Postal service mail sorter, processor, or processing machine operator 	High school diploma 	$53,080 	7,500 	None 	Short-term on-the-job training
First-line supervisor of transportation and material-moving machine and vehicle operators 	High school diploma 	$52,720 	69,300 	1 to 5 years 	None
Sales representative, wholesale and manufacturing, except technical and scientific products 	High school diploma 	$52,440 	559,900 	None 	Moderate-term on-the-job training
Building code inspector 	High school diploma 	$52,360 	48,600 	More than 5 years 	Moderate-term on-the-job training
Fire marshal 	High school diploma 	$52,230 	4,700 	More than 5 years 	Moderate-term on-the-job training
Stationary engineer 	High school diploma 	$52,140 	10,600 	None 	Long-term on-the-job training
Plant or system operator 	High school diploma 	$51,980 	3,700 	None 	Long-term on-the-job training
Paralegal 	High school diploma 	$51,800 	9,600 	None 	Short-term on-the-job training
Property manager 	High school diploma 	$51,480 	82,300 	1 to 5 years 	None
Telecommunications line installer or repairer 	High school diploma 	$50,850 	51,400 	None 	Long-term on-the-job training
Sales representative 	High school diploma 	$50,620 	270,100 	None 	Short-term on-the-job training
Socioeconomic status

Socioeconomic status can play a significant role in an individual's enrollment, performance, and completion of their college degree and pursuit of higher education.

Enrollment

The National Center for Education Statistics reports that in 2009 high school graduates from low-income families enrolled in college immediately at a rate of 55 percent. In comparison, 84 percent of high school graduates from high-income families enrolled immediately into college. Middle-class families also saw lower rates with 67 percent enrolling in college immediately.[125] It also found that a high percentage of students who delayed enrollment in college attended high schools that had a high level of participation in the free and reduced lunch program. Students who work long hours in high school are less likely to pursue post-secondary education.[126] Students who had access to financial aid contacts were more likely to enroll in higher education than students who did not have these contacts.[127]

When considering how a college degree affects labor market outcomes, it is especially important to consider differences in socioeconomic status (SES). For example, research shows that students of low SES are more likely than their high SES peers to delay entering a college.[128] This delay can cause different effects for different students. For example, research shows that students who delayed at least one year after high school were 64% less likely to complete their degree as opposed to those who enroll immediately after high school.[129] In the same study, Bozick and DeLuca found that the average time delay for students in the lowest SES quartile was 13 months, while for students in higher SES quartiles averaged about 4 months.[129]

Research in the area of delayed college enrollment is not extensive, however, a clear theme emerges in that lower SES students constitute a much larger percentage of students that delay enrollment, while students of higher SES tend to enroll immediately after high school.[129][130][131] According to a similar study “an increase in family income of $10,000 decreases a student’s odds of planning to delay by about 3%, and having a parent with a bachelor’s degree decreases the odds of planning to delay by about 34%.”[128] This is significant, because by delaying enrollment low SES students are less likely to earn a college degree, and therefore they do not receive the benefits associated with completion.[129][130] For example, studies have shown that earning a bachelor's degree will increase lifetime earnings by 31% more than an associate degree, and 74% more than those with a high school diploma.[132] These benefits extend beyond lifetime income and include a lower incidence of poverty, higher likelihood of being insured, higher retirement income, higher job safety, higher life expectancy, lower probability of being jailed, and more.[132] Completing a college degree also provides a communal benefit. For example, for example a decrease in crime, greater philanthropic contributions, lower government expenditures, higher voter turnout, and greater participation in overall community spaces.[133] By delaying college enrollment, students of lower SES are less likely to reap these benefits.

Persistence & Performance

A 2011 national study found that college students with a high socioeconomic status persisted in college 25 percent more than students with a low socioeconomic status.[134] In fact, students with a high socioeconomic status are 1.55 times more likely to persist in college than students with a low socioeconomic status. Attaining even higher degrees than a bachelor's degree can also be affected by socioeconomic status. A 2008 study reports that 11 percent of students with low socioeconomic status report earning a master's, medical, or law degree compared to 42 percent of high socioeconomic students.[135] Analyst Jeffrey Selingo wondered whether higher education had less and less ability to level the playing field.[2] A 2007 study found that 52 percent of low-income students who qualified for college enrolled within 2 years of graduation compared to 83 percent of high-income students.[127]

Socioeconomic status can also influence performance rates once at a university. According to a 2008 study, students with a low socioeconomic status study less, work more hours, have less interaction with faculty, and are less likely to join extra-curricular activities. Forty-two percent of students with low socioeconomic status indicated that they worked more than 16 hours a week during school, with a high percentage working up to 40 hours a week,[136] although such students may benefit since potential employers assign great importance to a graduate's work experience.[137] This is also evidence of a positive relation between socioeconomic status and social integration at university. In other words, middle-class students take part in more formal and informal social activities and have a greater sense of belonging to their universities than do working-class students.[138]

Completion

Suzanne Mettler notes in her book, Degrees of Inequality, that in 1970 40% of US students in top income quartile had achieved a bachelor's degree by the age of 24.[139] By 2013, this percentage rose to 77%. For students in the bottom income quartile, only 6% had earned a bachelor's degree in 1970. By 2013, this percentage was still at a marginal 9%. Unfortunately, there have been and continue to be many barriers for students of lower socioeconomic status to get access. There are certain organizations and programs that have capitalized on the idea that attaining a college degree, specifically at a top tier university, is critical to social mobility. Organizations like QuestBridge, a non-profit focused on helping students of low socioeconomic status and minority background, have helped historically underrepresented groups attain a significant degree of social advantage. QuestBridge is partnered with the nation’s top 38 colleges and has helped over 13,000 students gain entrance into these universities since 2004. However, even these accomplishments are minuscule, when we recognize that there are between 25,000 and 35,000 low income students that are qualified to gain entrance into the nation’s top universities each year, but do not even apply.[140] By these standards, QuestBridge has served less than 4% of the students that they focus on.
Race
Further information: Jewish quota and Asian quota

Since the 1950s, the United States higher education system has been increasingly viewed as a vehicle for social mobility and economic equality. As a result, there has been a clear struggle to try and open access to higher education for the wider population so that more individuals can benefit from this economic good. Programs like Affirmative Action have been at the forefront of this struggle, to help under-represented racial groups gain greater access to higher education. By increasing access to diverse and minority populations, greater social mobility is expected. However, a recent report by the Georgetown’s Center on Education and the Workforce demonstrates that progress has not yet been adequately made. The report actually makes clear that higher education has been a source of increasing racial inequality in the United States. The authors, Anthony Carnevale and Jeff Strohl, focused on Latinos and African American minority groups. Through their research they show that overall access for minority enrolment has increased at a greater rate than enrolment for white students, but this growth is heavily concentrated in the poorest, and least selective colleges and universities.[141]

This difference is very important to note, because growing inequality between universities has an effect on graduation rates and time to complete a degree for students. The study shows that more selective universities provide their students with better resources.[141] The authors show that the 82 most selective colleges spend $27,900/student on average, while the least selective open access two- and four-year colleges (where Latinos and African Americans are over-represented) spend $6,000/student on average.[141] Open-access colleges, are colleges that admit at least 80% of their students and typically include community colleges, for-profit schools, and some public universities. Unsurprisingly, graduation rates are the highest in the more selective universities, where more resources are available to students inside and outside of the universities. They further demonstrate that persistence and completion rates at more selective universities are higher regardless of race or ethnicity. The end product of this is the increased reproduction of educational inequalities across generations.[141] Furthermore, consider that roughly 34% of African American and Hispanic students who earn bachelor's degrees at the top 468 colleges attain graduate degrees, compared with 23 percent who attend open-access colleges.[141] The effect that top colleges have on minority students not only leads to better graduation rates for undergraduate students, but also makes the completion of a graduate education more likely.

It is very important to note that this data presents a serious challenge to proponents of “mismatch” theory. This theory was advocated very heavily during the Supreme Court Case, Fisher v. University of Texas in 2013. It claims that Affirmative Action causes more harm than benefit, because it provides access opportunities to students that are not prepared well enough to succeed at more elite institutions.[142] Proponents of "mismatch" theory argue that Affirmative Action should therefore be repealed, because it is not achieving its intended effect. This data shows that the opposite is true, in that many minority students are attending less selective universities, and are therefore not being given enough resources to succeed. These resources include, "substantial labor market advantages, including more than $2 million dollars per student in higher lifetime earnings, and access to professional and managerial elite jobs, as well as careers that bring personal and social empowerment."[141] The issue is not that they are attending more selective universities and failing, its that they are attending the least selective universities that are overcrowded and underfunded.

Enrollment

Race can also play a role in which students enroll in college. A 2007 study found that African Americans are more likely to delay enrolling in college.[127] The National Center for Education Statistics reports that between 2003 and 2009 rates of immediate college enrollment increased for Asian Americans and whites, but not for African Americans.[125] The 2011 Condition of Education study found that in 2008, 63% of college students were white, while 14 percent were African American and 12 percent were Hispanic.[125] Minority groups tend to remain the most underrepresented at more selective universities. This is despite programs like Affirmative Action that seek to provide underrepresented students with greater access to colleges. According to the National Center for Education Statistics, African American students suffer the most in regards to under-representation at more selective universities.[143] Consider that the cumulative percent change for African American students at open access universities has increased by 113.6% since 1994, but that at top tier universities it has barely changed, having gone down by 0.3%.[144] At Harvard, 6.5% of undergraduates were black in 2013, while it was 7.4% in 1994.[143] At universities focusing on bachelors, and graduate degrees African American enrollment in 2013 had only increased by 3% since 1994.

According to the Pew Research Center, Hispanic students college enrollment has increased by 240% since 1996, more than their African American or White counterparts.[145] However, this growth is similarly at the open access colleges and does not translate into enrollment at four-year colleges. A study by the Pew Research Center, claimed that "Young Hispanic college students are less likely than their white counterparts to enroll in a four-year college (56% versus 72%), they are less likely to attend a selective college, less likely to be enrolled in college full time, and less likely to complete a bachelor’s degree."[146] Given this information, it is clear that increased college enrollment may not mean that Hispanic students are reaping the benefits of completing a college degree.

Miscellaneous Issues

Race can play a part in a student's persistence rate in college: Drop-out rates are highest with the Native American and African American population, both greater than 50 percent.[134] Caucasians and Asian Americans had the lowest dropout rates. Another issue related to race is faculty representation at universities. According to data from the U.S. Department of Education, full-time faculty remain heavily white at universities across the country. In 2013, 78% of full-time faculty members nationwide were white.[147]
Gender

In discussing student's access to education in the United States, one area of concentration that current research has focused on in the last half century is the differences that exist between students entry and completion rates based on gender. In a study done by Bailey & Dynarski (2011) it was observed that the increase in inequality that has been observed in the last 40+ years has been predominantly driven by women.[148] In 2010-2011, 33% more bachelor's degrees were conferred on females than males.[149] This gender gap is projected to increase to 37% by 2021-2022, and is already over 50% for master's and associate degrees.
Degrees conferred in United States since 1970 by year, degree type, and gender. Dashed lines are projected. Since 1982 more bachelor's degrees have been conferred on women.[1]

Within higher-income families that are sending more children to universities and colleges, women make up a greater percentage (15% compared to 7%) of this growth. While the largest gap of educational attainment between men and women is seen in the highest income group, women are attaining higher levels of education than men in every income group. This observation poses a unique and confusing problem: if educational attainment has a positive correlation to familial income, why are more women entering and completing college than men? Bailey and Dynarski proposed that the observed educational gap by gender may be due to differing incentives to accumulate human capital. Men and women may participate in what they term "segregated labor markets" and "asymmetric marriage markets," and perhaps, to make up for those perceived market differences, females are more motivated to obtain higher levels of education.[148]

The gap of educational attainment between men and women is starting at a young age and affecting students access to higher education later on in life. According to Bailey & Dynarski, there are two main explanations for the gender differences in educational attainment and inequality. First, men and women respond in different and gender-specific ways to family and/or school circumstances, and second, the differences in circumstances across men and women of the same family income and race have shaped inequality in educational attainment for some time. More specifically, the bulk of primary and secondary teachers are female and women run most single parent households. The absence of a strong male role model affects males differently from females. Studies by Bailey & Dynarski have shown that teachers provide role models to demographically similar students, and their unintended biases affect their interactions and assessments of their students.[148]

When comparing graduation rates between men and women, in children born after 1960, more white women were graduating from college than white men, which was a change from children born before this time.[148]
Undocumented Students

It is estimated that 65,000 undocumented immigrants graduate from high school each year. These graduates have lived in the United States for more than 5 years and most were often brought to the United States by their parents as young children.[150] This leaves the U.S. Government with the question of what rights to give the undocumented immigrants after their graduation, particularly with access to higher education. A 2010 study conducted at the University of Nevada, Las Vegas (UNLV) on undocumented immigrants and higher education:

    Installing pathways to higher education and in-state tuition for undocumented students in the United States presents both opportunities and constraints in developing practices that promote social justice, equity, and equality. Those who are sympathetic to the challenges facing undocumented students may support opportunities to promote the potential of those who are deserving of incorporation and membership in U.S. society. On the other hand, proponents of tighter borders and tougher immigration laws may view all undocumented people, including model, hardworking young people, as "illegals" or temporary workers and consider them to be drains on the resources of society. This puts educational administrators in precarious positions since they are professionals who are trained to promote and support students in their pursuit of knowledge and self-improvement. Therefore, many professionals are left with little choice but to search for individuals and resources already established within outlaw cultures." [151]

In 1996, the United States passed a law banning states from offering residency benefits to undocumented immigrants that they didn't then also offer to every U.S. citizen. This basically made it so that states could not offer in-state tuition to undocumented immigrants, even if they technically qualified based on residency status. States have argued the clarity of this law and many have enacted their own laws allowing in-state tuition to be given on the claims that it is based on high school attendance and not explicitly residency.[150] This law is especially important since undocumented immigrants are also unable to obtain governmental financial aid and are unable to legally work, leaving them without sources to help pay for out-of-state tuition.[151]

The DREAM Act was introduced in 2001 and aims to give more access to higher education for undocumented immigrants by repealing the law 1996 law. It also aimed to set up pathways for students who obtain higher education to become legal residents. The act has been introduced in many states and many different times, but has still not been passed. Critics of the act argue that it encourages more undocumented immigration, that schools will engage in grade inflation so that border-line students can take advantage of the act, and that a financial burden could be placed on taxpayers. Proponents argue the opposite, emphasizing that giving the undocumented immigrants an opportunity at higher education means they will be more self-sufficient in the future, contributing more to taxes and relying less on state resources. They also claim that children should not be punished for the actions of their parents and that giving them this opportunity would encourage them to be contributing and law-abiding citizens. Whether this act would have positive effects on undocumented immigrants attending college is still hard to see since not many states have actually done it and the time span has not been enough for thorough research.[150]

The 2010 UNLV study recommends key policy changes to support undocumented immigrants access to higher education.

    In general, practitioners need to weigh opportunities against constraints and consider the potential opportunities to promote social justice, equality, and equity in higher education access. Rather than considering undocumented students as "illegals" and restricting their access to legitimate educational pathways, it is recommended that, at the very least, those in positions of power adopt an outlaw cultural framework to support the strengths inherent within diversity as well as pursue avenues of social justice for undocumented students who are seeking to access higher education to improve their future and secure permanent membership in U.S. society.[151]

MOOC
Main article: MOOC

A MOOC is a massive open online course aimed at unlimited participation and open access via the web. It became popular in 2010-2014. In addition to traditional course materials such as filmed lectures, readings, and problem sets, many MOOCs provide interactive user forums to support community interactions between students, professors, and teaching assistants.[152] Robert Zemsky (2014) notes that they at first seemed to be an extremely inexpensive method of bringing top teachers at low cost directly to students. However, very few students—usually under 5%--were able to finish a MOOC course. He argues that they have passed their peak: "They came; they conquered very little; and now they face substantially diminished prospects."[153]
Criticism
Cost and finances

Critics contend that tuition increases have outpaced inflation. Because schools are assured of receiving their fees no matter what happens to their students, they have felt free to raise their fees to very high levels, to accept students of inadequate academic ability, and to produce too many graduates in some fields of study. Despite the vast expense and economic distortions that result from student aid, the proportion of graduates who come from poor backgrounds has actually declined since 1970.[154] Analyst Robert E. Wright predicted cost increases without matching increases in quality would continue until professors were encouraged to own colleges in private partnerships; he predicted that would not happen until barriers to entry are decreased and government education subsidies are paid directly to students instead of to colleges and universities.[155] A report in The Economist criticized American universities for generally losing sight of how to contain costs.[156] Analyst Jeffrey Selingo in the Chronicle of Higher Education blamed rising costs on unnecessary amenities such as private residence rooms, luxury dining facilities, climbing walls, and sometimes even so-called lazy rivers similar to ones found in amusement parks.[2] The 2014 documentary Ivory Tower described colleges as participating in an "arms race" to provide the best luxury facilities, and asked whether college was worth the expense in an era of "predatory loan systems" and job scarcity and rampant inequality.[157] One analyst argued that second-tier schools with Ivy League Envy had become "so obsessed with rising up the academic hierarchy" that they focused too heavily on research while neglecting undergraduate education, and argued that schools should embrace Internet technology and online software to streamline costs.[156]
Amenities such as a lazy river at a dorm at the University of North Florida are reputed to be driving up costs for undergraduate education.[2]

Another issue is the rising cost of textbooks.[158] There are textbook exchanges for students who will accept a used text at a lower price. Lower priced alternatives offered by Flat World Knowledge are now available but have yet to make a significant impact on overall textbook prices.

The total cost of all higher education in 2002 was $289 billion.[159]

One theory for the continual increase in tuition is that universities prioritize endowment growth over educational interests.[160] A possible explanation for this is that universities are concerned with intergenerational equity for the benefit of future generations of students, as well as the overall benefit to society. This means that the universities will usually seek to grow their endowments to sustain their level of activity well into the future. Arguments against this justification mainly focus on the idea that the intergenerational equity theory does not accurately reflect the behavior of institutions with large endowments. Peter Conti-Brown, for example, describes how many of the elite universities cut their budgets during the recession despite sitting atop multibillion-dollar endowments, which were theoretically supposed to act as cushions during such economic downturns.[161]

Still, tuition increases may not be completely the responsibility of the higher education institutions. Instead, an article written by Archibald and Feldman suggests that tuition increases simply reflect the increasing costs of producing higher education.[162] According to the cost-disease theory, it would be difficult to achieve cuts in per-student cost without the deterioration of quality in the education. While the decision-making of college administrators does come into play, the argument is that there are more fundamental and economy-wide factors that result in cost increases. A general economic trend is that costs in service industries grow more rapidly than in manufacturing industries, and increase in higher education costs is simply a reflection of this phenomenon. Some universities describe being caught in a dilemma where they are pressured to offer broader curricula and improve facilities to attract new students on one hand, but on the other hand these universities must raise tuition to compensate for state spending cuts and rising expenses.[163]

Annual undergraduate tuition varies widely from state to state, and many additional fees apply. Listed tuition prices generally reflect the upper bound that a student may be charged for tuition. In many cases, the "list price" of tuition-—that is, the tuition rate broadcast on a particular institution's marketing platforms—-may turn out to be different from the actual (or net) tuition charged per student. A student that has applied for institution-based funding will know his or her net tuition upon receipt of a financial aid package. Since tuition does not take into account other expenses such as the cost of living, books, supplies and other expenses, such additional amounts can cause the overall cost of college to exceed the tuition rate multiplied by the number of courses the student is planning to take.[164]

In 2009, average annual tuition at a public university (for residents of the state) was $7,020.[165] Tuition for public school students from outside the state is generally comparable to private school prices, although students can often qualify for state residency after their first year. Private schools are typically much higher, although prices vary widely from "no-frills" private schools to highly specialized technical institutes. Depending upon the type of school and program, annual graduate program tuition can vary from $15,000 to as high as $50,000. Note that these prices do not include living expenses (rent, room/board, etc.) or additional fees that schools add on such as "activities fees" or health insurance. These fees, especially room and board, can range from $6,000 to $12,000 per academic year (assuming a single student without children).[166] Such fees are not at all government-regulated, allowing a theoretically enormous increase each year. While tuition is monitored to some degree in legislatures and is often publicly discussed, fees on the side are frequently overlooked in public opinion and regulatory policies.[167] Although tuition costs have risen, the rising costs have had little effect on transfer rates and overall enrollment. In a study on effects of rising tuition costs, analysis revealed that the rising costs of colleges have “weak or no effects” on enrollment. Rising tuition costs have not deterred enrollment “as long as students believe the potential return of a college education is much greater than the cost”.[168]

In addition to tuition, living expenses, books, supplies and fees, students also face a less-acknowledged opportunity cost in years of missed potential income. A high school educated person could expect to earn about $84,000 for four years of work; in choosing to attend and pay for college, an individual forgoes those earnings.[169]
Study comparing college revenue per student by tuition and state funding in 2008 dollars.[170]

In 2010, community colleges cost an average of $2,544 per year for tuition and fees. A private four-year college cost an average of $26,273 annually for tuition and fees.[171]

College costs are rising while state appropriations for aid are shrinking. This has led to debate over funding at both the state and local levels. From 2002 to 2004 alone, tuition rates at public schools increased by just over 14 percent, largely due to dwindling state funding. A more moderate increase of 6 percent occurred over the same period for private schools.[166] Between 1982 and 2007, college tuition and fees rose three times as fast as median family income, in constant dollars.[172] In the 2012 fiscal year, state and local financing declined to $81.2 billion, a drop in funding compared to record-high funding in 2008 of $88 billion in a pre-recession economy.[173]

To combat costs colleges have hired adjunct professors to teach. In 2008 these teachers cost about $1,800 per 3-credit class as opposed to $8,000 per class for a tenured professor. Two-thirds of college instructors were adjuncts, according to one estimate; a second estimate from NBC News in 2013 was that 76% of college professors were in "low-paying, part-time jobs or insecure, non-tenure positions," often lacking health insurance.[174] There are differences of opinion on whether these adjuncts teach more or less effectively than regular tenured or tenure-track professors. There is some suspicion that student evaluation of adjuncts, along with doubts on the part of teachers about subsequent continued employment, can lead to grade inflation.[175]

Additionally, schools are increasingly using price discrimination as a strategy across different programs to increase revenue (i.e., employing strategies like a for-profit business). Yet the school is still fundamentally different from a for-profit business entity in that it is restricted by its school mission. For example, a school may charge particular types of students (such as low-income or moderate-income students) less tuition in order to help them. Another example is merit-based aid, in which the school will grant high-achieving students money.[176]

Because of the decrease in public funding, public research universities have tried to compensate for those losses by increasing tuition revenue by enrolling more out of state students.[177] According to a 2011-2012 survey the average in state tuition was $8,775 while the out of state tuition was $27,539.[178] On average the increase in non resident enrollments has gone up from 20.7% of total freshman enrollment in 2003 to 24.7% in 2013. In some states the increase has been significantly higher, particularly in higher ranking universities. In the University of California Los Angeles the enrollment went up from 7.7% in 2003 to 28.5% in 2013.[178] In state students that would have previously been accepted at that high ranking university where no longer able to attend.[177] Aside from compensating for the decreases in funding, the increase in out of state admission has also allows universities to address the ever-present concern in rankings as they are able to increase the academic requirements for admission due to the rising number of applicants. In higher ranking universities the increases in out of state admissions has had a significant effect on admission of in state low income and underrepresented minority students.[177][178]

Princeton sociologists Thomas Espenshade and Alexandria Walton Radford published a book-length study of admissions that found that an upper-middle-class white applicant was three times as likely to be admitted to an American college as a lower-class white with similar qualification.[179] New York Times columnist Ross Douthat has cited this as an example of how U.S. universities can exacerbate wealth inequality.[180] A 2006 report by Future of Children, a collaboration of Princeton and the Brookings Institution, concluded that "the current process of admission to, enrollment in, and graduation from colleges and universities contributes to economic inequality as measured by income and wealth."[181] According to Suzanne Mettler of Cornell, government policy towards higher education has an effect of deepening inequality and disadvantaging students from the lower classes.[182]

Athletics have been increasingly subsidized by tuition. Fewer than one in eight of the 202 Division 1 colleges actually netted more money than they spent on athletics between the years 2005 and 2010. At the few money-making schools, football and sometimes basketball sales support the school's other athletic programs. Athletes, on average, cost six times what it cost to educate the non-athlete. Spending per student varied from $10,012 to $19,225; while the spending per athlete varied from $41,796 to $163,931.[183]

Issues related to financial aid
Main article: Student financial aid in the United States

The portion of state budget funding spent on higher education has decreased by 40 percent since 1978, while at the same time most tuition fees have significantly increased.[134] Between 2000 and 2010, the cost of tuition and room and board at public universities increased by 37 percent.[184] There is a misconception that there was no similar increase in financial aid to help cover the costs of tuition. This is incorrect. In 1965, $558 million was available for financial aid. In 2005 more than $129 billion was available. As college costs have risen, so has the amount of money available to finance a college education. However, the proportion of gift aid and self-help funding has shifted: loans and work make up a larger percentage of aid packages than they once did.[37] During the early 1980s, higher education funding saw a shift from reliance on state and federal government funding to a greater reliance on family contributions and student loans. Pell Grants, which were created to offset the cost of college for low-income students, started funding more middle-class students, stretching the funds thinner for everyone. During the mid-1990s 34 percent of the cost for college was covered by the maximum offered Pell Grant, compared to 84 percent during the 1970s.[185]

During Clinton's presidency, funding for higher education was focused on creating tax benefits tied to attending college. These proposed policies put less emphasis on developing grants to allow students to attend college. Some have argued that this approach did not adequately provide aid to those students most in need of it. Furthermore, there was fear that tax deductions or credits would actually work to drive up tuition costs.[186]

The federal government also began funding fewer grant programs and more loan programs, leaving students with higher amounts of debt. In 2003, almost 70 percent of federal student aid awarded was student loans, which was a much higher percentage than just a decade before.[185] In fact, the National Center for Education Statistics reports that during the 2007–2008 school year, 66% of degree recipients had borrowed money to complete their degree; 36% of these graduates had to borrow from state or private sources, averaging total loan amounts of $13,900; 95% of these loans were private. On average, a student borrowed $24,700.36 during the 2007–2008 school year.[187] One estimate of total debt of all ex-students in 2011 was $1 trillion.[156] Furthermore, the economic troubles of the recent decade have left higher education funding shifted toward other needs because higher education institutions have the ability to gain extra funds through raising tuition and private donations.[188]

Policy changes in higher education funding raise questions about the impact on student performance and access to higher education. Many early studies focused on social integration and a person's individual attributes as the factors for degree completion.[134] More recent studies have begun to look at larger factors including state funding and financial support. It has been found that providing need-based aid proved to increase degree completion in 48 states. There has also been a positive correlation between providing merit-based aid and degree completion.[134] Also, as the level to qualify for state need-based aid is lowered, the probability of persistence increases. Low-income families now have to pay more to attend college, making it harder for such populations to attain higher education. In 1980, low-income families had to use 13 percent of their income to pay for one year of college. In 2000, this proportion grew to 25 percent of their income, while high-income families use less than 5 percent of their income.[185] Thus, fully understanding how need and merit (non-need) aid is determined is critical when looking to ensure greater access to higher education. It is clear that at both private and public colleges and universities family income has a significant impact on need-based financial aid. As colleges and universities compete for students, the demarcation between merit-based aid and need-based aid is less clear. While there has been a traditional distinction between need-based and merit-based funding, recent trends indicate that these two categories are more blurred than their labels would suggest. Specifically, research confirms that merit-based financial aid often takes into account student need and vice versa.[189]

Controversy has also risen regarding performance-based funding. Performance-based funding is a system in which the state’s higher education budget is allocated to various institutions by several measures to best determine allocation of funds. This system has been criticized due to the complexity of the measurements as well as the resulting changed environment and goals of campuses. Many have criticized performance-funding, noting an overemphasis of test scores without consideration of other possible measures.[190]

A 2006 report by Michael S. McPherson and Morton Owen Schapiro indicated that financial aid to students in the 1990s held the strongest correlation with student SAT scores. The report was conducted in the interest of looking directly at the relationship between financial aid grants and various factors, with specific focus on the variables of family income level and SAT scores and minor focus on personal variables, such as race and gender. The reason these factors were given greater consideration was that, according to McPherson and Schapiro, the information was readily available and it led to a more meaningful comparison across students than variables like high school GPA. The report also made clear that it ignored the distinctions that universities make between "need-based" and "merit-based" aid. McPherson and Schapiro argued, "Although it is commonplace to track the importance of merit as opposed to need-based aid based on the responses given by college and university administrators on survey forms, we have argued that the distinction between 'need-based' and 'non-need-based' student grants is a slippery one."[191] The findings in the report indicated that "the principle of awarding financial aid strictly in relation to ability to pay is becoming an increasingly less important factor in the distribution of aid in America's private colleges and universities."[191]

Some low-income students have to work and study at the same time. This may adversely impact their performance in school.[192]

Most discussions on how higher education funding is determined have focused on the economic and demographic influences; however, according to a 2010 study on the relationship between politics and state funding many political factors influence higher education funding. First, as the number of interest groups for higher education in a state grows, so does the amount of money given to higher education. Second, states with a more liberal political ideology give more funding to higher education. Third, governors with more control over the state budget tend to award less money to higher education. This is attributed again to the fact that higher education funding is considered to be tradable with other programs. Fourth, a more professional state legislature correlates with more funding for higher education. (Professional in here refers to a legislature that acts much as the U.S. Congress does in that members have many staff members and spend more time in session.) Fifth, the more diverse a state population becomes, the less support there will be for higher education funding.[188]
For-profit schools

From 1972 to 2009, there was rapid growth of for-profit schools. Government funding in 1972 and government deregulation in 1998 fueled a dramatic rise in for-profit college enrollment. Government oversight and scrutiny since 2010 as well as competition from non-profit and public education has led to a dramatic decrease in enrollment.

At its peak, The University of Phoenix was the largest US for-profit college, with an enrollment of more than 500,000 students nationwide. Other large institutions included Devry University, ITT Technical Institute, the Art Institutes, Kaplan University, Ashford University, Colorado Technical Institute, Ashford University, Strayer University, Lincoln Tech, and Walden University.[193][194]

Altogether, at their peak, for-profit colleges enrolled about 11% of the students, but created approximately 47% of all the student loan defaults.[195]

Critics of for-profit colleges have pointed to the heavy dependence on federal loans and grants to students, the low student completion rate, and the inability of the majority of graduates to pay their student loans because they failed to secure high-paying jobs.[196]

For-profit colleges have aggressively recruited among military veterans, and in 2010 received 36% percent of all the tuition aid paid by the federal government. The University of Phoenix received 88% of its income from federal aid to students; the maximum allowed is 90%.

The National Center for Education Statistics reported a 52 percent rate of default on student loans at for-profit colleges. [197]The 12-year student loan default rate for African Americans is 65.7 percent. [198]
Student Loan Debt

The amount of debt that students have after graduation has become an issue of concern, especially given the weak job market after 2008.[199][200][201] Nearly all loans are financed by the federal government at an artificially low rate,[202] but students sometimes obtain private loans (which generally have higher interest rates and start accumulating interest immediately). In 2010, the U.S. Department of Education announced stricter eligibility rules for federal financing of loans to student at for-profit schools, which were experiencing higher default rates.[203] Student loans total more than $1.3 trillion, averaging $25,000 each for 40 million debtors. The debtors average age is 33. Forty percent of the debt is owed by people 40 or older.[202] A 2013 poll by NBC News found that more than 40% of college graduates from 2011 to 2012 were underemployed, and that some were "heavily in debt because of the cost of their education."[204]

In a 2017 report by the National Center for Education Statistics, the researchers found that 27% of all student loans resulted in default within 12 years. [197]Children in poor families were particularly vulnerable, still maintaining an average balance that was 91% of the original loan. [205]
Graduation rates

Six years after entering a four-year program, 58% of students at public colleges will have graduated, 65% of students at private non-profit colleges will have graduated, while 27% of students at for-profit colleges will have graduated. Six-year graduation rates of four-year programs depend to a great extent on a college's entrance requirements, ranging from 89% at those which accept less than one-quarter of applicants to 36% at those with an open admissions policy.[206]
Academic standards

Grade inflation has been a pernicious aspect of American college life since the 1960s. Between 1965 and 1975, GPAs sharply increased so that the most common letter grade went from a long-standing C to a B. Since the mid-1990s it has been an A. On average, private colleges have been more subject to this phenomenon than public colleges, as have the humanities compared to STEM courses, post-graduate courses compared to undergraduate courses, and courses taught by women compared to courses taught by men. Although standardized tests are certainly imperfect measures of aptitude, comparing trends in scoring with those in grades is revealing. Unlike GPAs, overall test scores have remained relatively steady over time, demonstrating that the grade inflation is artificial. Graduate literacy has also remained constant. A graduate may take pride in having a straight-A transcript, but his or her potential employers know that factors such as internships, work experience, choice of major, volunteering, choice of extracurricular activity and relevance of coursework are all more reliable indicators of aptitude and attitude.[207][208][209]

In 1961, the average full-time student at a four-year college studied for about twenty-four hours per week, while his modern counterpart in all demographic subgroups averages only fourteen hours per week. This cannot be explained by technological innovations such as the internet, since most of the decline predates the innovations that are most relevant to education. The most plausible explanation for these findings is a general decline in academic standards. Longitudinal data indicate that the few students who take full academic advantage of their time in college earn more in the long run.[210]

In Academically Adrift, Richard Arum and Josipa Roksa draw on transcript data, the Collegiate Learning Assessment, and survey responses from more than 2,300 undergraduates at twenty-four institutions in their first semester and again at the end of their second year. Their analysis reveals that 45 percent of these students demonstrated no significant improvement in a range of skills — including critical thinking, complex reasoning, and writing — during their first two years of college.[211]

Financial pressures have made college administrations increasingly reluctant to lose the tuition obligations of students who might otherwise be failed or expelled, and to fill their classrooms they must accept students who will certainly not be able to complete a four-year degree in four years. Disruptive, immature or otherwise irresponsible behavior on the part of some of these students can impede the learning experiences of other students.[212]

When employers in any profession consider hiring a college graduate, they are looking for evidence of critical thinking and analytical reasoning skills, teamworking skills, information literacy, ethical judgment, decision-making skills, skill in speaking and writing clearly and concisely, problem solving skills, and a wide knowledge of liberal arts and sciences. However, employers consider the average graduate to be more or less deficient in all of these areas.[213]

While the traditional approach to pedagogy in higher education focuses on the teacher's responsibility, J. Scott Armstrong argues that students have a "natural learning" ability. They should take responsibility for their learning. The teacher-centered approach inhibits learning.[214]
Political views
See also: Liberal bias in academia, Academic bias, and Universities and antisemitism

Research since the 1970s have consistently found that professors are more liberal and Democratic than the general population.[215][216][217][218] Surveys conducted in the last 10 years indicate that between 44%-62% faculty self-identify as liberal, while 9%-18% self-identify as conservative. Conservative self-identification is higher in two-year colleges than other categories of higher education but has been declining overall.[219] Those in natural sciences, engineering, and business were less liberal than those in the social sciences and humanities. A 2005 study found that liberal views had increased compared to the older studies. 15% in the survey described themselves as right of center. While the humanities and the social sciences are still the most left leaning, 67% of those in other fields combined described themselves as left of center. In business and engineering, liberals outnumber conservatives by a 2:1 ratio. The study also found that more women, practicing Christians, and Republicans were employed to teach at lower ranked schools (such as two-year community colleges or medium-sized universities) than would be expected from their professional accomplishments, measured objectively.[220][221] One conservative critic has suggested that liberal "Groupthink" explains why liberals appear to be overrepresented.[222]

A 2007 study criticized some recent surveys, such as the above 2005 study, on methodological grounds as well as being motivated by conservative concerns. It also pointed to the influence of conservative think tanks outside academia. In its own survey, it found that while conservatives were rare, there was a large centrist group between those self-identifying as liberals or conservatives. More moderate views were more common in younger professors, although also in this age group liberals were several times more common than conservatives. The age group with most liberal professors were the professors who were teenagers or young adults in the radical 1960s. Of all surveyed, 3% identified themselves as Marxists with the highest numbers being in social sciences (17%) and humanities (5%).[223][224]

A 2011 study disagreed with younger professors being more moderate and instead argued that the average view may shift further left in the future. The study also found that the years of college education had little effect on the political view of undergraduates. There was little evidence that right leaning professors were treated poorly. Regarding the cause of the apparent liberal overrepresentation, it found that conservative students preferred to major in fields leading to immediate employment, such as hotel management or accounting, rather than further studies.[225][226] Self-selection has also been suggested by others as the main explanation.[227][228]

In a 2011 nationwide study conducted by the Higher Education Research Institute (HERI) of UCLA, it found that compared to its previous 2008 study of professors nationwide, that more professors in 2011 self-identified as liberal (50.3%) compared to in the 2008 study (47%).[229] In the study, 62.7% of responding professors self-identify as being politically liberal or "far left".[230] According to an article published in Academe, the impact of having more liberal professors meant that fewer conservative students were likely to pursue advanced or doctoral degrees.[231] According to Stephen Hayward, the fewer conservative professors results in fewer conservative students being mentored and supported to seek graduate level education, creating a "self-reinforcing" cycle.[232]

In one study the researchers sent out e-mails to graduate studies directors at top ranked departments. They claimed to be an undergraduate asking for guidance regarding if this was a suitable department. The e-mails differed regarding which presidential campaign the undergraduate had worked for. There was no statistical difference in the replies. On the other hand, a survey of sociology professors found that one quarter stated that they would be more likely to vote for hiring a declared Democrat and less likely to vote for hiring a declared Republican. Around 40% stated that they would be less likely to vote for hiring an Evangelical or a member of the National Rifle Association. Another survey found a similar situation for humanities and other social sciences professors.[233][improper synthesis?]

A 2007 poll found that 58% of Americans thought that college professors' political bias was a "serious problem". This varied depending on the political views of those asked. 91% of "very conservative" adults agreed compared with 3% of liberals.[234]

In 2012, Tilburg University psychologists Yoel Inbar and Joris Lammers conducted anonymous random surveys of 800 members of the Society for Personality and Social Psychology and found that 85% of respondents self-identified as liberal and 6% self-identified as conservative. Respondents that self-identified as either conservative or moderate were found to be significantly more reluctant to express their political views to their colleagues for fear of negative consequences, and were more likely to believe that their colleagues would actively discriminate against them on the basis of their political beliefs. Self-identified conservative respondents were also more likely to feel that there was a hostile climate directed towards their political beliefs, while self-identified liberal respondents did not believe that there was a hostile climate directed towards conservative beliefs, and the more liberal the respondent self-identified as being, the less likely they were to believe that there was a hostile climate. The respondents were also asked four questions assessing (along a 1-to-7 integer scale) their personal stated willingness to discriminate against conservatives, along with how willing they thought other members of their academic department would be to discriminate against conservatives. For all but one of the questions where the respondents were speaking for themselves, and for every question where the respondent was speaking for the rest of their department, the statistical mean for the responses was more than a standard deviation above the lowest option ("not at all" willing to discriminate). The percentage of responses from the middle to the highest option ("somewhat" to "very much" willing to discriminate) ranged from 14 to 44% for each question whether respondents were speaking for themselves or for their departmental colleagues, and the highest percentages for both were to the question about hiring a conservative job applicant in their academic department if the applicant was equally qualified with a liberal applicant. Also, the more liberal the respondent self-identified as being, the more they reported being willing to discriminate against conservatives on each question.[235]

In January 2015, a major literature review co-written by psychologists José L. Duarte, Jarret T. Crawford, Jonathan Haidt, Lee Jussim, Philip E. Tetlock and sociologist Charlotta Stern summarized numerous studies of how academic psychology has little ideological diversity, that the ratio of liberal-to-conservative or Democratic-to-Republican professors has dramatically increased since 1990, that the disparity is undermining the quality of research in psychology, and that the main causes of the lack of ideological diversity are self-selection, hostile climate, and discrimination.[236] In 2014, survey data from HERI indicated that the ratio of liberal-to-conservative college professors increased from 2:1 in 1995 to roughly 5:1 in 2014.[237][238] In 2016, psychologists Bill von Hippel and David Buss surveyed 335 members of the Society of Experimental Social Psychology (along a 1-to-11 integer scale) and found that 89.3% of respondents self-identified as liberal, 8.3% self-identified as centrist or moderate, and 2.5% self-identified as conservative. 94.7% of respondents voted for Barack Obama in the 2012 presidential election, 1.2% of respondents voted for Mitt Romney, and 4% of respondents voted for other candidates. After averaging the scores of the respondents on nine major political issues, 96% of respondents were categorized as left of center, 3.7% were categorized as centrist or moderate, and 0.3% were categorized as conservative.[239][240] In September 2016, a replication and extension of the 2012 Inbar and Lammers study conducted by psychologists Nathan Honeycutt and Laura Freberg surveyed 618 faculty members of four California State University campuses and confirmed the previous finding of a hostile climate towards conservative professors in academic psychology departments, but also extended their study to 76 other academic departments spanning agricultural, business, education, arts and letters, engineering, and science colleges and found that there are sizable percentages of professors willing to discriminate against conservative academics in every academic department that they surveyed.[241]
Professor Watchlist

In 2016, the conservative campus group Turning Point USA began publishing its Professor Watchlist to expose faculty who "discriminate against conservative students, promote anti-American values and advance leftist propaganda in the classroom."[242][243][244] Charlie Kirk, TPUSA's president, has criticized college campuses as “islands of totalitarianism” filled with liberal students and faculty members who force their worldview upon those around them.[245] Polls by Pew and Gallup in 2017 indicate that this belief is common among political conservatives.[13] Even some liberal professors are concerned that the polarization and hardening of political viewpoints on campus is hampering students' intellectual development.[246]
Geographic considerations

While many private liberal arts colleges are located in the Midwest and Northeast, population growth of 18-year-olds is strongest in the South and Southwest, making it more difficult to attract potential students to "fly halfway across the country" to get a degree, according to Jeffrey Selingo of the Chronicle of Higher Education.[2]
Skepticism about higher education

A 2017 poll funded by House Majority PAC found that white working-class voters were skeptical about higher education The key findings:

    57 percent said a college degree “would result in more debt and little likelihood of landing a good-paying job.”
    83 percent said a college degree was “no longer any guarantee of success in America.”

Declining accessibility and high cost

According to an analysis of social mobility and higher education in the US by Equality of Opportunity, "colleges that oﬀered many low-income students pathways to success are becoming less accessible over time."[247]

According to a Public Agenda poll, only 43 percent of Americans say private, nonprofit universities and colleges are worth the cost.[248]
Ending affirmative action

According to multiple sources, the Trump administration's Department of Justice will be conducting investigations to ensure that African Americans and Latinos are not favored over whites and Asians.[249][250]
Sexual assault

Campus sexual assault is a common and underreported crime.[citation needed] Among undergraduate students, 23.1% of females and 5.4% of males experience rape or sexual assault through physical force, violence, or incapacitation.[251] However, the Trump administration has rescinded Obama era measures to reduce sexual assault. [252]
See also

    Academic ranks in the United States
    Association of American Universities
    Campus carry in the United States
    Carnegie Classification of Institutions of Higher Education
    Center for Excellence in Higher Education
    College admissions in the United States
    Education in the United States
    G. I. American Universities
    Hispanic-serving institution
    Historically black colleges and universities
    History of education in the United States
    History of education in the United States: Bibliography
    Land-grant university
    Liberal arts colleges in the United States
    List of Roman Catholic universities and colleges in the United States
    Men's colleges in the United States
    Morrill Land-Grant Acts
    National Collegiate Athletic Association
    Transfer admissions in the United States
    Undermatching
    Women's colleges in the United States
    Work college

References

National Center for Education Statistics (December 2012). "Table 5 Number of educational institutions, by level and control of institution: Selected years, 1980-81 through 2010-11". U.S. Department of Education. Retrieved 21 May 2014.
NPR Staff, interview with Jeffrey J. Selingo, with David Greene, May 8, 2013, With Gorgeous Dorms But Little Cash, Colleges Must Adapt, Accessed May 9, 2013
[1][dead link]
"College and university enrollment is down for fifth straight year - The Hechinger Report". Hechingerreport.org. 19 December 2016. Retrieved 18 October 2017.
"Universities and colleges struggle to stem big drops in enrollment - The Hechinger Report". Hechingerreport.org. 29 June 2017. Retrieved 18 October 2017.
Phil Baty (16 September 2010). "The World University Rankings: Measure by measure: the US is the best of the best". TSL Education Ltd. Retrieved 12 December 2012.
"Several countries launch campaigns to recruit research talent from U.S. and elsewhere". Insidehighered.com. Retrieved 18 October 2017.
"Senate Appropriations Bill Cuts NSF Funding - Inside Higher Ed". Insidehighered.com. Retrieved 18 October 2017.
"Proposal on indirect costs would put research universities in an impossible situation (essay) - Inside Higher Ed". Insidehighered.com. Retrieved 18 October 2017.
Turnage, Clara (10 July 2017). "Most Republicans Think Colleges Are Bad for the Country. Why?". The Chronicle of Higher Education.
"Republicans don't trust higher ed. That's a problem for liberal academics". Latimes.com. 24 July 2017. Retrieved 18 October 2017.
Hartle, Terry W. (19 July 2017). "Why Most Republicans Don’t Like Higher Education". The Chronicle of Higher Education.
"New data explain Republican loss of confidence in higher education". Insidehighered.com. Retrieved 18 October 2017.
Rivlin, Gary (6 June 2014). "‘Degrees of Inequality,’ by Suzanne Mettler". Nytimes.com. Retrieved 18 October 2017.
Schuessler, Jennifer (18 October 2013). "‘Ebony and Ivy,’ About How Slavery Helped Universities Grow". Nytimes.com. Retrieved 18 October 2017.
"What is Liberal Arts Education?". Topuniversities.com. 28 January 2014. Retrieved 18 October 2017.
"Digest of Education Statistics, 2015". Nces.ed.gov. Retrieved 18 October 2017.
NCES (2014). "Degree-granting postsecondary institutions, by control and level of institution: Selected years, 1949-50 through 2012-13". U.S. Department of Education. Retrieved 1 April 2015.
"Education Longitudinal Study of 2002 (ELS:2002): A First Look at 2002 High School Sophomores 10 Years Later First Look" (PDF). NCES 2014-363. U.S. DEPARTMENT OF EDUCATION. January 2014. Retrieved 23 May 2014.
Jordan Weissmann, Atlantic Monthly, January 14, 2014, Highly Educated, Highly Indebted: The Lives of Today's 27-Year-Olds, In Charts: A new study by the Department of Education offers up a statistical picture of young-adult life in the wake of the Great Recession, Accessed Jan. 26, 2014
see "Knocking at the College Door, 9th Edition: Projections of High School Graduates" Dec 6, 2016 .
Marcus, Jon (29 June 2017). "Many small colleges face big enrollment drops. Here’s one survival strategy in Ohio.". Washingtonpost.com. Retrieved 18 October 2017.
Long, Heather. "U.S. college enrollment is dropping. Bad sign?". Money.cnn.com. Retrieved 18 October 2017.
"Number of colleges and universities drops sharply amid economic turmoil". Insidehighered.com. Retrieved 18 October 2017.
"Not Impartial : Examining Accreditation Commissioners’ Conflicts of Interest" (PDF). Economics21.org. Retrieved 18 October 2017.
Beth Frerking, Community Colleges: For Achievers, a New Destination, The New York Times, April 22, 2007.
"Community colleges examining low and stagnant enrollments". Insidehighered.com. Retrieved 18 October 2017.
Gail O. Mellow, August 28, 2017, New York Times, The Biggest Misconception About Today’s College Students, Retrieved August 28, 2017
"Basic Classification Technical Details". Carnegie Foundation for the Advancement of Teaching. n.d. Retrieved 2007-03-20.
Yoder, Dezarae (February 25, 2013). "Marijuana illegal on campus despite Amendment 64". The Scribe.
34 C.F.R. 86; specifically 34 C.F.R. 86.100
Media, American Public. "The Story of the University of Phoenix". americanradioworks.publicradio.org.
Armour, Stephanie; Zibel, Alan (13 January 2014). "For-Profit College Probe Expands". Wsj.com. Retrieved 18 October 2017.
[2][dead link]
[3][dead link]
Elite Colleges Struggle To Recruit Smart, Low-Income Kids 9 January 2013, Shankar Vedantam, NPR.org
College Board (2007). "3". Meeting College Costs: A Workbook for Families. New York: College Board.
Jennifer Levitz; Scott Thurm (20 December 2012), "Shift to Merit Scholarships Stirs Debate", The Wall Street Journal, pp. A1;A16
"FAFSA - Free Application for Federal Student Aid". FAFSA on the Web.
Lederman, Doug (22 Jan 2013). "What Student Aid Research Shows". Inside Higher Ed. Retrieved 2013-04-01.
Flaherty Manchester, Colleen (2012). "General human capital and employee mobility: How tuition reimbursement increases retention through sorting and participation". Industrial & Labor Relations Review. 65 (4): 951–974. doi:10.1177/001979391206500408.
"Student Loan Debt Exceeds One Trillion Dollars". National Public Radio. Retrieved 9 May 2012.
Kingkade, Tyler (9 January 2013). "Paying Off $26,500 In Debt In Two Years: How Brian McBride Did It". HUFF POST. Retrieved 2013-07-26.
Berman, Jillian. "Watch America’s student-loan debt grow $2,726 every second". Marketwatch.com. Retrieved 18 October 2017.
https://studentloans.net/perkins-loan-program-expires/
"Rhode Island Makes Community College Free - Inside Higher Ed". Insidehighered.com. Retrieved 18 October 2017.
"Free Community College Studied by Tennessee, Oregon -- Stateline". Pewtrusts.org. Retrieved 18 October 2017.
"Free Community College". Ncsl.org. Retrieved 18 October 2017.
John R. Thelin, A History of American Higher Education (2004) pp 1-40
"Book review: ‘Ebony & Ivy: Race, Slavery, and the Troubled History of America’s Universities’ by Craig Steven Wilder". The Boston Globe. Retrieved 18 October 2017.
"America's Top Colleges and Universities Have a Hidden Legacy of Slavery [INTERVIEW]". historynewsnetwork.org.
Swarns, Rachel L. (16 April 2016). "272 Slaves Were Sold to Save Georgetown. What Does It Owe Their Descendants?". Nytimes.com. Retrieved 18 October 2017.
Jamon Smith. "Slavery marks University's past". Tuscaloosanews.com. Retrieved 18 October 2017.
"Campus Slaves & Slavery - Slavery at South Carolina College, 1801-1865 - University of South Carolina Libraries". library.sc.edu.
Love, David (21 April 2016). "Enslaved Black People Helped Build America’s Top Universities — How Should the Colleges Right These Wrongs?". Atlantablackstar.com. Retrieved 18 October 2017.
"Rutgers confronts ties to slavery by renaming buildings, walkway". Nj.com. Retrieved 18 October 2017.
David B. Potts, "American colleges in the nineteenth century: From localism to denominationalism." History of Education Quarterly (1971): 363-380 in JSTOR.
David B. Potts, Baptist colleges in the development of American society, 1812-1861 (1988).
Frederick Rudolph, The American College and University: A History (1991) pp 3-22
Michael Katz, "The Role of American Colleges in the Nineteenth Century", History of Education Quarterly, Vol. 23, No. 2 (Summer, 1983), pp. 215-223 in JSTOR, summarizing Colin B. Burke, American Collegiate Populations: A Test of the Traditional View (New York University Press, 1982) and Peter Dobkin Hall, The Organization of American Culture: Private Institutions, Elites, and the Origins of American Nationality (New York University Press, 1982)
Anton-Hermann Chroust, Rise of the Legal Profession in America (1965) vol 1 ch 1-2
Genevieve Miller, "A Physician in 1776", Clio Medica, Oct 1976, Vol. 11 Issue 3, pp 135-146
Jacob Ernest Cooke, ed. Encyclopedia of the North American colonies (3 vol 1992) 1:214
LaBelle, Jeffrey (2011). Catholic Colleges in the 21st Century: A Road Map for Campus Ministry. Paulist Press. ISBN 9781893757899.
John Rodden, "Less 'Catholic,' More 'catholic'? American Catholic Universities Since Vatican II." Society (2013) 50#1 pp: 21-27.
S. J. Currie, and L. Charles. "Pursuing Jesuit, Catholic identity and mission at US Jesuit colleges and universities." Catholic Education: A Journal of Inquiry and Practice (2011) 14#3 pp 4+ online
Matthew Thomas Larsen, The Duty and Right of the Diocesan Bishop to Watch Over the Preservation and Strengthening of the Catholic Character of Catholic Universities in His Diocese" (PhD dissertation Catholic University of America, 2012)
Leonard V. Koos, The Junior College Movement (1924).
Cohen and Brawer, The American Community College (4th ed. 2003) p 13-14
Jesse P. Bogue, ed. American Junior Colleges (American council on education, 1948)
Cohen and Brawer, The American Community College (6th ed. 2013)
"11 Past and Present Social Movements Led By College Students - Fresh U". Freshu.io. Retrieved 18 October 2017.
Angulo, A. J. (27 January 2016). "Diploma Mills: How For-Profit Colleges Stiffed Students, Taxpayers, and the American Dream". JHU Press – via Google Books.
Korn, Melissa (7 September 2016). "ITT Technical Institute Shuts Down After Government Cut Off New Funding". Wsj.com. Retrieved 18 October 2017.
"Reports finds rising competition in online education market". Insidehighered.com. Retrieved 18 October 2017.
"Behind Berkeley’s Semester of Hate". Nytimes.com. Retrieved 18 October 2017.
"White supremacy is turning up on campus in speeches and leaflets". Insidehighered.com. Retrieved 18 October 2017.
"More Diversity Means More Demands". Nytimes.com. Retrieved 18 October 2017.
"Colleges brace for more violence in wake of Charlottesville rally". Cbsnews.com. Retrieved 18 October 2017.
"College students unmasked as 'Unite the Right' protesters". Insidehighered.com. Retrieved 18 October 2017.
Stripling, Jack; Gluckman, Nell (16 August 2017). "UVa Employee Suffers a Stroke After Campus Clash With White Supremacists". The Chronicle of Higher Education.
"Opinion - What U.Va. Students Saw in Charlottesville". Nytimes.com. 13 August 2017. Retrieved 18 October 2017.
"Public universities are on solid ground to cancel Richard Spencer events, legal experts say". Insidehighered.com. Retrieved 18 October 2017.
"Funding Down, Tuition Up". Center on Budget and Policy Priorities. 2016-05-18. Retrieved 2017-10-12.
"A Lost Decade in Higher Education Funding". Center on Budget and Policy Priorities. 2017-08-22. Retrieved 2017-10-12.
"State Funding: A Race to the Bottom". Acenet.edu. Retrieved 2017-10-12.
Belkin, Douglas (7 April 2014). "Corporate Cash Alters University Curricula". WSJ. Retrieved 23 April 2014.
Calia, Michael (2 July 2014). "Corinthian Colleges, ITT Educational Face U.S. Government Sanctions". Retrieved 7 July 2014.
CCAP. "The 25 Wealthiest Universities In The U.S. - pg.1". Forbes.
Coates, Ken; Morrison, Bill (2015), What to Consider If You're Considering College: New Rules for Education and Employment, Dundurn, p. 280, ISBN 978-1459723726
Marklein, Mary Beth (15 November 2010). "Learning abroad suffers". Melbourne, Florida: Florida Today. pp. 4A.
"GLOBAL FLOW OF TERTIARY-LEVEL STUDENTS". UNESCO Institute for Statistics. 2012. Retrieved October 7, 2013.
Marklein, Mary Beth (November 16, 2009). "More U.S. students going abroad, and vice versa". USA Today. pp. 5D.
Marklein, Mary Beth (November 14, 2010). "More foreign students in USA". Melbourne, Florida: Florida Today. pp. 4A.
"Paying for US uni.". UNI. in the USA and Beyond. 2012. Retrieved October 7, 2013.
Stainburn, Samantha (3 January 2010). "The Case of the Vanishing Full-Time Professor". The New York Times. Retrieved 12 December 2012.
Reynolds, Glenn Harlan (February 19, 2017). "President Trump should take pity on poor PhD". Florida Today. Melbourne, Florida. pp. 19A. Retrieved February 20, 2017.
Aronowitz, Stanley. The Knowledge Factory: Dismantling the Corporate University and Creating True Higher Learning, p. 76. ISBN 0-8070-3123-2.
Aronowitz, The Knowledge Factory 75-76. ISBN? year?
Kingkade, Tyler (14 May 2012). "United States Ranked To Have Best Higher Education System In The World". Huffington Post.
Bowman, Nicholas and Michael Bastedo, personal.umich.edu "Getting on the Front Page: Organizational Reputation, Status Signals, and the Impact of U.S. News & World Report Rankings on Student Decisions" Retrieved June 29, 2010
"US News Best Colleges Rankings 2011: Changes in Methodology Make Them Less Helpful! - Ivey Files". Annaivey.com. Retrieved 18 October 2017.
"U.S. News College Rankings 2011 Are Out!". Education-portal.com. Retrieved 18 October 2017.
Jaschik, Scott (20 June 2007). "More Momentum Against ‘U.S. News’". Inside Higher Ed.
"ANNAPOLIS GROUP STATEMENT ON RANKINGS AND RATINGS". Annapolis Group. 19 June 2007. Archived from the original on 26 June 2007.
Morse, Robert (22 June 2007). "About the Annapolis Group's Statement". U.S. News and World Report.
"PayScale College Salary Report". Payscale. Retrieved 12 December 2014.
Barrett, Jennifer (19 June 2015). "What's the value of a college education? It depends.". CNBC. Retrieved 27 June 2015. "...while students can up their odds of success, college remains a risky, and expensive investment for families—and one whose value diminishes if costs increase faster than wages. At some point, if tuition costs continues to climb, the benefits simply may not be worth the price of admission for some."
"The Economic Value of College Majors" (Press release). Georgetown University. May 2015. Retrieved 7 May 2015.
David Leonhardt (April 24, 2015). "College for the Masses" (Upshot blog). The New York Times. Retrieved April 26, 2015. "Only about a third of young adults today receive a bachelor’s degree. The new research confirms that many more teenagers have the ability to do so — and would benefit from it"
Joshua Goodman; Michael Hurwitz; Jonathan Smith (February 2015). "College Access, Initial College Choice and Degree Completion" (PDF). National Bureau of Economic Research. doi:10.3386/w20996.
Seth Zimmerman (May 2013). "The Returns to College Admission for Academically Marginal Students" (PDF). Retrieved April 26, 2015. "Students with grades just above a threshold for admissions eligibility at a large public university in Florida are much more likely to attend any university than below-threshold students. The marginal admission yields earnings gains of 22 percent between eight and fourteen years after high school completion. These gains outstrip the costs of college attendance, and are largest for male students and free lunch recipients."
"The NCES Fast Facts Tool provides quick answers to many education questions (National Center for Education Statistics)". Nces.ed.gov.
"Reforming Remedial Education". National Conference of State Legislatures. 2017. Retrieved 9 August 2017.
Vedder, Richard; Denhart, Christopher; Robe, Jonathan (January 2013). "Why are Recent College Graduates Underemployed? : University Enrollments and Labor Market Realities". Center for College Affordability and Productivity. Retrieved June 2, 2013. "Increasing numbers of recent college graduates are ending up in relatively low-skilled jobs that, historically, have gone to those with lower levels of educational attainment."
"Is a College Degree Still Worth It?". Debate Club. US News & World Report. Retrieved 7 December 2013. "Frustration with the economy and high unemployment rates is consistently shaping public opinion as college degrees, traditionally thought of as safeguards against unemployment, no longer guarantee gainful positions."
Thompson, Derek (20 September 2016). "Fear of a College-Educated Barista: Is there really a Millennial underemployment crisis? Yes, but only among liberal-arts majors". The Atlantic. Retrieved 21 September 2016.
Pappano, Laura (22 July 2011). "The Master’s as the New Bachelor’s". The New York Times. Retrieved 10 September 2011.
Barshay, Jill (25 May 2015). "Many community college grads continue to out-earn B.A. holders a decade after graduation". Hechinger Report. Teachers College, Columbia University. Retrieved 29 October 2015. "It’s not the degree that matters, but what you got the degree in and, to some extent, where you got it"
"Occupational Outlook Handbook". Bureau of Labor Statistics. Department of Labor. 17 December 2015. Retrieved 29 September 2017. "The OOH can help you find career information on duties, education and training, pay, and outlook for hundreds of occupations."
"Good Jobs That Pay Without a BA". Center on Education and the Workforce. Georgetown University. Retrieved 26 July 2017. "Although the decline in the manufacturing economy eliminated many good jobs for high school graduates, there are still 30 million good jobs in the U.S. that pay well without a BA."
Krupnick, Matt (28 October 2015). "Graduates of four-year universities flock to community colleges for job skills". Hechinger Report. Teachers College, Columbia University. Retrieved 29 October 2015. "There’s a lot of disciplines universities aren’t offering."
"Outcomes by Major", The Labor Market for Recent College Graduates, Federal Reserve Bank of New York, January 29, 2016, retrieved June 24, 2016
Torpey, Elka (2012). "High wages after high school - without a bachelor's degree" (PDF). Occupational Outlook Quarterly. Bureau of Labor Statistics. Retrieved 10 August 2016.
Aud, S., Hussar, W., Kena, G., Bianco, K., Frohlich, L., Kemp, J., Tahan, K. (2011). The Condition of Education 2011 (NCES 2011-033). U.S. Department of Education, National Center for Education Statistics. Washington, DC: U.S. Government Printing Office.
Carr, RV; Wright, JD; Brody, CJ (1996). "Effects of high school work experiences a decade later: Evidence from the National Longitudinal Survey". Sociology of Education. 69: 61–81. doi:10.2307/2112724.
Rowan-Kenyon, H. T. (2007). "Predictors of Delayed College Enrollment and the Impact of Socioeconomic Status". Journal Of Higher Education. 78 (2): 188–214. doi:10.1353/jhe.2007.0012.
Wells, Ryan S.; Lynch, Cassie M. (2012-08-18). "Delayed College Entry and the Socioeconomic Gap: Examining the Roles of Student Plans, Family Income, Parental Education, and Parental Occupation". The Journal of Higher Education. 83 (5): 671–697. ISSN 1538-4640. doi:10.1353/jhe.2012.0028.
Bozick, Robert; DeLuca, Stefanie (2005-09-01). "Better Late Than Never? Delayed Enrollment in the High School to College Transition". Social Forces. 84 (1): 531–554. ISSN 0037-7732. doi:10.1353/sof.2005.0089.
"Predictors of Delayed College Enrollment and the Impact of Socioeconomic Status on JSTOR". Jstor.org. Retrieved 2016-12-12.
Goldrick-Rab, Sara (2011). "Managing to Make It: The College Trajectories of Traditional-age Students with Children". The Wisconsin Scholars Longitudinal Study.
Carnevale, Anthony. "The College Payoff". Center on Education and the Workforce.
Trostel, Phillip. "It's Not Just the Money: The Benefits of College Education to Individuals and to Society" (PDF). Lumina Foundation. Margaret Chase Smith Policy Center & School of Economics University of Maine.
Chen, R.; St; John, E. P. (2011). "£State Financial Policies and College Student Persistence: A National Study£.". Journal Of Higher Education. 82 (5): 629–660. doi:10.1353/jhe.2011.0032.
"Emerging from the Pipeline: African American Students, Socioeconomic Status, and College Experiences and Outcomes". Research In Higher Education. 49 (3): 237–255. 2008. doi:10.1007/s11162-007-9079-y.
Walpole, M. (2008). Emerging from the Pipeline: African American Students, Socioeconomic Status, and College Experiences and Outcomes. Research In Higher Education, 49(3), 237-255. doi:10.1007/s11162-007-9079-y
Thompson, Derek (December 2012). "The Role of Higher Education in Career Development: Employer Perceptions" (PDF). The Chronicle of Higher Education. Retrieved 15 December 2015.
Rubin, M (2012). "Social class differences in social integration among students in higher education: A meta-analysis and recommendations for future research". Journal of Diversity in Higher Education. 5: 22–38. doi:10.1037/a0026162.
Mettler, Suzanne (2014). Degrees of Inequality: How the Politics of Higher Education Sabotaged the American Dream. Basic Books. ISBN 978-0465044962.
Hoxby, Caroline M.; Avery, Christopher (2012-12-01). "THE MISSING "ONE-OFFS": THE HIDDEN SUPPLY OF HIGH-ACHIEVING, LOW INCOME STUDENTS". NBER Working Paper Series. National Bureau of Economic Research.
Carnevale, Anthony (July 2013). "Separate & Unequal" (PDF). Center on Education and the Workforce. Georgetown Public Policy Institute. Retrieved December 12, 2016.
Sander, Richard (2012). Mismatch: How Affirmative Action Hurts Students It's Intended to Help, and Why Universities Won't Admit it. Basic Books. ISBN 978-0465029969.
McGill, Andrew. "The Share of Black Students at Top Universities Has Been Stagnant for 20 Years". The Atlantic. Retrieved 2016-12-12.
"Black Students at Top Colleges: Exceptions, Not the Rule | Brookings Institution". Brookings. 2016-12-12. Retrieved 2016-12-12.
"More Hispanics, blacks enrolling in college, but lag in bachelor’s degrees". Pew Research Center. 2014-04-24. Retrieved 2016-12-12.
Fry, Richard; Taylor, Paul (2013-05-09). "Hispanic High School Graduates Pass Whites in Rate of College Enrollment". Pew Research Center's Hispanic Trends Project. Retrieved 2016-12-12.
"The NCES Fast Facts Tool provides quick answers to many education questions (National Center for Education Statistics)". Nces.ed.gov. Retrieved 2016-12-12.
Bailey, Martha; Dynarski, Susan (2011). "Inequality in Postsecondary Education".
National Center for Education Statistics, Digest of Education Statistics. Retrieved 2017-09-09
Zota, S. (2009). "Undocumented immigrants' access to higher education: fifty states, different direction" (PDF). Popular Government. 74(3): 46–54.
Harmon, C.; Carne, G.; Lizardy-Hajbi, K.; Wilkerson, E. (2010-04-01). "Access to higher education for undocumented students: "Outlaws" of social justice, equity, and equality'.". Journal of Praxis in Multicultural Education. 5(1): 67–82.
Bozkurt, A.; et al. (2015). "Trends in Distance Education Research: A Content Analysis of Journals 2009-2013". International Review of Research in Open and Distributed Learning. 16 (1): 330–363.
Zemsky, Robert (2014). "With a MOOC MOOC here and a MOOC MOOC there, here a MOOC, there a MOOC, everywhere a MOOC MOOC". Journal of General Education. 63 (4): 237–243. JSTOR 10.5325/jgeneeduc.63.4.0237. doi:10.1353/jge.2014.0029.
"Reflections on the underemployment of college graduates". Teachers College at Columbia University. 25 June 2014. Retrieved 18 January 2015.
Robert E. Wright, Fubarnomics: A Lighthearted, Serious Look at America's Economic Ills (Buffalo, N.Y.: Prometheus, 2010).
Schumpeter (December 10, 2011). "University challenge: Slim down, focus and embrace technology: American universities need to be more businesslike". The Economist. Retrieved 2011-12-23. "ex-students have debts approaching $1 trillion."
Nona Willis-Aronowitz, NBC News, June 13, 2014, Is College Worth It? New Documentary Weighs Costs of Higher Ed, accessed June 13, 2014, "...ballooning tuition costs driven by the arms race of providing luxury facilities to student-consumers; predatory loan systems burying students in debt; and the endangerment of a useful, worthwhile college degree in an era of job scarcity and rampant inequality...."
Buss, Dale (2005-09-04). "Sometimes, It's Not the Tuition. It's the Textbooks". The New York Times. Retrieved 2010-05-04.
Linda Gorman (2009-12-05). "State Education Subsidies Shift Students to Public Universities". National Bureau of Economic Research.
Willie, Matt (2012). "Taxing and Tuition: A Legislative Solution to Growing Endowments and the Rising Costs of a College Degree". Brigham Young University Law Review. 2013 (5). 1665. ISSN 0360-151X.
Conti-Brown, Peter (6 August 2009). "Scarcity Amidst Wealth: The Law, Finance, and Culture of Elite University Endowments in Financial Crisis". Stanford Law Review. 63 (5). ISSN 0038-9765.
Archibald, Robert B., and Feldman, David H. (2008). "Why Do Higher-Education Costs Rise More Rapidly Than Prices in General?". Change. 40 (3). ISSN 0009-1383.
Kiener, R. (2013, January 18). "Future of public universities". CQ Researcher. 23: 53–80. Check date values in: |date= (help)
Weisbrod, B. A. (2008). Mission and Money : Understanding the University [electronic resource]. 78-81.
Michelle Singletary (2009-10-22). "The Color of Money: Getting through college these days almost requires a degree in thrift". Washington Post. pp. 20A.
"Tuition Levels Rise but Many Students Pay Significantly Less than Published Rates" Archived 2006-06-03 at the Wayback Machine.. The College Board (2003). URL accessed on June 20, 2005
Weisbrod, B. A. (2008). Mission and Money : Understanding the University [electronic resource]. 79.
Shin, Jung Cheol; Milton, Sande (2007). "Student response to tuition increase by academic majors: empirical grounds for a cost-related tuition policy". Higher Education. 55 (6): 719–734. ISSN 0018-1560. doi:10.1007/s10734-007-9085-1.
Weisbrod, B. A. (2008). Mission and Money : Understanding the University [electronic resource]. 84.
"Trends in College Spending 1998–2008" Delta Cost Project.
"My Portfolios - Finance Portfolio". Dailyfinance.com. Retrieved 18 October 2017.
Broder, David S. (columnist) (December 7, 2008). College affordability about future. Burlington Free Press (and other column subscribers).
Lewin, Tamar (6 March 2013). "Financing For Colleges Declines As Costs Rise". The New York Times. p. 17.
Barbara Raab, Senior Producer, NBC News, Apr 9, 2013 Meet your new professor: Transient, poorly paid, Accessed April 9, 2013
Clark, Kim (November 17–24, 2008). Does it Matter That Your Professor Is Part Time?. US News and World Report.
Weisbrod, B. A. (2008). Mission and Money : Understanding the University [electronic resource]. 87.
Saul, Stephanie (2016-07-07). "Public Colleges Chase Out-of-State Students, and Tuition". The New York Times. ISSN 0362-4331. Retrieved 2016-07-09.
Jaquette, Ozan; Curs, Bradley R; Posselt, Julie R (2015-04-01). "Tuition rich, mission poor: Nonresident enrollment growth and the socioeconomic and racial composition of public research universities". The Journal of Higher Education. Forthcoming. ISSN 0022-1546.
Espenshade, Thomas J.; Walton Radford, Alexandria (2009). No longer separate, not yet equal: race and class in elite college admission and campus life. Princeton, New Jersey: Princeton University Press. ISBN 9780691141602.
Douthat, Ross (2010-07-18). "The Roots Of White Anxiety". The New York Times.
The Role of Higher Education in Social Mobility. Robert Haveman and Timothy Smeeding. Opportunity in America Volume 16 Number 2 Fall 2006
Seth Freed Wessler, May 16, 2014, NBC News, Great Unequalizer: Is Higher-Education Policy Making Inequality Worse?, Accessed May 19, 2014
Marklein, Mary Beth (January 16, 2013). "Athletics get more dollars than academics". Florida Today. Melbourne, Florida. pp. 4A.
"The NCES Fast Facts Tool provides quick answers to many education questions (National Center for Education Statistics)". Nces.ed.gov. Retrieved 18 October 2017.
"Why Student Aid Pays Off for Society and Individuals" (PDF). Ihep.org. Retrieved 18 October 2017.
McPherson, Michael; Morton Owen Schapiro. "Financing Undergraduate Education: Designing National Policies". National Tax Journal. 1 (3): 557–571.
"The NCES Fast Facts Tool provides quick answers to many education questions (National Center for Education Statistics)". Nces.ed.gov. Retrieved 18 October 2017.
Tandberg, D (2010). "Politics, Interest Groups and State Funding of Public Higher Education". Research In Higher Education. 51 (5): 416–450. doi:10.1007/s11162-010-9164-5.
McPherson, M. S.; Schapiro, M. O. (2002). "The Blurring Line between Merit and Need in Financial Aid". Change. 34 (2): 38–46. doi:10.1080/00091380209601844.
Hoyt, Jeff (Feb 2001). "Performance Funding in Higher Education: The Effects of Student Motivation on the Use of Outcomes Tests to Measure Institutional Effectiveness". Research in Higher Education. 42 (1): 71–85.
Schapiro, Morton and Michael S. McPherson. 2006. "Watch What We Do (and Not What We Say)." In College Access: Opportunity or Privilege?, edited by McPherson, M.S. & Schapiro, M.O., 49-73. New York: The College Board
Scholarships.com. "Why Students Dont Go To College - Scholarships.com". Scholarships.com. Retrieved 18 October 2017.
"For Profit Higher Education: The Failure to Safeguard the Federal Investment and Ensure Student Success" (PDF). Help.senate.gov. Retrieved 18 October 2017. line feed character in |title= at position 54 (help)
Kirkham, Chris (30 July 2012). "For-Profit Colleges Get Scathing Indictment In Senate Report". Huffingtonpost.com. Retrieved 18 October 2017.
Puzzanghera, Jim (11 September 2015). "Rise in student loan defaults driven by for-profit colleges, study says". Latimes.com. Retrieved 18 October 2017.
John Lauerman and Esmé E. Deprez, "Apollo, Education Shares Plunge on Enrollment Outlook" Bloomberg Oct. 14, 2010
Danilova, Maria. "More than half of students at for-profit colleges defaulted on loans, study finds". Chicagotribune.com. Retrieved 18 October 2017.
"Black Americans Twice as Likely as Whites to Default on Student Debt". Bloomberg.com. 11 October 2017. Retrieved 18 October 2017.
"The Decline of the 'Great Equalizer'" Reuters, December 19, 2012
"For Poor, Leap to College Often Ends in a Hard Fall" New York Times, December 22, 2012
"The Education Bubble, Part 3: ‘Shared Responsibility’ To Shrink Student Debt". Wbur.org. Retrieved 18 October 2017.
Vedder, Richard (May–June 2012). "Federal Student Aid and the Law of Unintended Consequence". Imprimis. 41 (5/6): 1–5.
"Department Of Education Puts Restrictions On For-Profit College Student Debt". Citytowninfo.com. Retrieved 18 October 2017.
Patricia Reaney of Reuters, NBC News, 2013, Poll: Nearly half of US college grads are underemployed Accessed April 30, 2013
Berman, Jillian. "Students from poor families are struggling way more to pay back their student loans". Marketwatch.com. Retrieved 18 October 2017.
"Institutional Retention and Graduation Rates for Undergraduate Students". The Condition of Education. National Center for Education Statistics. May 2015. Retrieved 7 May 2016. "About 59 percent of students who began seeking a bachelor's degree at a 4-year institution in fall 2007 completed that degree within 6 years."
Thompson, Derek (December 2012). "The Role of Higher Education in Career Development: Employer Perceptions" (PDF). The Chronicle of Higher Education. Retrieved 6 November 2015.
Jaschik, Scott (29 March 2016). "Grade Inflation, Higher and Higher". Inside Higher Ed. Retrieved 7 July 2016.
Katsikas, Aina (13 January 2015). "Same Performance, Better Grades: Academic achievement hasn't improved much, so why are college-goers getting higher GPAs than ever before?". The Atlantic. Retrieved 12 September 2015.
Babcock, Philip; Marks, Mindy (2010), Leisure College, USA: The Decline in Student Study Time, American Enterprise Institute for Public Policy Research, retrieved 8 May 2015
Arum, Richard; Roksa, Josipa (2011), Academically adrift: limited learning on college campuses, University of Chicago Press, p. 259, ISBN 9780226028552
"Address Problematic Student Behavior". Design and Teach Your Course. Carnegie Mellon University. Retrieved 5 May 2015.
"Employers Judge Recent Graduates Ill-Prepared for Today’s Workplace, Endorse Broad and Project-Based Learning as Best Preparation for Career Opportunity and Long-Term Success" (Press release). Washington, DC: Association of American Colleges and Universities. 20 January 2015. Retrieved 11 April 2017.
J. Scott Armstrong (2012). "Natural Learning in Higher Education". Encyclopedia of the Sciences of Learning.
Everett Carll Ladd and Seymour Martin Lipset, Academics, politics, and the 1972 election (1973)
Schuster, Jack; Finkelstein, Martin (2008), The American Faculty: The Restructuring of Academic Work and Careers, Johns Hopkins University Press, p. 600, ISBN 978-0801891038
Horowitz, David; Laskin, Jacob (March 2009), One-Party Classroom: How Radical Professors at America's Top Colleges Indoctrinate Students and Undermine Our Democracy, New York: Random House, p. 336, ISBN 978-0307452559
Menand, Louis (2010). The Marketplace of Ideas: Reform and Resistance in the American University. Norton. p. 176. ISBN 978-0393339161.
Maranto, Redding, Hess (2009). The Politically Correct University: Problems, Scope, and Reforms (PDF). The AEI Press. pp. 25–27. ISBN 978-0-8447-4317-2.
Rothman, S.; Lichter, S. R.; Nevitte, N. (2005). "Politics and Professional Advancement Among College Faculty" (PDF). The Forum. 3. doi:10.2202/1540-8884.1067.
College Faculties A Most Liberal Lot, Study Finds, Howard Kurtz, Tuesday, March 29, 2005 Washington Post https://www.washingtonpost.com/wp-dyn/articles/A8427-2005Mar28.html
"Groupthink in Academia: Majoritarian Departmental Politics and the Professional Pyramid." Pp. 79–98 in The Politically Correct University: Problems, Scope, and Reforms, edited by Robert Maranto, Richard Redding, and Frederick Hess. Washington, DC: AEI Press.
Gross, Neil; Simmons, Solon (September 24, 2007), The Social and Political Views of American Professors (PDF), retrieved January 31, 2017
"The '60s Begin to Fade as Liberal Professors Retire", Patricia Cohen, July 3, 2008, New York Times
The Still Divided Academy: How Competing Visions of Power, Politics, and Diversity Complicate the Mission of Higher Education, Stanley Rothman, April Kelly- Woessner, Matthew Woessner, Rowman & Littlefield Publishers, Inc., 2011.
"Five myths about liberal academia", Matthew Woessner, April Kelly-Woessner and Stanley Rothman Friday, February 25, 2011 Washington Post
Diversity in American Higher Education: Toward a More Comprehensive Approach, Lisa Stulberg and Sharon Weinberg, eds, 2011, Routledge. Explaining professor's politics: An indirect text of the self-selection hypothesis, Neil Gross and Catherine Cheng
Maranto, Robert. 2009. "Why Political Science Is Left but Not Quite PC." Pp. 209–24 in The Politically Correct University: Problems, Scope, and Reforms, edited by Robert Maranto, Richard Redding, and Frederick Hess. Washington, DC: AEI Press.
Jaschik, Scott (October 24, 2012). "Moving Further to the Left". Inside Higher Ed. Washington, D.C. Retrieved June 24, 2015.
"Study: College Professors Continue Shift To ‘Far Left’ Stance". WNEW. Washington, DC. October 25, 2012. Retrieved June 24, 2015.
Woessner, Matthew (2012). "Rethinking the Plight of Conservatives in Higher Education". Academe. Washington, DC: American Association of University Professors. Retrieved June 24, 2015.
Nong, Eric (December 4, 2014). "Why Do So Few Conservatives Work in Higher Education?". The Wilson Quarterly. Washington, DC: Woodrow Wilson International Center for Scholars. Retrieved June 24, 2015.
"The Left-Leaning Tower", John Tierney, The New York Times, July 22, 2011
Zogby Poll: Most Think Political Bias Among College Professors a Serious Problem, July 10, 2007
Inbar, Yoel; Lammers, Joris (September 5, 2012), "Political Diversity in Social and Personality Psychology" (PDF), Perspectives on Psychological Science, SAGE Publications, 7 (5), doi:10.1177/1745691612448792, retrieved January 20, 2017
Duarte, José L.; Crawford, Jarret T.; Stern, Charlotta; Haidt, Jonathan; Jussim, Lee; Tetlock, Philip E. (January 2015), "Political diversity will improve social psychological science" (PDF), Behavioral and Brain Sciences, Cambridge University Press, 38, doi:10.1017/S0140525X14000430, retrieved January 20, 2017
Abrams, Sam (January 9, 2016), Professors moved left since 1990s, rest of country did not, Heterdox Academy, retrieved January 20, 2017
Liberal professors dominate US colleges, Informative Statistics, retrieved January 20, 2017
Haidt, Jonathan; Jussim, Lee (February 2016), "Psychological Science and Viewpoint Diversity", Observer, Association for Psychological Science, 29 (2), retrieved January 20, 2017
Haidt, Jonathan (January 7, 2016), New Study Indicates Existence of Eight Conservative Social Psychologists, Heterdox Academy, retrieved January 20, 2017
Honeycutt, Nathan; Freberg, Laura (September 5, 2012), "The Liberal and Conservative Experience Across Academic Disciplines: An Extension of Inbar and Lammers", Social Psychological and Personality Science, SAGE Publications, 7 (5), doi:10.1177/1948550616667617, retrieved January 31, 2017
"Professors as Targets of Internet Outrage". Nytimes.com. Retrieved 18 October 2017.
Mele, Christopher (28 November 2016). "Professor Watchlist Is Seen as Threat to Academic Freedom". Nytimes.com. Retrieved 18 October 2017.
Strauss, Valerie (14 December 2016). "‘The sort of company we wish to keep’: More than 1,500 academics ask to join controversial ‘Professor Watchlist’". Washingtonpost.com. Retrieved 18 October 2017.
Glatter, Hayley. "Student-Government Politics and Identity Politics: This Week's Top 7 Education Stories". Theatlantic.com. Retrieved 18 October 2017.
Atkins, Larry (28 August 2017). "There Should Be Free Speech On College Campuses For Conservative Students, Conservative Speakers And Liberal Professors". Huffington Post. Retrieved 31 August 2017. "Universities shouldn’t shelter students and protect them in a liberal bubble. It’s good for them to be exposed to ideas that might differ from theirs. By all means, hold a peaceful protest before or after a controversial conservative comes to speak on campus, but let the speaker give their speech to those who want to hear it."
"Mobility Report Cards: The Role of Colleges in Intergenerational Mobility" (PDF). Equality-of-opportunity.org. 2017. Retrieved 18 October 2017.
"They still value a degree, but Americans increasingly question the cost - The Hechinger Report". Hechingerreport.org. 11 May 2017. Retrieved 18 October 2017.
Savage, Charlie (1 August 2017). "Justice Dept. to Take On Affirmative Action in College Admissions". Nytimes.com. Retrieved 18 October 2017.
Selingo, Jeffrey J. (2 August 2017). "Perspective - Trump administration is taking aim at affirmative action in college admissions. Why it won’t fix what’s broken.". Washingtonpost.com. Retrieved 18 October 2017.
"Campus Sexual Violence: Statistics - RAINN". Rainn.org. Retrieved 18 October 2017.

    [4][dead link]

Further reading
Main article: History of higher education in the United States § Further reading

    Bogue, E. Grady and Aper, Jeffrey. Exploring the Heritage of American Higher Education: The Evolution of Philosophy and Policy. Oryx, 2000. 272 pp.
    Brint, S., & Karabel, J. The Diverted Dream: Community colleges and the promise of educational opportunity in America, 1900–1985. Oxford University Press. (1989).
    Carney, Cary Michael. Native American Higher Education in the United States. Transaction, 1999. 193 pp.
    Cohen, Arthur M. (1998). The Shaping of American Higher Education: Emergence and Growth of the Contemporary System. San Francisco: Jossey-Bass.
    Dorn, Charles. For the Common Good: A New History of Higher Education in America (Cornell UP, 2017) 308 pp
    Eisenmann, Linda. Higher Education for Women in Postwar America, 1945–1965. Johns Hopkins U. Press, 2006. 304 pp.
    Faragher, John Mack and Howe, Florence, ed. Women and Higher Education in American History. Norton, 1988. 220 pp.
    Geiger, Roger L. Research and Relevant Knowledge: American Research Universities Since World War II. Oxford University Press. (2001).
    Gleason, Philip. Contending with Modernity: Catholic Higher Education in the Twentieth Century. Oxford U. Press, 1995. 434 pp.
    Ihle, Elizabeth L., ed. Black Women in Higher Education: An Anthology of Essays, Studies, and Documents. Garland, 1992. 341 pp.
    Lucas, C. J. American higher education: A history. (1994).
    Ruben, Julie. The Making of the Modern University: Intellectual Transformation and the Marginalization of Morality. University Of Chicago Press. (1996).
    Rudolph, Frederick. The American College and University: A History (1991), a standard survey
    Thelin, John R. A History of American Higher Education. Johns Hopkins U. Press, 2004. 421 pp.
    Veysey Lawrence R. The emergence of the American university. (1965).

External links
	Wikivoyage has a travel guide for Touring prestigious and notable universities in the U.S..

    U.S. Department of Education college navigator
    Guide to U.S. higher education for international students
    Guide to U.S. higher education for UK students
    Wikisource-logo.svg "author=Frank L. Tolman". Encyclopedia Americana. 1920.

[hide]

    v t e 

Higher education in the United States
States 	

    Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi Missouri Montana Nebraska Nevada New Hampshire New Jersey New Mexico New York North Carolina North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode Island South Carolina South Dakota Tennessee Texas Utah Vermont Virginia Washington West Virginia Wisconsin Wyoming 

Federal district 	
Washington, D.C.
Insular areas 	

    American Samoa The Marianas
        Guam Northern Mariana Islands Puerto Rico U.S. Virgin Islands 

Categories:

    Higher education by countryUniversities and colleges in the United StatesHigher education in the United States

	
Student financial aid (United States)
From Wikipedia, the free encyclopedia
  (Redirected from Student financial aid in the United States)
	
This article is written like a personal reflection or opinion essay that states a Wikipedia editor's personal feelings about a topic. Please help improve it by rewriting it in an encyclopedic style. (September 2013) (Learn how and when to remove this template message)

Student financial aid in the United States is funding that is available exclusively to students attending a post-secondary educational institution in the United States. This funding is to assist in covering the many costs incurred in the pursuit of post-secondary education. Financial aid is available from federal, state, educational institutions, and private agencies (foundations), and can be awarded in the forms of grants, education loans, work-study and scholarships. Please note that in order to apply for any federal financial aid students must first complete the Free Application for Federal Student Aid (FAFSA).

Contents

    1 Types of financial aid
        1.1 Grants
        1.2 Education loans
            1.2.1 Federal student loan programs
                1.2.1.1 Direct subsidized loans
                1.2.1.2 Direct unsubsidized loans
                1.2.1.3 Parent loans
            1.2.2 Private loans
            1.2.3 Consolidation loans
        1.3 Work Study
        1.4 Scholarships
    2 Financial aid application process
        2.1 Application process for need-based aid
        2.2 Need-based aid
        2.3 Non-need based financial aid
        2.4 Non-need-based aid versus need-based aid
    3 Graduate and professional students
    4 International students
    5 College cost calculators
    6 Debt vs. grants
        6.1 No-loan financial aid
        6.2 Loan cap
    7 Effect of financial aid on enrollment
        7.1 Need-blind admissions
    8 Outside the United States
    9 See also
    10 References
    11 External links

Types of financial aid
Education in the United States

    By state + insular areas
    By subject area
    History of
    Issues: Finance – Law – Literacy – Reform
    Levels: Primary – Secondary – Higher
    Organizations

Nuvola apps bookcase.svg Education portal
Flag of the United States.svg United States portal

    v t e 

Grants

In the United States, grants come from a wide range of government departments, colleges, universities or public and private trusts. Grant eligibility is typically determined by financial need and academic merit. The application process is set by the agency providing the funds and often relies on data submitted via the FAFSA.

While the terms grant and scholarship are frequently used interchangeably, there is a difference. Scholarships may have a financial need component but rely on other criteria as well. Some private need-based awards are confusingly called scholarships, and require the results of a FAFSA (the family's EFC). However, scholarships are often merit-based, while grants tend to be need-based.[1]

Some examples of grants commonly applied for in the U.S.:

    Federal Pell Grant, the largest of the federal grant options and based on an individual’s EFC (expected family contribution) as determined by the FAFSA.[2]
    Federal Supplemental Educational Opportunity Grant (FSEOG), federal grant program that is need-based, but directed towards students whose FAFSA results exhibit exceptional financial need, such as being among the lowest Expected Family Contribution (EFC).[3]
    The Teacher Education Assistance for College and Higher Education (TEACH) Grant requires you to take certain classes in order to get the grant, followed by performing a specific job, sometimes in a specific location, to keep the grant from becoming a loan.
    Institutional Grants, grants provided by educational institutions. Some institutional grants are based on academic achievement (merit awards or merit scholarships), while others are based on financial need, and some are a combination of the two.
    Private and Employer Grants, grants provided by the private sector, for students who meet specific criteria for eligibility related to the private organization.
    State Grants, public funds received from state agencies that are completely separate from those listed in the federal sector. These grants vary by state and awarded based on financial need.[4]

Education loans

An education loan is taken out by the student (or parent) in order to pay for educational expenses. Unlike scholarships and grants, this money must be repaid with interest. Educational loan options include federal student loans, federal parent loans, private loans, and consolidation loans.
Federal student loan programs

Federal student loans are loans directly to the student; the student is responsible for repayment of the loan. These loans typically have low interest rates and do not require a credit check or any other sort of collateral. Student loans provide a wide variety of deferment plans, as well as extended repayment terms, making it easier for students to select payment methods that reflect their financial situation. There are federal loan programs that consider financial need.
Direct subsidized loans

Direct subsidized loans are the most sought, as they have few requirements other than enrollment and demonstration of financial need. However, the amount you may borrow is determined by your school and may not exceed your financial need, which is based on the EFC from your FAFSA. You are not required to begin repaying these loans for as long as you are in school at least half-time. They also offer a six-month grace period, meaning you do not begin repaying them until six months after you leave school. These loans also offer a deferment period in some cases.[5]
Direct unsubsidized loans

Direct Unsubsidized Loans are available to all undergraduate and graduate students, with no requirement to demonstrate financial need. Your school will determine how much you are allowed to borrow based on your cost of attendance and adjust for any other financial aid you are receiving. However, you are responsible for paying the interest on these loans even during school. If you choose not to pay interest while enrolled, your interest will accrue and be added to the principal amount of your loan.[5]
Parent loans

Federal parent loans are a federally funded loan option if the student is dependent on his or her parents. Parent loans allow parents to take out student loans, the repayment of which will be their responsibility. The parents use these loans to pay for educational expenses on behalf of the student. For undergraduate students there is the parent loan for undergraduate students or PLUS Loan. This loan allows parents to borrow up to the total cost of attendance, minus any other financial aid the student receives. Eligibility will be determined upon review of the parent's credit history.[6]
Private loans

Private student loans are offered by private lenders (financial institutions). These loans typically have much higher interest rates, have fewer repayment/deferment options, and are not supervised by any agency.[7]
Consolidation loans

Consolidation Loans combine two or more student and/or parent loans into one loan. They are an option for those who find themselves struggling with multiple student loan payments. Consolidation loans are available for most federal loan types, and some private lenders offer private consolidation loans for private education loans.[8]
Work Study

The Federal Work-Study Program is a form of financial aid that can be used not only as a means of maintain a stable bank account, but to also earn money toward paying off tuition. Work study jobs allow students to get jobs within their field or given interest, and are more flexible than off-campus part-time jobs because they are designed to accommodate student schedules.
Scholarships

While the terms grant and scholarship are frequently used interchangeably, there is a difference. Scholarships may have a financial need component but rely on other criteria as well. Some private need-based awards are confusingly called scholarships, and require the results of a FAFSA (the family's EFC). However, scholarships are often merit-based, while grants tend to be need-based.[1]

Scholarships, similar to grants, do not need to be repaid. Scholarships come from state, educational institutions, and private agencies. Scholarships can be awarded based on merit, financial need, student characteristics (such as gender, race, religion, family and medical history, and the like), creativity, career field, college, athletic ability, among other categories.

There are search engines available to find scholarships such as Peterson’s, Unigo, Fastweb, Cappex, Chegg, The College Board, Niche (formerly known as College Prowler), Scholarships.com, Collegenet.com, and Scholarship Monkey.[9]
Financial aid application process
Main article: FAFSA
Application process for need-based aid

To qualify for need-based aid a student must have a significant amount of financial need, which is determined by the federal government based on the FAFSA. Using the information submitted on the FAFSA, the U.S. Department of Education calculates a figure called the Expected Family Contribution (EFC). If the EFC is less than the cost of attending a college, the student has financial need (as the term is used in the U.S. financial aid system).

Some well-to-do colleges have need-based aid of their own to distribute, in addition to federal and state aid (if any). These colleges require, in addition to the FAFSA, the CSS Profile financial form, which goes into greater detail.[10]
Need-based aid

Need-based financial aid is awarded on the basis of the financial need of the student. The Free Application for Federal Student Aid application (FAFSA) is generally used for determining federal, state, and institutional need-based aid eligibility. At private institutions, a supplemental application may be necessary for institutional need-based aid.

A recent trend shows that what is purely need-based aid is not entirely clear. According to the National Postsecondary Aid Survey (NPSAS), SAT scores affect the size of institutional need-based financial aid.[11] If a student has a high SAT score and a low family income, they will receive larger institutional need-based grants than a student with a low family income that has low SAT scores. In 1996, public higher education institutions gave students with high SAT scores and a low family income $1,255 in need-based grants. However, only $565 in need-based grants were given to students with low SAT scores who had low family incomes. The lower a student’s SAT score, the smaller the amount of need-based grants a student received no matter what their family income level was. The same trend holds true for higher education private institutions. In 1996, private institutions gave students with high SAT scores and a low family income $7,123 versus $2,382 for students with low SAT scores and a low family income. Thus, “institutional need-based awards are less sensitive to need and more sensitive to ‘academic merit’ than the principles of needs analysis would lead us to expect.” [12] It has been found that increasing an SAT score in the range of 100-200 points can result in hundreds of dollars more in institutional grants and on average substantially more if one is attending a private institution.[13]

While providing financial information to the government is a reasonable expectation to calculate a student’s financial need, it does not necessarily follow that colleges should have access to this information. Providing that information to schools may be problematic because schools learn about students’ other sources of funding and may adjust their financial aid packages accordingly. There is an asymmetric information problem since schools have full knowledge of their customers' ability to pay while students and their families have little information about costs that colleges face to provide their services. That is, when planning for the next academic year, a school will know its current and projected costs as well as each student’s ability to pay after receiving state and federal grants. According to the Center for College Affordability and Productivity (CCAP), “If the federal or state authorities increase financial support per student, the institution has the opportunity to capture part or all of that increased ability to pay by reducing institutional grants and/or raising their charges for tuition, fees, room, or board.” Importantly, it also notes that “the exception to this general pattern is modest aid targeted at only low-income students, like the Pell grant.” The center uses data about net proceeds (tuition plus room, board and other fees) as a percentage of median income to show that financial aid practices have not been effective in decreasing prices in an effort to increase access. Net proceeds at public four-year institutions rose from 15% to 20% of median income from 1987 to 2008. In that same time, productivity has declined in the form of lighter teaching loads for professors and increased expenditures on administrative staff.[14]
Non-need based financial aid

Non-need based loans are available for students and families who cannot afford to pay the entire cost of college. These loans are directed toward those individuals and families who did not qualify for need-based loans due to the amount of their personal assets. There is usually a higher interest rate associated with non-need based loans. Because these loans are not need-based, the U.S. government does not pay the interest for the student while enrolled in school; they are often referred to as unsubsidized loans. The Unsubsidized Stafford Loan and Grad PLUS loan are non-need based loans available for both undergraduate and graduate students who do not qualify for need-based financial aid.[10]

Even though these loans are not subsidized, interest rates are set by Congress, the programs are closely supervised, and they provide many protections that private loans rarely offer.

There are also non-need based grants and scholarships that consider merit rather than financial need. These awards are granted by the college or university as well as outside organizations. Merit-based scholarships are typically awarded for outstanding academic achievements and maximum SAT or ACT scores. However, some scholarships may be awarded due to special talents like athletic scholarships, leadership potential, and other personal characteristics. In order to be considered for such awards some institutions require an additional application process while others automatically consider all admitted students for their merit-based scholarships.
Non-need-based aid versus need-based aid

With the yearly rising cost of tuition, room and board, and fees among schools across the nation, low-income students are finding it harder to pay for their education. In an attempt to help students meet the high, costly demands of college, schools have increased merit-based grants, for students with outstanding academic position, involvement in organizations, or high athletic talent. The issue is that these reasons for awarding scholarships take away from low-income students who often do not meet these merit standards. In other words, funds for merit-based scholarships are taking away from the already small amount of federal aid available to low-income students who simply cannot pay for college without some kind of financial aid.

In recent years, government has responded to the financial crisis students are facing and therefore passed legislation that boosted the value of grants for low-income students and trimmed subsidies for private education lenders.[15] Schools have also taken action for the sake of students. Harvard University, a well-known costly but wealthy institution that had previously cut tuition for students whose families earned less than $60,000 a year, proceeded to cut costs by nearly fifty percent for those students whose families earned between $120,000 and $180,000 a year.[15] Institutions will consider students' financial needs as well as their academic merit standing when applying for financial aid. Merit-based aid and need-based aid have been linked together for many financial aid scholarships. This relationship is beneficial as it underlies that one form of financial aid, particularly merit-based, is not completely taking over need-based aid. Statistics do show results of studies performed from 1992-2000 that the increase in financial aid awarded was based entirely on merit.[16] However, when viewing numbers of both merit-based and need-based aid closely, the differences are not significant.
Graduate and professional students

The following types of federal financial aid are available to graduate and professional students. Aid for these students is primarily loans.

    The William D. Ford Federal Direct Loan (Direct Loan) Program: Eligible students may borrow up to $20,500 per school year. These loans are unsubsidized; Congress has determined that subsidized loans (no interest while enrolled) are only available to undergraduates. Graduate and professional students enrolled in certain health profession programs may receive additional Direct Unsubsidized Loan amounts each academic year. These federal loans, although unsubsidized, are far superior in interest rate and repayment terms to private student loans.
    Federal Perkins Loan (Perkins Loan) Program: This is a school-based loan program for eligible students with exceptional financial need. Students may qualify for a Perkins Loan of up to $8,000 each year depending on financial need, the amount of other aid received, and the availability of funds at the school. Each college has a set amount of Perkins Loans for its students; there has been controversy over the formula that is used to apportion the loans to colleges.
    Teacher Education Assistance for College and Higher Education (TEACH) Grant: The TEACH Grant Program provides grants of up to $4,000 a year to students who are completing or plan to complete coursework needed to begin a career in teaching. The TEACH Grant is different from other federal student grants in that it requires students to take certain kinds of classes to get the grant, and then to do a certain kind of job to keep the grant from turning into a loan.
    Federal Work-Study (FWS) Program: The Work-Study Program provides part-time jobs for undergraduate and graduate students with financial need. This program allows students to earn money to help pay education expenses. The program encourages community service work and work related to a student’s course of study.
    Federal Pell Grant: A Pell Grant, unlike a loan, does not have to be repaid. Most graduate and professional students are not eligible for Pell Grants, but those enrolled in a post-baccalaureate teacher certification program are eligible.[2]

Graduate students may also be eligible for these financial aid programs:

    Aid from other federal agencies (i.e., research grants or fellowships)
    State aid (i.e., state loans)
    Institutional aid (i.e., institutional scholarships or graduate assistantships/fellowships)
    Non-institutional scholarships[17]

International students

There is little financial aid is available for foreign students, with the unique exception of Canadian and Mexican students. A majority of aid is awarded as grants, scholarships, and loans that come through public and private sources which restrict their awards to American citizens. That being said there is financial aid still available for international students.

There are colleges and universities that offer aid to international students. To find out if the school in question offers such assistance inquire of the financial aid office of the institution. Some schools offer grants, loans and jobs, and give anywhere from 15 to 150 awards to foreign students. For example, schools such as Harvard, Princeton, University of Pennsylvania, University of Miami, Ithaca College, Cornell University, Johns Hopkins, University of Chicago, and University of Oregon all offer packages to foreign students. Graduate students may have more luck with financial aid. This is because graduate and teaching assistantships are offered on the basis of academic achievement, regardless of citizenship.[18] Although International students are not eligible for the US government aid programs like the Pell Grant, SEOG Grant, Stafford Loan, Perkins Loan, PLUS Loan, and Federal Work study, many schools will ask international students to submit a FAFSA so that they may use the data for assessing financial need.[19]

There is also assistance a student can seek from their native country. Canadian students attending colleges in the USA may obtain loans through the Canadian government’s Ministry of Skills, Training, and Labour. Alternative loans Canadian international students may apply for are the Canadian Higher Education Loan Program,[20] Global Student Loan Corporation (GLSC),[21] and International Student Loan Program (ISLP).[22][23] Financial Aid for European Students can be looked by using Noopolis, a database in Italy run by CNR (the Italian equivalent of the US’s National Science Foundation). It has information regarding financial aid for Italian citizens to study abroad. There are also U.S. Educational Advising Centers throughout the world that assist prospective students by answering the questions they have about studying in the United States.[18]
College cost calculators
Main article: College cost calculator

Post-secondary institutions post a Cost of Attendance or Price of Attendance, also known as a "sticker price." However, that price is not how much an institution will cost an individual student. To make higher education costs more transparent before a student actually applies to college, federal law requires all post-secondary institutions receiving Title IV funds (federal funds for student aid) to post net price calculators on their websites by October 29, 2011.

As defined in The Higher Education Opportunity Act of 2008, the net price calculator’s purpose is:

    “…to help current and prospective students, families, and other consumers estimate the individual net price of an institution of higher education for a student. The net price calculator shall be developed in a manner that enables current and prospective students, families, and consumers to determine an estimate of a current or prospective student’s individual net price at a particular institution.”

The law defines estimated net price as the difference between an institution’s average total Price of Attendance (the sum of tuition and fees, room and board, books and supplies, and other expenses including personal expenses and transportation for a first-time, full-time undergraduate students who receive aid) and the institution’s median need- and merit-based grant aid awarded.[24]

Elise Miller, program director for the U.S. Department of Education's Integrated Postsecondary Education Data System (IPEDS) stated the idea behind the requirement: "We just want to break down the myth of sticker price and get beyond it. This is to give students some indication that they will not necessarily be paying that full price."[25]

The template was developed based on the suggestions of an IPEDS’ Technical Review Panel (TRP), which met on January 27–28, 2009, and included 58 individuals representing federal and state governments, post-secondary institutions from all sectors, association representatives, and template contractors. Mary Sapp, Ph.D., assistant vice president for planning and institutional research at the University of Miami, served as the panel’s chair. She described the mandate’s goal as “to provide prospective and current undergraduate students with some insight into the difference between an institution’s sticker price and the price they will end up paying.”[26]

To meet the requirement, post-secondary institutions may choose between a basic template developed by the U.S. Department of Education or an alternative net price calculator that offers at least the minimum elements the law requires.[27] A recent report issued by the Institute for College Access and Success, "“Adding it all up 2012: are net price calculators easy to find, use and compare?”, found key issues with the implementation of the net price calculator requirement.[28] In “Adding it all up,” the authors state, “this report takes a more in-depth look at the net price calculators from 50 randomly selected colleges. While we found some positive practices that were not evident at the time of our previous report, net price calculators are still not reliably easy for prospective college students and their families to find, use, and compare”.[28]

After the requirement came into effect, the free website CollegeAbacus.org began creating a system that would allow students to enter the personal information once, and then use and compare net-prices of multiple schools.[29] The Gates Foundation's College Knowledge Challenge announced College Abacus as one its winners in January 2013; the $100,000 grant from the Gates Foundation will enable College Abacus to expand from its beta version with 2500+ schools to a fully comprehensive version with all the colleges and universities in the United States.[30]
Debt vs. grants
No-loan financial aid

In 2001, Princeton University became the first university in the United States to eliminate loans from its financial aid packages. Since then, many other schools have followed in eliminating some or all loans from their financial aid programs. Many of these programs are aimed at students whose parents earn less than a certain income — the figures vary by college or university. These new initiatives were designed to attract more students and applicants from lower socioeconomic backgrounds, reduce student debt loads, and provide the offering institutions with an advantage over their rivals in attracting commitments from accepted students. Most students prefer no-loan financial aid as a way to relieve the amount of debt they are in after college

The following colleges and universities offer such no-loan financial aid packages as of March 2008:
Post-secondary institution 	No-loan financial aid for families meeting these eligibility requirements:
Amherst College 	No max income
Arizona State University 	Arizona residents with family income of up to $60,000[31]
Bowdoin College 	No max income[32]
Brown University 	Family income below $100,000[33]
Caltech 	Annual income below $60,000[34]
Claremont McKenna College 	No max income[35]
Colby College 	No max income; all students[36]
Columbia University 	No max income[37]
Cornell University 	Annual income below $75,000
Dartmouth College 	Annual income below $100,000[38]
Davidson College 	No max income
Duke University 	Annual income below $40,000[39]
Emory University 	Annual income below $100,000
Haverford College 	No max income[40]
Harvard University 	No max income
Lafayette College 	Annual income below $50,000[41]
Lehigh University 	Annual income below $50,000[42]
MIT 	Annual income below $75,000[43]
University of Maryland, College Park 	Maryland resident with 0 EFC[44]
Michigan State University 	Michigan resident with family incomes at or below the federal poverty line[45]
Northwestern University 	Family income lower than approx. $55,000[46]
North Carolina State University 	North Carolina resident with income less than 150% of the poverty line.[47]
University of Chicago 	No max income[48]
UNC Chapel Hill 	200% of federal poverty line[49]
University of Pennsylvania 	No max income[50]
Pomona College 	No max income[51]
Princeton University 	No max income
Rice University 	Annual income below $80,000
Stanford University 	No max income
Swarthmore College 	Anyone with financial need[52]
Tufts University 	Annual income below $40,000[53]
Vanderbilt University 	No max income[54]
Vassar College 	Annual income below $60,000[55]
University of Virginia 	200% of federal poverty line ($24,000 to $37,000)
Washington and Lee University 	No max income
Washington University in St. Louis 	Annual Income below $60,000[56]
Wellesley College 	$60,000[57]
Wesleyan University 	$40,000[58]
College of William and Mary 	$40,000 (VA residents only)
Yale University 	No max income
Loan cap

Some universities have opted to have a "loan cap" program, which is a maximum loan — either per year or for the four years combined — designed to reduce the cost of attendance for low-income and middle-class students. The following schools have a loan cap program:
School 	Loan cap for students meeting these eligibility requirements:
Brown University 	Family earning less than about $125,000: Caps total loans to $3,000 per year. Family earning up to $150,000: Caps total loans to $4,000 per year. Family earning up to $150,000: Caps total loans to $5,000 per year.
University of Chicago 	"Those whose families make between $60,000 and $75,000 will have 50% of their loans replaced."[48]
Cornell University 	Undergraduates with family incomes less than $120,000 will have loans limited to $3,000 per year.
Duke University 	Undergraduate students with family income between $40,000 and $100,000 will have their loans limited on a graduated basis ($1,000 to $4,000 per year) and loans "frozen" at the freshman level.[39]
Emory University 	"Annual assessed incomes of $50,000 to $100,000 who demonstrate need for financial aid. The program caps total need-based loans at $15,000, assuming on-time progression toward graduation with up to eight semesters of study."[59]
Grinnell College 	"Beginning in the 2008-09 academic year, need-based loans for all eligible students will be capped at $2,000 per year."[60]
University of Maryland, College Park 	Students with need-based financial aid will have their loans capped at $15,900 for their four years of attendance.[44]
Middlebury College 	Family income below $40,000: $1,500 per year; family income $40,000 to $80,000: $2,500 per year; family income above $80,000: $3,500 per year.[61]
Rice University 	Students with a family income below $60,000 will not have loans. Families with incomes over $60,000 will have their loans capped at about $14,500.
University of Virginia 	200% of federal poverty line ($24,000 to $37,000). Need-based loans are capped at 25% of the in-state cost of attendance, regardless of state residency.
Effect of financial aid on enrollment

In a study on the correlation between the price of higher education and enrollment rates, Donald Heller finds that the amount of financial aid available for students is a strong factor in enrollment rates.[62]

Different factors have different effects on financial aid:

    Decreases in the amount of financial aid leads to decreases in enrollment. However, different types of financial aid have differing effects. Grant awards tend to have a stronger effect on enrollment rates.[62]
    Changes in tuition and financial aid affect poorer students more than they affect students with higher incomes.[62]
    In terms of race, changes in financial aid affects black students more than it affects white students.[62]
    Changes in financial aid affect students from community colleges more than students from four-year schools.[62]

Need-blind admissions

Need-blind admissions do not consider a student’s financial need. In a time when colleges are low on financial funds, it is difficult to maintain need-blind admissions because schools cannot meet the full need of the poor students that they admit.[63]

There are different levels of need-blind admissions. Few institutions are fully need-blind. Others are not need-blind for students who apply after certain deadlines, international students, and students from a waitlist.[63] Some institutions are moving away from need-blind admissions so that they can fulfill the full need of the students that are admitted.[63] Meeting the full-need will probably increase the funds for financial aid.[63] For example, Wesleyan University is only need-blind if it has enough money to satisfy the full need of admitted students.[63]
Outside the United States

Most national governments provide student financial aid to students attending a university, even and especially in countries where education is free. Several of these countries even provide financial aid to students in secondary schools, especially but not only if they don't live with their parents. See student financial aid for a list of articles on such countries.

Proposed changes to government-financed student financial aid have engendered considerable debate in many countries such as Canada, the United Kingdom, Germany, the Netherlands, and Scandinavian countries. The heavy reliance on private subsidies seen in the United States is not common, although this may be changing.

In Germany, the main source of financial aid is provided by the Bundesausbildungsförderungsgesetz, colloquially known as BAFöG.
See also

    College admissions in the United States
    Scholarship
    Student financial aid (list of articles on student financial aid in other countries)
    Transfer admissions in the United States

References

https://www.nerdwallet.com/blog/loans/student-loans/grants-for-college/
https://studentaid.ed.gov/sa/types/grants-scholarships/pell
https://studentaid.ed.gov/sa/types/grants-scholarships/fseog
[1]
https://studentaid.ed.gov/sa/types/loans/subsidized-unsubsidized#subsidized-vs-unsubsidized
https://studentaid.ed.gov/sa/types/loans/plus
https://studentaid.ed.gov/sa/types/loans/federal-vs-private
[2]
[3]
[4]
McPherson, M. S. & Schapiro, M. O. (2002) “The Blurring Line between Merit and Need in Financial Aid” in Change, Vol. 34, No. 2, p. 40
McPherson, M. S. & Schapiro, M. O. (2002) “The Blurring Line between Merit and Need in Financial Aid” in Change, Vol. 34, No. 2, p. 41
McPherson, M. S. & Schapiro, M. O. (2002) “The Blurring Line between Merit and Need in Financial Aid” in Change, Vol. 34, No. 2, p. 42
How College Pricing Undermines Financial Aid
Clemmitt, Marcia. "Will many low-income students be left out?". CQ Researcher.
"What Colleges Contribute: Institutional Aid to Full-Time Undergraduates Attending 4-Year Colleges and Universities--Executive Summary".
https://studentaid.ed.gov/sites/default/files/graduate-professional-funding-info.pdf
[5]
[6]
[7]
[8]
[9]
[10]
Association of Institutional Research Net Price Calculator Resource Center Archived June 13, 2010, at the Wayback Machine.
University Business, "Preparing for the Net Price Calculator: Avoid Potential Pitfalls by Taking These Steps Today," Haley Chitty, October 2009
Challenges and Opportunities: Meeting the Federal Net Price Calculator Mandate by David Childress, Bill Smith, and Marc Alexander, May 2010
Report and Suggestions from IPEDS Technical Review Panel #26 prepared by RTI International [11]
http://ticas.org/files/pub/Adding_It_All_Up_2012_NR.pdf
http://www.cbsnews.com/8301-505145_162-57536803/a-great-new-tool-for-comparing-college-costs/
http://www.collegeknowledgechallenge.org/winners/
President Barack Obama Scholars | Arizona State University
Bowdoin Eliminates Student Loans While Vowing to Maintain its Com, Campus News (Bowdoin)
07-105 (Financial Aid Changes)
Caltech Press Release, 12/11/2007, Jean-Lou Chameau
News Release, News and Events, Claremont McKenna College
Colby College | News & Events | Colby Replaces Loans With Grants, Allowing Students to Graduate Without Debt
Columbia News ::: Columbia Expands Financial Aid
Dartmouth News - Dartmouth announces new financial aid initiative - 01/22/12
New Financial Aid Support
[12]
Lafayette strengthens financial aid
Lehigh to enhance financial aid policy
MIT to be tuition-free for families earning less than $75,000 a year - MIT News Office
Interpretations, TERP Magazine Winter 2005
Spartan Advantage Program | Office of Financial Aid | Michigan State University
<Northwestern: Grants Replace Loans for Neediest Students>
Pack Promise
The University of Chicago: No application fee. No loans. Expanded career opportunities
Carolina Covenant
Penn Admissions: Paying for a Penn Education
Pomona College : News@Pomona
Swarthmore College :: Financial Aid :: More about Swarthmore's
Expanded Financial Aid Program
Tufts E-News: Tufts University Eliminates Loans for Lower Income Students
http://www.vanderbilt.edu/expandedaidprogram/
Vassar College further strengthens commitment to access and affordability
WUSTL to expand financial aid for low-income families
Wellesley College Increases Financial Aid
http://www.wesleyan.edu/cgi-bin/cdf_manager/template_renderer.cgi?item=57727
Loan Cap Program
Tuition and Financial Aid - Grinnell College
Financial Aid
Heller, Donald E. (1997). "Student Price Response in Higher Education: An Update to Leslie and Brinkman on JSTOR". The Journal of Higher Education. 68 (6): 624–659. JSTOR 2959966. doi:10.2307/2959966. Retrieved 2017-09-14.

    Need and Want. Retrieved 31 March 2013.


	
Private student loan (United States)
From Wikipedia, the free encyclopedia
  (Redirected from Private student loans)
	This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2011) (Learn how and when to remove this template message)
Student loans in the U.S.
Regulatory framework
Higher Education Act of 1965
U.S. Dept. of Education · FAFSA
Cost of attendance · Expected Family Contribution
Distribution channels
Federal Direct Student Loan Program
Federal Family Education Loan Program
Loan products
Perkins · Stafford
PLUS · Consolidation Loans
Private student loans

A private student loan is a financing option for higher education in the United States that can supplement, but should not replace, federal loans, such as Stafford loans, Perkins loans and PLUS loans. Private loans, which are heavily advertised, do not have the forbearance and deferral options available with federal loans (which are never advertised). In contrast with federal subsidized loans, interest accrues while the student is in college, although repayment may not begin until after graduation. While unsubsidized federal loans do have interest charges while the student is studying, private student loan rates are higher, sometimes much higher. Fees vary greatly, and legal cases have reported fees reaching 50% of amount of the loan.[citation needed] Although traditionally unsecured, these loans are increasingly secured, so that the borrower must offer collateral or a third-party guarantee of repayment.[citation needed]

Interest rates and loan terms are set by the financial institution that underwrites the loan, typically based on the perceived risk that the borrower may be delinquent or in default of payments of the loan. The underwriting decision is complicated by the fact that students often do not have a credit history that would indicate creditworthiness. As a result, interest rates may vary considerably across lenders, and some loans have variable interest rates.

Unlike other consumer loans, Congress made student loans, both federal and private, exempt from discharge (cancellation) in the event of a personal bankruptcy. This is a serious restriction that students rarely appreciate when obtaining a student loan.

Financial aid, including loans, may not exceed the cost of attendance.

Contents

    1 Parallels to mortgage lending
    2 Criticisms
    3 Participants
    4 Choosing a lender
    5 References
    6 External references

Parallels to mortgage lending

The increase in use of private student loans came about around 2001 once the increase in the cost of education began to exceed the increase in the amount of federal student aid available.[citation needed]

The recent history of student loans has been compared to the history of the mortgage industry.[citation needed] Similar to the way in which mortgages were securitized and sold off by lenders to investors, student loans were also sold off to investors, thereby eliminating the risk of loss for the actual lender.

Another parallel between the student loan industry and the mortgage industry is the fact that subprime lending has run rampant over the past few years.[citation needed] Just as little documentation was needed to take out a subprime mortgage loan, even less was needed to take out a subprime or "non-traditional" student loan.
Criticisms

After the passage of the bankruptcy reform bill of 2005, even private student loans are not discharged during bankruptcy. This provided a credit-risk-free loan for the lender, averaging 7 percent a year.[1]

In 2007, the Attorney General of New York State, Andrew Cuomo, led an investigation into lending practices and anti-competitive relationships between student lenders and universities. Specifically, many universities steered student borrowers to "preferred lenders" which resulted in those borrowers incurring higher interest rates. Some of these "preferred lenders" allegedly rewarded university financial aid staff with "kickbacks." This has led to changes in lending policy at many major American universities. Many universities have also rebated millions of dollars in fees back to affected borrowers.[2][3]

The biggest lenders, Sallie Mae and Nelnet, are criticized by borrowers. They frequently find themselves embroiled in lawsuits, the most serious of which was filed in 2007. The False Claims Suit was filed on behalf of the federal government by former Department of Education researcher, Dr. Jon Oberg, against Sallie Mae, Nelnet, and other lenders. Oberg argued that the lenders overcharged the U.S. Government and defrauded taxpayers of millions of dollars. In August 2010, Nelnet settled the lawsuit and paid $55 million.[4]

In 2011, The New York Times published an editorial endorsing the return of bankruptcy protections for private student loans in response to the economic downturn and universally increasing tuition at all colleges and graduate institutions.[5]

A 2014 report from Consumer Financial Protection Bureau (CFPB), shows a rising problem with these types of loans. Borrowers face “auto-default” when cosigner dies or goes bankrupt. The report shows that some lenders demand immediate full repayment upon the death or bankruptcy of their loan cosigner, even when the loan is current and being paid on time.[6]
Participants

The biggest student loan lender, Sallie Mae, was formerly a government-sponsored entity, which became private between 1997-2004. A number of financial institutions offer private student loans, including banks like Wells Fargo, and specialized companies. Student loan search and comparison websites allow visitors to evaluate loan terms from a variety of partner lenders, and financial aid offices in universities typically have a preferred vendor list, but borrowers are free to obtain loans wherever they can find the most favorable terms.[7]

As the economy collapsed in 2008-2011, many players withdrew from the private student loan lending world.[citation needed]. The remaining lenders tightened the credit criteria, making it more difficult to receive a loan. Most now require a credit-worthy cosigner.[8] After the economic collapse of 2008, a number of peer-to-peer lending and alternative lending platforms emerged to help students find private student loans. For example, U.S. online marketplace lending platform LendKey allows consumers to book loans directly from community lenders like credit unions and community banks.
Choosing a lender

Barring exceptional circumstances, all U.S. citizens and residents should use Federal student loans, which have lower interest and better terms, in preferance to private loans.

Relevant factors in choosing a private lender include:

    Interest rates throughout the life of the loan - interest may accrue at one rate while the student is in school and another after graduation
    Payment options - lenders typically offer loans on which repayment begins immediately; loans on which only the interest must be paid while the student is enrolled; and loans with no payments until graduation or withdrawal from school. In the latter case, the unpaid interest while in school is capitalized (added to the balance due).
    Incentives - lenders may offer improved terms based on the student's payment record
    Origination fees - lenders typically charge a fee for originating the loan; the fee is added to the principal (amount borrowed).

The total cost of the loan is documented in the Truth in Lending statement, which the borrower should receive when the loan is originated.
References

Collinge, Alan. The student loan scam : the most oppressive debt in U.S. history, and how we can fight back. Boston, MA : Beacon Press, c2009. ISBN 978-0-8070-4229-8 http://lccn.loc.gov/2008012230
"Cuomo: School loan corruption widespread". U.S.A. Today. April 10, 2007. Retrieved 2008-04-08.
Lederman, Doug (May 15, 2007). "The First Casualty". Inside Higher Education. Retrieved 2008-04-08.
Field, Kelly (August 15, 2010). "Nelnet to Pay $55 Million to Resolve Whistle Blower Lawsuit". The Chronicle of Higher Education. Retrieved 2011-07-14.
"Relief for Student Debtors". The New York Times. 26 August 2011.
PÉREZ-PEÑA, RICHARD (April 22, 2014). "Student Loans Can Suddenly Come Due When Co-Signers Die, a Report Finds". The New York Times.
Clark, Jane Bennett (July 2007). "Best Deal on Student Loans". Kiplinger's Personal Finance. Retrieved 6 July 2010.

	
Wikipedia:Verifiability
From Wikipedia, the free encyclopedia
"WP:V" redirects here. To discuss particular sources, see the reliable sources noticeboard. For vandalism, see WP:VD. For the default Wikipedia skin, see WP:VECTOR.
Green check.svg 	
This page documents an English Wikipedia policy.
It describes a widely accepted standard that all editors should normally follow. Changes made to it should reflect consensus.
	
Shortcuts:

    WP:V
    WP:VER
    WP:VERIFY

	This page in a nutshell: Readers must be able to check that any of the information within Wikipedia articles is not just made up. This means all material must be attributable to reliable, published sources. Additionally, quotations and any material challenged or likely to be challenged must be supported by inline citations.
Core content policies

    Neutral point of view
    No original research
    Verifiability

Other content policies

    Article titles
    Biographies of living persons
    Image use policy
    What Wikipedia is not

    v t e 

In Wikipedia, verifiability means that other people using the encyclopedia can check that the information comes from a reliable source. Wikipedia does not publish original research. Its content is determined by previously published information rather than the beliefs or experiences of its editors. Even if you're sure something is true, it must be verifiable before you can add it.[1] When reliable sources disagree, maintain a neutral point of view and present what the various sources say, giving each side its due weight.

All material in Wikipedia mainspace, including everything in articles, lists and captions, must be verifiable. All quotations, and any material whose verifiability has been challenged or is likely to be challenged, must include an inline citation that directly supports the material. Any material that needs a source but does not have one may be removed. Please immediately remove contentious material about living people that is unsourced or poorly sourced.

For how to write citations, see citing sources. Verifiability, no original research and neutral point of view are Wikipedia's core content policies. They work together to determine content, so editors should understand the key points of all three. Articles must also comply with the copyright policy.

Contents

    1 Responsibility for providing citations
    2 Reliable sources
        2.1 What counts as a reliable source
        2.2 Newspaper and magazine blogs
        2.3 Reliable sources noticeboard and WP:IRS
    3 Sources that are usually not reliable
        3.1 Questionable sources
        3.2 Self-published sources
        3.3 Self-published or questionable sources as sources on themselves
        3.4 Wikipedia and sources that mirror or use it
    4 Accessibility
        4.1 Access to sources
        4.2 Non-English sources
            4.2.1 Citing non-English sources
            4.2.2 Quoting non-English sources
    5 Other issues
        5.1 Verifiability does not guarantee inclusion
        5.2 Tagging a sentence, section, or article
        5.3 Exceptional claims require exceptional sources
    6 Verifiability and other principles
        6.1 Copyright and plagiarism
        6.2 Neutrality
        6.3 Notability
        6.4 Original research
    7 See also
    8 Notes
    9 Further reading

Responsibility for providing citations
Policy shortcuts:

    WP:UNSOURCED
    WP:CHALLENGE
    WP:BURDEN
    WP:PROVEIT

"WP:PROVEIT" redirects here. For the editing tool, see Wikipedia:ProveIt.

All content must be verifiable. The burden to demonstrate verifiability lies with the editor who adds or restores material, and is satisfied by providing a citation to a reliable source that directly supports the contribution.[2]

Attribute all quotations and any material whose verifiability is challenged or likely to be challenged to a reliable, published source using an inline citation. The cited source must clearly support the material as presented in the article. Cite the source clearly and precisely (specifying page, section, or such divisions as may be appropriate). See Citing sources for details of how to do this.

Any material lacking a reliable source directly supporting it may be removed and should not be restored without an inline citation to a reliable source. Whether and how quickly material should be initially removed for not having an inline citation to a reliable source depends on the material and the overall state of the article. In some cases, editors may object if you remove material without giving them time to provide references; consider adding a citation needed tag as an interim step.[3] When tagging or removing material for lacking an inline citation, please state your concern that it may not be possible to find a published reliable source for the content, and therefore it may not be verifiable.[4] If you think the material is verifiable, you are encouraged to provide an inline citation yourself before considering whether to remove or tag it.

Do not leave unsourced or poorly sourced material in an article if it might damage the reputation of living people[5] or existing groups, and do not move it to the talk page. You should also be aware of how the BLP policy applies to groups.
Reliable sources
Policy shortcuts:

    WP:SOURCE
    WP:SOURCES

"WP:SOURCE" redirects here. For the <source> tag, see Wikipedia:Syntaxhighlight. For how to reference sources, see Help:Referencing for beginners.
What counts as a reliable source
Further information: Wikipedia:Identifying reliable sources

The word "source" when citing sources on Wikipedia has three related meanings:

    The piece of work itself (the article, book)
    The creator of the work (the writer, journalist)
    The publisher of the work (for example, Random House or Cambridge University Press)

All three can affect reliability.

Base articles on reliable, third-party, published sources with a reputation for fact-checking and accuracy. Source material must have been published, the definition of which for our purposes is "made available to the public in some form".[6] Unpublished materials are not considered reliable. Use sources that directly support the material presented in an article and are appropriate to the claims made. The appropriateness of any source depends on the context. The best sources have a professional structure in place for checking or analyzing facts, legal issues, evidence, and arguments. The greater the degree of scrutiny given to these issues, the more reliable the source. Be especially careful when sourcing content related to living people or medicine.

If available, academic and peer-reviewed publications are usually the most reliable sources, such as in history, medicine, and science.

Editors may also use material from reliable non-academic sources, particularly if it appears in respected mainstream publications. Other reliable sources include:

    University-level textbooks
    Books published by respected publishing houses
    Magazines
    Journals
    Mainstream newspapers

Editors may also use electronic media, subject to the same criteria. See details in Wikipedia:Identifying reliable sources and Wikipedia:Search engine test.
Newspaper and magazine blogs
Policy shortcut:

    WP:NEWSBLOG

Several newspapers, magazines, and other news organizations host columns on their web sites that they call blogs. These may be acceptable sources if the writers are professionals, but use them with caution because the blog may not be subject to the news organization's normal fact-checking process.[7] If a news organization publishes an opinion piece in a blog, attribute the statement to the writer (e.g. "Jane Smith wrote..."). Never use as sources the blog comments that are left by readers. For personal or group blogs that are not reliable sources, see Self-published sources below.
Reliable sources noticeboard and WP:IRS
Further information: Wikipedia:Reliable sources noticeboard and Wikipedia:Identifying reliable sources

To discuss the reliability of a specific source for a particular statement, consult the reliable sources noticeboard, which seeks to apply this policy to particular cases. For a guideline discussing the reliability of particular types of sources, see Wikipedia:Identifying reliable sources (WP:IRS). In the case of inconsistency between this policy and the WP:IRS guideline, or any other guideline related to sourcing, this policy has priority.
Sources that are usually not reliable
See also: Wikipedia:Identifying reliable sources § Questionable and self-published sources
Policy shortcuts:

    WP:NOTRELIABLE
    WP:NOTRS
    WP:QS

Questionable sources

Questionable sources are those that have a poor reputation for checking the facts, lack meaningful editorial oversight, or have an apparent conflict of interest.[8] Such sources include websites and publications expressing views that are widely considered by other sources to be extremist or promotional, or that rely heavily on unsubstantiated gossip, rumor or personal opinion. Questionable sources should only be used as sources for material on themselves, such as in articles about themselves; see below. They are not suitable sources for contentious claims about others.
Self-published sources
Policy shortcuts:

    WP:SPS
    WP:SELFPUBLISH
    WP:BLOGS

Further information: Wikipedia:Biographies of living persons § Avoid self-published sources, and Wikipedia:List of self-publishing companies

Anyone can create a personal web page or publish their own book, and also claim to be an expert in a certain field. For that reason, self-published media, such as books, patents, newsletters, personal websites, open wikis, personal or group blogs (as distinguished from newsblogs, above), content farms, Internet forum postings, and social media postings, are largely not acceptable as sources. Self-published expert sources may be considered reliable when produced by an established expert on the subject matter, whose work in the relevant field has previously been published by reliable third-party publications.[7] Exercise caution when using such sources: if the information in question is really worth reporting, someone else will probably have published it in independent reliable sources.[9] Never use self-published sources as third-party sources about living people, even if the author is an expert, well-known professional researcher, or writer.
Self-published or questionable sources as sources on themselves
"WP:SOCIALMEDIA" redirects here. For the policy on what Wikipedia is not, see WP:NOTSOCIALNETWORK.
"WP:TWITTER" redirects here. For the external links essay, see WP:Twitter-EL.
Policy shortcuts:

    WP:ABOUTSELF
    WP:SELFPUB
    WP:TWITTER
    WP:SOCIALMEDIA

Self-published and questionable sources may be used as sources of information about themselves, usually in articles about themselves or their activities, without the self-published source requirement that they be published experts in the field, so long as:

    the material is neither unduly self-serving nor an exceptional claim;
    it does not involve claims about third parties;
    it does not involve claims about events not directly related to the source;
    there is no reasonable doubt as to its authenticity;
    the article is not based primarily on such sources.

This policy also applies to material published by the subject on social networking websites such as Twitter, Tumblr, Reddit, and Facebook.
Wikipedia and sources that mirror or use it
Policy shortcuts:

    WP:CIRC
    WP:CIRCULAR
    WP:REFLOOP

"WP:CIRCULAR" redirects here. For links on a page that refer back to the same page, see Wikipedia:Redirect § Self-redirects.
See also: WP:COPYWITHIN and Wikipedia:List of citogenesis incidents

Do not use articles from Wikipedia (whether this English Wikipedia or Wikipedias in other languages) as sources. Also, do not use websites that mirror Wikipedia content or publications that rely on material from Wikipedia as sources. Content from a Wikipedia article is not considered reliable unless it is backed up by citing reliable sources. Confirm that these sources support the content, then use them directly.[10] (There is also a risk of circular reference/circular reporting when using a Wikipedia article or derivative work as a source.)

An exception is allowed when Wikipedia itself is being discussed in the article, which may cite an article, guideline, discussion, statistic, or other content from Wikipedia (or a sister project) to support a statement about Wikipedia. Wikipedia or the sister project is a primary source in this case, and may be used following the policy for primary sources. Any such use should avoid original research, undue emphasis on Wikipedia's role or views, and inappropriate self-reference. The article text should make it clear that the material is sourced from Wikipedia so the reader is made aware of the potential bias.
Accessibility
Access to sources
Policy shortcuts:

    WP:PAYWALL
    WP:SOURCEACCESS

See also: Wikipedia:Offline sources, Wikipedia:WikiProject Resource Exchange/Resource Request, and Wikipedia:Reliable sources/Cost

Some reliable sources may not be easily accessible. For example, an online source may require payment, and a print-only source may be available only in university libraries. Do not reject reliable sources just because they are difficult or costly to access. If you have trouble accessing a source, others may be able to do so on your behalf (see WikiProject Resource Exchange).
Non-English sources
Policy shortcuts:

    WP:RSUE
    WP:NOENG
    WP:NONENG

See also: Wikipedia:Translators available and Wikipedia:No original research § Translations and transcriptions
Citing non-English sources

Citations to non-English reliable sources are allowed on the English Wikipedia. However, because this project is in English, English-language sources are preferred over non-English ones when available and of equal quality and relevance. As with sources in English, if a dispute arises involving a citation to a non-English source, editors may request that a quotation of relevant portions of the original source be provided, either in text, in a footnote, or on the article talk page.[11] (See Template:Request quotation.)
Quoting non-English sources

If you quote a non-English reliable source (whether in the main text or in a footnote), a translation into English should always accompany the quote. Translations published by reliable sources are preferred over translations by Wikipedians, but translations by Wikipedians are preferred over machine translations. When using a machine translation of source material, editors should be reasonably certain that the translation is accurate and the source is appropriate. Editors should not rely upon machine translations of non-English sources in contentious articles or biographies of living people. If needed, ask an editor who can translate it for you.

In articles, the original text is usually included with the translated text when translated by Wikipedians, and the translating editor is usually not cited. When quoting any material, whether in English or in some other language, be careful not to violate copyright; see the fair-use guideline.
Other issues
Verifiability does not guarantee inclusion
See also: WP:UNDUE, WP:PAGEDECIDE, WP:PRESERVE, and WP:SUMMARY
Shortcuts:

    WP:VNOTSUFF
    WP:ONUS

While information must be verifiable in order to be included in an article, this does not mean that all verifiable information must be included in an article. Consensus may determine that certain information does not improve an article, and that it should be omitted or presented instead in a different article. The onus to achieve consensus for inclusion is on those seeking to include disputed content.
Tagging a sentence, section, or article
Further information: Wikipedia:Citation needed and Wikipedia:Template messages/Sources of articles
Shortcut:

    WP:FAILEDVERIFICATION

If you want to request a source for an unsourced statement, you can tag a sentence with the {{citation needed}} template by writing {{cn}} or {{fact}}. There are other templates here for tagging sections or entire articles. You can also leave a note on the talk page asking for a source, or move the material to the talk page and ask for a source there. To request verification that a reference supports the text, tag it with {{verification needed}}. Material that fails verification may be tagged with {{failed verification}} or removed. When using templates to tag material, it is helpful to other editors if you explain your rationale in the template, edit summary, or on the talk page.

Take special care with material about living people. Contentious material about living people that is unsourced or poorly sourced should be removed immediately, not tagged or moved to the talk page.
Exceptional claims require exceptional sources
Policy shortcuts:

    WP:REDFLAG
    WP:EXCEPTIONAL
    WP:EXTRAORDINARY

See also: Wikipedia:Fringe theories

Any exceptional claim requires multiple high-quality sources.[12] Red flags that should prompt extra caution include:

    surprising or apparently important claims not covered by multiple mainstream sources;
    challenged claims that are supported purely by primary or self-published sources or those with an apparent conflict of interest;[8]
    reports of a statement by someone that seems out of character, or against an interest they had previously defended;
    claims that are contradicted by the prevailing view within the relevant community, or that would significantly alter mainstream assumptions, especially in science, medicine, history, politics, and biographies of living people. This is especially true when proponents say there is a conspiracy to silence them.

Verifiability and other principles
Copyright and plagiarism
Policy shortcut:

    WP:YTCOPYRIGHT

Further information: Wikipedia:Copyright, Wikipedia:Plagiarism, Wikipedia:Copying within Wikipedia, Wikipedia:MOS § Attribution, and Wikipedia:CITE § In-text attribution

Do not plagiarize or breach copyright when using sources. Summarize source material in your own words as much as possible; when quoting or closely paraphrasing a source use an inline citation, and in-text attribution where appropriate.

Do not link to any source that violates the copyrights of others per contributors' rights and obligations. You can link to websites that display copyrighted works as long as the website has licensed the work, or uses the work in a way compliant with fair use. Knowingly directing others to material that violates copyright may be considered contributory copyright infringement. If there is reason to think a source violates copyright, do not cite it. This is particularly relevant when linking to sites such as Scribd or YouTube, where due care should be taken to avoid linking to material that violates copyright.
Neutrality
Further information: Wikipedia:Neutral point of view

Even when information is cited to reliable sources, you must present it with a neutral point of view (NPOV). All articles must adhere to NPOV, fairly representing all majority and significant-minority viewpoints published by reliable sources, in rough proportion to the prominence of each view. Tiny-minority views need not be included, except in articles devoted to them. If there is disagreement between sources, use in-text attribution: "John Smith argues that X, while Paul Jones maintains that Y," followed by an inline citation. Sources themselves do not need to maintain a neutral point of view. Indeed, many reliable sources are not neutral. Our job as editors is simply to summarize what the reliable sources say.
Notability
Further information: Wikipedia:Notability

If no reliable third-party sources can be found on a topic, Wikipedia should not have an article on it.
Original research
Further information: Wikipedia:No original research

The "No original research" policy (NOR) is closely related to the Verifiability policy. Among its requirements are:

    All material in Wikipedia articles must be attributable to a reliable published source. This means that a source must exist for it, whether or not it is cited in the article.
    Sources must support the material clearly and directly: drawing inferences from multiple sources to advance a novel position is prohibited by the NOR policy.[11]
    Base articles largely on reliable secondary sources. While primary sources are appropriate in some cases, relying on them can be problematic. For more information, see the Primary, secondary, and tertiary sources section of the NOR policy, and the Misuse of primary sources section of the BLP policy.

See also

    Wikipedia:Identifying reliable sources (medicine), a guideline
    Wikipedia:List of free online resources
    Wikipedia:Template messages/Sources of articles
    Wikipedia:Wikipedia is not a reliable source
    Wikipedia:WikiProject Fact and Reference Check
    Wikipedia:WikiProject Resource Exchange
    Wikipedia:Core content policies

Related essays

    Wikipedia:Citation clutter
    Wikipedia:How to mine a source
    Wikipedia:Improving referencing efforts
    Wikipedia:Use of tertiary sources
    Wikipedia:Verifiability, not truth
    Wikipedia:Video links
    Wikipedia:When to cite

Notes

This principle was previously expressed on this policy page as "the threshold for inclusion is verifiability, not truth." See the essay, WP:Verifiability, not truth.
Once an editor has provided any source that he or she believes, in good faith, to be sufficient, then any editor who later removes the material has an obligation to articulate specific problems that would justify its exclusion from Wikipedia (e.g., undue emphasis on a minor point, unencyclopedic content, etc.). All editors are then expected to help achieve consensus, and any problems with the text or sourcing should be fixed before the material is added back.
It may be that the article contains so few citations that it is impractical to add specific citation needed tags, in which case consider tagging a section with {{unreferencedsection}}, or the article with {{refimprove}} or {{unreferenced}}. In the case of a disputed category or on a disambiguation page, consider asking for a citation on the talk page.
When tagging or removing such material, please keep in mind that such edits can be easily misunderstood. Some editors object to others making chronic, frequent, and large-scale deletions of unsourced information, especially if unaccompanied by other efforts to improve the material. Do not concentrate only on material of a particular POV, as that may result in accusations that you are in violation of WP:NPOV. Also check to see whether the material is sourced to a citation elsewhere on the page. For all of these reasons, it is advisable to communicate clearly that you have a considered reason to believe that the material in question cannot be verified.
Wales, Jimmy. "Zero information is preferred to misleading or false information", WikiEN-l, May 16, 2006: "I can NOT emphasize this enough. There seems to be a terrible bias among some editors that some sort of random speculative 'I heard it somewhere' pseudo information is to be tagged with a 'needs a cite' tag. Wrong. It should be removed, aggressively, unless it can be sourced. This is true of all information, but it is particularly true of negative information about living persons."
This includes material such as documents in publicly accessible archives, inscriptions on monuments, gravestones, etc., that are available for anyone to see.
Please do note that any exceptional claim would require exceptional sources.
Sources that may have interests other than professional considerations in the matter being reported are considered to be conflicted sources. Further examples of sources with conflicts of interest include but are not limited to articles by any media group that promote the holding company of the media group or discredit its competitors; news reports by journalists having financial interests in the companies being reported or in their competitors; material (including but not limited to news reports, books, articles and other publications) involved in or struck down by litigation in any country, or released by parties involved in litigation against other involved parties, during, before or after the litigation; and promotional material released through media in the form of paid news reports. For definitions of sources with conflict of interest:

    The Columbia Center for New Media Teaching and Learning, Columbia University mentions: "A conflict of interest involves the abuse – actual, apparent, or potential – of the trust that people have in professionals. The simplest working definition states: A conflict of interest is a situation in which financial or other personal considerations have the potential to compromise or bias professional judgment and objectivity. An apparent conflict of interest is one in which a reasonable person would think that the professional's judgment is likely to be compromised. A potential conflict of interest involves a situation that may develop into an actual conflict of interest. It is important to note that a conflict of interest exists whether or not decisions are affected by a personal interest; a conflict of interest implies only the potential for bias, not a likelihood. It is also important to note that a conflict of interest is not considered misconduct in research, since the definition for misconduct is currently limited to fabrication, falsification, and plagiarism."
    The New York Times Company forwards this understanding: "Conflicts of interest, real or apparent, may come up in many areas. They may involve the relationships of staff members with readers, news sources, advocacy groups, advertisers, or competitors; with one another, or with the newspaper or its parent company. And at a time when two-career families are the norm, the civic and professional activities of spouses, family and companions can create conflicts or the appearance of conflicts."

Self-published material is characterized by the lack of independent reviewers (those without a conflict of interest) validating the reliability of content. Further examples of self-published sources include press releases, material contained within company websites, advertising campaigns, material published in media by the owner(s)/publisher(s) of the media group, self-released music albums and electoral manifestos:

    The University of California, Berkeley library states: "Most pages found in general search engines for the web are self-published or published by businesses small and large with motives to get you to buy something or believe a point of view. Even within university and library web sites, there can be many pages that the institution does not try to oversee."
    Princeton University offers this understanding in its publication, Academic Integrity at Princeton (2011): "Unlike most books and journal articles, which undergo strict editorial review before publication, much of the information on the Web is self-published. To be sure, there are many websites in which you can have confidence: mainstream newspapers, refereed electronic journals, and university, library, and government collections of data. But for vast amounts of Web-based information, no impartial reviewers have evaluated the accuracy or fairness of such material before it's made instantly available across the globe."
    The Chicago Manual of Style, 16th Edition[dead link] states, "any Internet site that does not have a specific publisher or sponsoring body should be treated as unpublished or self-published work."

Rekdal, Ole Bjørn (1 August 2014). "Academic urban legends". Social Studies of Science. 44 (4): 638–654. ISSN 0306-3127. doi:10.1177/0306312714535679. Retrieved 30 April 2016.
When there is dispute about whether a piece of text is fully supported by a given source, direct quotes and other relevant details from the source should be provided to other editors as a courtesy. Do not violate the source's copyright when doing so.

    Hume, David. An Enquiry concerning Human Understanding, Forgotten Books, 1984, pp. 82, 86; first published in 1748 as Philosophical enquiries concerning human Understanding, (or the Oxford 1894 edition OL 7067396M at para. 91) "A wise man ... proportions his belief to the evidence. ... That no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavours to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior." In the 18th century, Pierre-Simon Laplace reformulated the idea as "The weight of evidence for an extraordinary claim must be proportioned to its strangeness." Marcello Truzzi recast it again, in 1978, as "An extraordinary claim requires extraordinary proof." Carl Sagan, finally, popularized the concept broadly as "Extraordinary claims require extraordinary evidence" in 1980 on Cosmos: A Personal Voyage; this was the formulation originally used on Wikipedia.

Further reading

    Wales, Jimmy. "Insist on sources", WikiEN-l, July 19, 2006: "I really want to encourage a much stronger culture which says: it is better to have no information, than to have information like this, with no sources."—referring to a rather unlikely statement about the founders of Google throwing pies at each other.


[hide]

    v t e 

Wikipedia referencing
Policies and guidelines 	

    Verifiability Biographies of living persons Identifying reliable sources
        Medicine Citing sources Scientific citations 

General advice 	

    Citation needed Find sources Combining sources Offline sources Referencing styles 

Citing sources 	

    Citation Style 1 Citation Style 2 Citation Style Vancouver LSA Comics Citation templates Reflist template 

Inline citations 	

    Embedded citations Footnotes Parenthetical referencing Punctuation and footnotes Shortened footnotes Nesting footnotes 

Help for beginners 	

    Reference-tags Citations quick reference Introduction to referencing Referencing with citation templates Referencing without using templates Referencing dos and don'ts Citing Wikipedia 

Advanced help 	

    Cite link labels Citation tools Cite errors Cite messages Converting between references formats Reference display customization References and page numbers 

	
Personal web page
From Wikipedia, the free encyclopedia
	
This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)
This article is written like a personal reflection or opinion essay that states a Wikipedia editor's personal feelings about a topic. (December 2012)
This article possibly contains original research. (August 2007)
This article needs additional citations for verification. (October 2014)
The personal "homepage" or web page of athlete Keira Shepherd. The "toolbar" on the top of the page contains links to additional content, such as more digital photos, information about her sponsors, press clippings and news links, a calendar of her appearances at athletic competitions, and contact information.

Personal web pages are World Wide Web pages created by an individual to contain content of a personal nature rather than content pertaining to a company, organization or institution. Personal web pages are primarily used for informative or entertainment purposes but can also be used for personal career marketing (by containing a list of the individuals skills, experience and a CV), social networking with other people with shared interests, or as a space for personal expression.

The terms "personal web site", "personal home page", "home page" or "homepage" are also used to refer to personal web pages. These terms do not usually refer to just a single "page" or HTML file, but to a collection of webpages and related files under a common URL or Web address. In strictly technical terms, a site's actual home page (index page) often only contains sparse content with some catchy introductory material and serves mostly as a pointer or table of contents to the more content-rich pages inside, such as résumés, family, hobbies, family genealogy, a web log/diary ("blog"), opinions, online journals and diaries or other writing, examples of written work, digital audio sound clips, digital video clips, digital photos, or information about a user's other interests.[1] Many personal pages only include information of interest to friends and family of the author. However, some webpages set up by hobbyists or enthusiasts of certain subject areas can be valuable topical web directories.

Contents

    1 History
    2 Motivations
    3 Contrast with social network accounts
    4 Official celebrity sites
    5 Sites of academics
    6 Domain names
    7 See also
    8 References

History

Since the early 1990s, most Internet Service Providers (ISPs) provided a free small personal, user-created webpage along with free Usenet News service. These were all considered part of full Internet service. Also several free web hosting services such as Geocities provided free web space for personal web pages. These free web hosting services would typically include web-based site management and a few pre-configured scripts to easily integrate an input form or guestbook script into the user's site. Since the early 2000s, the rise of blogging and the development of user friendly web page designing software made it easier for amateur users who did not have computer programming or website designer training to create personal web pages. Some website design websites provided free ready-made blogging scripts, where all the user had to do was input her content into a template. An example of this kind of application is My Blog. At the same time, a personal web presence became easier with the increased popularity of social networking services, some with blogging platforms such as LiveJournal and Blogger. These websites provided an attractive and easy-to-use content management system for regular users. Most of the early personal websites were Web 1.0 style, in which a static display of text and images or photos was displayed to individuals who came to the page. About the only interaction that was possible on these early websites was signing the virtual "guestbook".

With the collapse of the dot-com bubble in the late 1990s, the ISP industry consolidated, and the focus of web hosting services shifted away from the surviving ISP companies to independent Internet hosting services and to ones with other affiliations. For example, many university departments provided personal pages for professors and television broadcasters provided them for their on-air personalities. These free webpages served as a perquisite ("perk") for staff, while at the same time boosting the Web visibility of the parent organization. Web hosting companies either charge a monthly fee, or provide service that is "free" (advertising based) for personal web pages. These are priced or limited according to the total size of all files in bytes on the host's hard drive, or by bandwidth, (traffic), or by some combination of both. For those customers who continue to use their ISP for these services, national ISPs commonly continue to provide both disk space and help including ready-made drop-in scripts.[2]

With the rise of Web 2.0-style websites, both professional websites and user-created, amateur websites tended to contain interactive features, such as "clickable" links to online newspaper articles or favourite websites, the option to comment on content displayed on the website, the option to "tag" images, videos or links on the site, the option of "clicking" on an image to enlarge it or find out more information, the option of User participation for website guests to evaluate or review the pages, or even the option to create new user-generated content for others to see. A key difference between Web 1.0 personal webpages and Web 2.0 personal pages was while the former tended to be created by hackers, computer programmers and computer hobbyists, the latter were created by a much wider variety of users, including individuals whose main interests lay in hobbies or topics outside of computers (e.g., indie music fans, political activists, and social entrepreneurs).
Motivations
A personal web page user's logo from their homepage.

Many people maintain personal web pages to express their opinions on issues ranging from news and politics to movies or to serve as a showcase for their creative endeavors in writing, songwriting, poetry or music. They also provide a link from the world to the individual (and from the individual to the world) along the lines of a telephone book listing. Some people use their personal web page as a communication method. For example, an aspiring artist might give out business cards with her personal web page, and invite people to visit her page and see her artwork, "like" her page or sign her guestbook. For users not well-versed in HTML and other Web technologies, personal accounts with social networking services may be faster to set up for creating a simple personal Web presence (due in part to the communal nature of social networks), provided that the page's author does not object to the network's online advertising and in some cases exclusion of readers and browsers who do not wish to open an account. Institutions such as universities often provide home page facilities to their members which are both advertisement-free and world-readable without registration, although the content might be subject to institutional rules (e.g., against inappropriate photos or hate speech).

A personal web page can be used for self promotion for a person's small business or entrepreneurial venture, to promote an amateur rock band they play in, to promote community activities or charitable causes they support, to provide quick access to information about the user, or just as something "cool" to do.[3] A personal web page gives the owner generally more control on presence in search results and how he/she wishes to be viewed online. It also allows more freedom in types and quantity of content than a social network profile offers,[4] and can link various social media profiles with each other. It can be used to correct the record on something, or clear up potential confusion between you and someone with the same name.[5][6] Early personal web pages were often called "home pages" and were intended to be set as a default page in a web browser's preferences, usually by their owner. These pages would often contain links, to-do lists, and other information their author found useful. In the days when search engines were in their infancy, these pages (and the links they contained) could be an important resource in navigating the web.[citation needed]

In the 2010s, some amateur writers, bands and filmmakers release digital versions of their stories, songs and short films online, with the aim of gaining an audience and becoming more well-known. While the huge number of aspiring artists posting their work online makes it unlikely for individuals and groups to become popular via the Internet, there are a small number of YouTube stars who were unknown until their online performances garnered them a huge audience.
Contrast with social network accounts
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2014) (Learn how and when to remove this template message)

Both individual, personal web sites and personal social networking accounts give the user a personally customized Web presence. In the 2010s most casual Internet users join free social networking services such as Twitter, Facebook, or Google+ accounts to serve many of the same purposes as a personal webpage without having to learn web design and writing HTML markup. Yet that prerequisite in many cases is not required, as Web hosting services provide more help to enable regular users to create Web pages.[7]

Social networks often used prefabricated "black box" structures. On one hand, these templates are much easier for neophyte users to work with, since users simply have to add in information in spaces which indicate the required information. Once the user "saves" or finishes entering the information, the social network website's software system automatically creates a fairly professional-looking layout. "Black box" templates are much simpler to begin using and navigating, but more advanced users may be frustrated that they cannot "tweak" the formatting, amount of content, type of content, etc. For example, most social networks have rules regarding casual users who are uploading (loading files onto the website) audio files to their account. Furthermore, these companies intentionally retain the specific service's look and feel and identity of each user personal account within that corporate social network. For example, all profiles may have the same background color, font and website trademark. The emphasis there is on being part of a branded "network," not on the "personal," or the individual. Thus, these accounts are not normally thought of as (personal) web sites or home pages.

There are other differences. Unlike actual personal web pages, social networking services and ad-based "free" web hosting service personnel, advertisers and nanny-bots can see everything inside the user accounts, and rules are enforced by the firm,[citation needed] not by the courts as would be the case with a personally owned, full-featured personal web page.[citation needed] However some social services allow the display of almost any content or media produced by the site's creator. This avenue of distribution satisfies most amateurs and aspiring content creators. Web site creation tools permitted by some companies have the potential to allow users more flexibility. As a rule of thumb, the amount of creative freedom one is allowed in creating a personal Web page can be predicted by the site's URL. A pure URL similar to www.yourname.com predicts total ownership and the resulting rights. But a shared-name URL similar to www.yourname.home.othercompany.com suggests account rental and regulations which benefit or protect a corporation (in this case, Othercompany Inc).[citation needed]

"Free" sites based on advertising revenue face the dilemma that while relaxation of the rules encourages users to post their feelings and opinions and upload user-generated content with less fear of being censored or shut down, it also increases the risk of an offended sponsor pulling its sponsorship, if offensive materials or comments are made online. With more uploading and content-posting freedom comes an increased risk of copyright and defamation lawsuits, hate speech charges and other legal problems. Free hosting services do not allow users many options to customize the look of pages, because this would reduce page uniformity, thus reducing the common "look and feel" on the website, which becomes a key part of its identity and "branding". In short, if a social networking company allowed total personal freedom of content posting and profile modification for users, it also risks a degradation of its own look-and-feel, branding, function, and profit and legal risks. In the 2010s, this balance of interests is leading toward more user choices and a narrowing of the differences between personal web sites and other personal web presence providers.
Official celebrity sites
	
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2014) (Learn how and when to remove this template message)

Many celebrities from the movies, TV shows, professional sports and popular music have websites. Were their owners not famous, some might think due to their tone and personal ambiance that these sites were personal web pages. However, the celebrity is the "product" or brand being sold, and however casual a celebrity website may appear, with short blog posts and comments appearing on a regular basis, these are typically professionally authored and maintained. Some celebrities' public relations firms and managers hire ghostwriters to author blog posts and Tweets in the style of the celebrity. The celebrity status of the subject and the existence of separate fan-created sites (over which the celebrity in question has no direct control) leads the existence of multiple websites for each celebrity: a personal site authorized by the celebrity and maintained by an individual or company directly associated with the celebrity to be labeled an "official website", and one or more fan-run websites. This designation is often a seal of approval and an assurance to the public that the information provided on the site (including press releases, tour dates, and promotional materials) has been authored or approved by the celebrity in question. Some celebrities involved in criminal and civil trials, such as late pop star Michael Jackson and media mogul Martha Stewart, as well as celebrity chef Paula Deen establish official websites to issue statements to the press and to respond to statements and press releases issued by the prosecuting officials. Most celebrity sites are created and maintained by marketing and web professionals employed by the celebrity or the celebrity's publicist; however, some celebrities, such as film director Roger Avary, actor Wil Wheaton, and video game developer John Romero, maintain their own official sites without professional help, although many of them still use third-party templates and blogging software.
Sites of academics

Academic professionals (especially at the college and university level), including professors and researchers, are often given online space for creating and storing personal web documents, including personal web pages, CVs and a list of their books, academic papers and conference presentations, on the websites of their employers. This goes back to the early decade of the World Wide Web and its original purpose of providing a quick and easy way for academics to share research papers and data. Many professors post a digital photo on their profile page. In the 2010s, some professors and researchers have excerpts from their published articles and books, or even the full text of some articles. Some academics in the 2010s use content-rich, interactive Web 2.0 features on their websites, such as clips of television interviews, podcasts, and space for online comments or discussion.
Domain names

Many people choose a domain name like FirstnameLastname.com to host their personal website on (e.g., patxhosa.com), whereas outside the English-speaking world, the home country's top level domain (TLD) is commonly used. People with common names may choose to add their middle name or initial in the URL if the primary choice of domain name is already in use.[8] For example, a woman named "Jane Doe" will probably find that "janedoe.com" is already taken, so she may have to use a variant of her name, such as adding in a number ("janedoe1.com"), a birth year ("janedoe1980.com") or a middle initial ("janeqdoe.com"). The .name TLD is specifically intended to be used for personal web pages, but has not proven to be popular. Personal websites may instead use other generic TLDs like .me, .co, .net and .info, but also .com, .biz and .org, even though individuals rarely think of themselves as companies or (non-profit) organizations. Some people opt to find a TLD that forms a word when combined with the domain name; this is known as domain hacking.
See also

    Blog
    Blog hosting service
    Digerati
    Electronic portfolio
    Free web hosting service
    Home server
    IndieWeb
    Social networking service

References

"People's Personal Web Sites - People Tell About Themselves" ..."some great personal Web sites" -- about.com.
HostingLords.com: "Personal Web Page [1]"
Eight Clever Things You Can Do with Your Underused Personal Domain Name - lifehacker.com
Avenue, Next. "Beyond LinkedIn - Why You Need Your Own Website For A Job Search".
Motivation for: "A Web Page of One's Own" - WSJ.com -- http://online.wsj.com/article/SB121562102257039585.html
"Mark Zuckerberg - Facebook - The Social Network".
Create a free personal web page with Google -- "Get started with [free] Personal Web Pages"

    "8 Great Examples of Personal Domain Names in Action". 14 June 2012.

	
Activism
From Wikipedia, the free encyclopedia
  (Redirected from Political activism)
This article is about political activism. It is not to be confused with ethical activism.
Barricade at the Paris Commune, 1871
Civil rights activists at the March on Washington for Jobs and Freedom during the Civil Rights Movement in 1963
A Women's Liberation march in Washington, D.C., 1970

Activism consists of efforts to promote, impede, or direct social, political, economic, or environmental reform or stasis with the desire to make improvements in society. Forms of activism range from writing letters to newspapers or to politicians, political campaigning, economic activism such as boycotts or preferentially patronizing businesses, rallies, street marches, strikes, sit-ins, and hunger strikes.

One can also express activism through different forms of art (artivism). Daily acts of protest such as not buying clothes from a certain clothing company because they exploit workers is another form of activism. One view holds that acknowledging privileges and oppressions on a daily basis ranks as a form of activism.[1] Research has begun to explore how activist groups use social media to facilitate civic engagement and collective action.[2][3]

The Online Etymology Dictionary records the English words "activism" and "activist" (in the political sense) from 1920[4] and from 1915[5] respectively.

Contents

    1 Types
        1.1 Environmental activists
        1.2 Internet activism
        1.3 Design activism
    2 Activism industry
    3 Methods
    4 See also
    5 References
    6 Further reading

Types

Activists can function in roles as public officials, as in judicial activism. Arthur Schlesinger, Jr., introduced the term "judicial activism" in a January 1946 Fortune magazine article titled "The Supreme Court: 1947".[6] Activists are also public watchdogs and whistle blowers, attempting to understand all the actions of every form of government that acts in the name of the people: all government must be accountable to oversight and transparency. Activism is an engaged citizenry.[7]

Some activists try to persuade people to change their behavior directly, rather than to persuade governments to change or not to change laws. Other activists try to persuade people to remain the same, in an effort to counter change. The cooperative movement seeks to build new institutions which conform to cooperative principles, and generally does not lobby or protest politically.

In his 2008 book, Liberating Voices: A Pattern Language for Communication Revolution,[8] Douglas Schuler suggests something he calls an activist road trip, whereby activism and road trips are merged into an activity that can be pursued on geographical levels that range from neighborhood to international.[9]

As with those who engage in other activities such as singing or running, the term may apply broadly to anyone who engages in it even briefly, or be more narrowly limited to those for whom it is a vocation, habit, or characteristic practice. Activism is not always an action by Activists.[10]
Environmental activists
See also: Environmentalism and List of environmental activists assassinated

Environmental activists, in the narrower sense, align themselves with Earth First or Road Protestors and would commonly be labeled activists, while a local community fighting to stop their park or green being sold off or built on would fit the broader application, due to their using similar means to similarly conservative ends. Every year more than 100 environmental activists are killed: a Global Witness report[11] found that in 2014 at least 116 environmental activists were assassinated,[12] and in 2015 at least 185 activists were killed around this planet.[13]
Internet activism
Further information: Internet activism, Hacktivism, Online social movement, Anonymous (group), and Category:Internet activism

Since the 1990s, the Internet has been a tool used by activists for mobilization and communication of causes. Specific platforms like MoveOn.org, founded in 1998, allow individuals to establish petitions and movements for social change. Protesters in Seattle in 1999 used email to organize protests against the WTO Ministerial Conference. Throughout the 2000s, protesters continued to use social media platforms to generate interest.

The power of Internet Activism came into a global lens with the Arab Spring protests. People living in the Middle East and North African countries that were experiencing revolutions used social networking to communicate protests, which put the issues in front of an international audience.[14]

They use different means to avoid political persecution, such as Tor Browser (a browser that uses Tor network to protect users' identity, IP address, network or location), and encryption data tools and encrypted mails to prevent governments or anyone else intercepting their communications.
Design activism

'Design activism' is a conceptualization that occurs across various communities of practice and can be associated with diverse initiatives such as transition movement, speculative design,[15] design futuring,[16] activist systems,[17] biopolitics[18] and others. One working definition of design activism describes it as "design thinking, imagination and practice applied knowingly or unknowingly to create a counter-narrative aimed at generating and balancing positive social, institutional, environmental and/or economic change."[19]
Activism industry

The activism industry consists of organizations and individuals engaged in activism. Activism is often done full-time, as part of an organization's core business. Many organizations in the activism industry are either non-profit organizations or non-governmental organizations. Most activist organizations do not manufacture goods.[citation needed]

The term activism industry has often been used to refer to outsourced fundraising operations. However, activist organizations engage in other activities as well.[20] Lobbying, or the influencing of decisions made by government, is another activist tactic. Many groups, including law firms, have designated staff assigned specifically for lobbying purposes. In the United States, lobbying is regulated by the federal government.[21]

Many government systems encourage public support of non-profit organizations by granting various forms of tax relief for donations to charitable organizations. Governments may attempt to deny these benefits to activists by restricting the political activity of tax-exempt organizations.
Methods
	
This article is in a list format that may be better presented using prose. You can help by converting this article to prose, if appropriate. Editing help is available. (February 2012)
The longest running peace vigil in U.S. history, started by activist Thomas in 1981.

    Agitation
    Art
    Civil disobedience
    Community building
        Conflict transformation
        Cooperative movement
        Craftivism
        Grassroots
        Guerrilla gardening
        Voluntary simplicity
    Demonstration
    Dissident
    Economic activism
        Boycott
            Freeganism
            Veganism (boycott of animal usage)
            Vegetarianism (boycott of animal meat usage)
        Consumer activism
        Divestment (a.k.a. Disinvestment)
        Simple living
        Tax resistance
    Franchise activism
    Lobbying
    Media activism
        Culture jamming
        Hacktivism
        Internet activism
    Nonviolence
    Peace activist and Peace movement
    Political campaigning
    Propaganda
        Guerrilla communication
    Protest
        Demonstration
        Direct action
        Protest songs
        Theater for Social Change
    Strike action
    Youth activism
        Student activism
        Youth-led media
    Atheist activism

See also

    List of activists
    Advocacy Evaluation
    Advocacy group
    Agitator
    Counterculture of the 1960s
    Community leader
    Human rights activists
    Media manipulation
    Restorationism
    Saul Alinsky
    Slacktivism
    Social engineering (political science)
    Social movement
    Yogyakarta Principles in Action

References

"Inspired Voices: 5 Unconventional forms of Activism.". Elephant Journal. Retrieved 2015-12-09.
Obar, Jonathan; et al. (2012). "Advocacy 2.0: An Analysis of How Advocacy Groups in the United States Perceive and Use Social Media as Tools for Facilitating Civic Engagement and Collective Action". Journal of Information Policy. SSRN 1956352 Freely accessible.
Obar, Jonathan (2014). "Canadian Advocacy 2.0: A Study of Social Media Use by Social Movement Groups and Activists in Canada". Canadian Journal of Communication. SSRN 2254742 Freely accessible.
Harper, Douglas. "activism". Online Etymology Dictionary. Retrieved 17 December 2015.
Harper, Douglas. "activist". Online Etymology Dictionary. Retrieved 17 December 2015.
Keenan Kmiec in a 2004 California Law Review article
"Politically Active? 4 Tips for Incorporating Self-Care, US News". US News. 27 February 2017. Retrieved 5 March 2017.
Schuler, Douglas (2008). Liberating Voices: A Pattern Language for Communication Revolution. Cambridge, Massachusetts: MIT Press. ISBN 9780262693660.
"Activist Road Trip". Public Sphere Project. 2008. Retrieved 1 November 2015.
"Introduction to Activism". Permanent Culture Now. Permanent Culture Now. Retrieved 20 December 2011.
"Report | How Many More?", Global Witness, 20 April 2015.
Cronin, Melissa, "Map: 116 environmental activists were killed in just one year", Grist.org, 4 March 2016.
Holmes, Oliver, "Environmental activist murders set record as 2015 became deadliest year", The Guardian, 20 June 2016.
Sliwinski, Michael (21 January 2016). "The Evolution of Activism: From the Streets to Social Media". Law Street. Retrieved 6 February 2016.
Dunne, Anthony; Raby, Fiona (2013). Speculative Everything: Design, Fiction, and Social Dreaming. Cambridge, MA: MIT Press.
Fry, Tony (2008). Design Futuring: Sustainability, Ethics and New Practice. Oxford: Berg.
Roudavski, Stanislav; Jahn, Gwyllim (2016). "Activist Systems: Futuring with Living Models". International Journal of Architectural Computing. 16 (2): 182–196. doi:10.1177/1478077116638946.
Da Costa, Philip; Kavita, Fiona (2008). Tactical Biopolitics: Art, Activism, and Technoscience. Cambridge, MA: MIT Press.
Fuad-Luke; Alastair (2009). Design Activism: Beautiful Strangeness for a Sustainable World. Sterling: Earthscan. 27.
Dana R. Fisher, "The Activism Industry: The Problem with the Left's Model of Outsourced Grassroots Canvassing Archived 5 December 2010 at the Wayback Machine.", The American Prospect, 14 September 2006

    New Federal Lobbying Law Reporting Periods Begin


	
History of the cooperative movement
From Wikipedia, the free encyclopedia
  (Redirected from Cooperative movement)

The history of the cooperative movement concerns the origins and history of cooperatives. Although cooperative arrangements, such as mutual insurance, and principles of cooperation existed long before, the cooperative movement began with the application of cooperative principles to business organization.

Contents

    1 Beginnings
    2 Robert Owen
    3 William King
    4 The Rochdale Pioneers
    5 The English CWS and Co-operative Group
    6 Co-operative Women's Guild
    7 Other developments
    8 Co-operatives today
    9 See also
    10 References
    11 Further reading
    12 External links

Beginnings

The cooperative movement began in Europe in the 19th century, primarily in Britain and France, although The Shore Porters Society claims to be one of the world's first cooperatives, being established in Aberdeen in 1498 (although it has since demutualized to become a private partnership).[1] The industrial revolution and the increasing mechanism of the economy transformed society and threatened the livelihoods of many workers. The concurrent labour and social movements and the issues they attempted to address describe the climate at the time.

The first documented consumer cooperative was founded in 1769,[2] in a barely furnished cottage in Fenwick, East Ayrshire, when local weavers manhandled a sack of oatmeal into John Walker's whitewashed front room and began selling the contents at a discount, forming the Fenwick Weavers' Society.

In the decades that followed, several cooperatives or cooperative societies formed including Lennoxtown Friendly Victualling Society, founded in 1812.[3]

By 1830, there were several hundred co-operatives.[4] Some were initially successful, but most cooperatives founded in the early 19th century had failed by 1840.[5] However, Lockhurst Lane Industrial Co-operative Society (founded in 1832 and now Heart of England Co-operative Society), and Galashiels and Hawick Co-operative Societies (1839 or earlier, merged with The Co-operative Group) still trade today.[6][7]

It was not until 1844 when the Rochdale Society of Equitable Pioneers established the "Rochdale Principles" on which they ran their cooperative, that the basis for development and growth of the modern cooperative movement was established.[8]

Financially, credit unions were invented in Germany in the mid-19th century, first by Franz Hermann Schulze-Delitzsch (1852, urban), then by Friedrich Wilhelm Raiffeisen (1864, rural). While Schulze-Delitzsch is chronologically earlier, Raiffeisen has proven more influential over time – see history of credit unions. In Britain, the friendly society, building society, and mutual savings bank were earlier forms of similar institutions.
Robert Owen
Main article: Robert Owen

Robert Owen (1771–1858) is considered as the father of the cooperative movement. A Welshman who made his fortune in the cotton trade, Owen believed in putting his workers in a good environment with access to education for themselves and their children. These ideas were put into effect successfully in the cotton mills of New Lanark, Scotland. It was here that the first co-operative store was opened. Spurred on by the success of this, he had the idea of forming "villages of co-operation" where workers would drag themselves out of poverty by growing their own food, making their own clothes and ultimately becoming self-governing. He tried to form such communities in Orbiston in Scotland and in New Harmony, Indiana in the United States of America, but both communities failed.
William King
Main article: William King (physician)

Although Owen inspired the co-operative movement, others – such as Dr. William King (1786–1865) – took his ideas and made them more workable and practical. King believed in starting small, and realized that the working classes would need to set up co-operatives for themselves, so he saw his role as one of instruction. He founded a monthly periodical called The Co-operator,[9] the first edition of which appeared on 1 May 1828. This gave a mixture of co-operative philosophy and practical advice about running a shop using cooperative principles. King advised people not to cut themselves off from society, but rather to form a society within a society, and to start with a shop because, "We must go to a shop every day to buy food and necessaries – why then should we not go to our own shop?" He proposed sensible rules, such as having a weekly account audit, having 3 trustees, and not having meetings in pubs (to avoid the temptation of drinking profits).
The Rochdale Pioneers
Main article: Rochdale Pioneers

The Rochdale Society of Equitable Pioneers was a group of 10 weavers and 20 others in Rochdale, England, that was formed in 1844.[2] As the mechanization of the Industrial Revolution was forcing more and more skilled workers into poverty, these tradesmen decided to band together to open their own store selling food items they could not otherwise afford. With lessons from prior failed attempts at co-operation in mind, they designed the now famous Rochdale Principles, and over a period of four months they struggled to pool one pound sterling per person for a total of 28 pounds of capital. On December 21, 1844, they opened their store with a very meagre selection of butter, sugar, flour, oatmeal and a few candles. Within three months, they expanded their selection to include tea and tobacco, and they were soon known for providing high quality, unadulterated goods.
The English CWS and Co-operative Group
Main article: The Co-operative Group

The Co-operative Group formed gradually over 140 years from the merger of many independent retail societies, and their wholesale societies and federations. In 1863, twenty years after the Rochdale Pioneers opened their co-operative, the North of England Co-operative Society was launched by 300 individual co-ops across Yorkshire and Lancashire. By 1872, it had become known as the Co-operative Wholesale Society (CWS). Through the 20th century, smaller societies merged with CWS, such as the Scottish Co-operative Wholesale Society (1973) and the South Suburban Co-operative Society (1984).
The old Co-operative building behind the Gateshead Millennium Bridge in Newcastle upon Tyne.

By the 1990s, CWS's share of the market had declined considerably and many came to doubt the viability of co-operative model. CWS sold its factories to Andrew Regan in 1994. Regan returned in 1997 with a £1.2 billion bid for CWS. There were allegations of "carpet-bagging" – new members who joined simply to make money from the sale – and more seriously fraud and commercial leaks. After a lengthy battle, Regan's bid was seen off and two senior CWS executives were dismissed and imprisoned for fraud. Regan was cleared of charges. The episode recharged CWS and its membership base. Tony Blair's Co-operative Commission, chaired by John Monks, made major recommendations for the co-operative movement, including the organisation and marketing of the retail societies. It was in this climate that, in 2000, CWS merged with the UK's second largest society, Co-operative Retail Services.

Its headquarters complex is situated on the north side of Manchester city centre adjacent to the Manchester Victoria railway station. The complex is made up of many different buildings with two notable tower blocks of New Century House and the solar panel-clad CIS tower.

Other independent societies are part owners of the Group. Representatives of the societies that part own the Group are elected to the Group's national board. The Group manages The Co-operative brand and the Co-operative Retail Trading Group (CRTG), which sources and promotes goods for food stores.[10] There is a similar purchasing group (CTTG) for co-operative travel agents.
Co-operative Women's Guild
Main article: Co-operative Women's Guild

Alice Acland, the editor of the "Women's Corner" in the Co-operative News publication, and Mary Lawrenson, a teacher, recognized the need for a separate women's organization within the Cooperative Movement and began organizing a "Woman's League for the Spread of Co-operation" in 1883. This League formally met for the first time during the 1883 Co-operative Congress in Edinburgh in a group of 50 women and established Acland as its organizing secretary. By 1884 it had six different branches with 195 members, and the League was renamed the Women's Cooperative Guild.[11]

The Guild organized around working women's issues and expanding the Cooperative Movement. It continued to publish articles advocating for women's involvement in the Cooperative Movement in the "Women's Corner," and later through its own publications such as "The importance of women for the cooperative movement." The Guild also opened the Sunderland cooperative store in 1902, which catered to poor working class women. It engaged in many political campaigns concerning women's health, women's suffrage and pacifism.[12] Until recently the organisation participated in social justice activism, but has now closed.[13]
Other developments

In Russia the village co-operative (obshchina or mir), operated from pre-serfdom times until the 20th century.

Raiffeisen and Schultz-Delitsch developed an independently formulated co-operative model in Germany, the credit union. The model also moved abroad, reaching the United States by the 1880s and the Knights of Labour's projects.[14] Leland Stanford, the railroad magnate and Robber Baron, became a Senator and advocated for co-operatives.[15] By 1920 a national association had formed in the U.S. This organization began to develop international programs, and by the 1970s, a World Council formed.[16]

Co-operatives in the U.S. have a long history, including an early factory in the 1790s. By the 1860s Brigham Young had started applying co-operative ideas in Utah,[17] and by the 1880s, the Knights of Labor and the Grange both promoted member-owned organizations.[18] Energy co-operatives were founded in the U.S. during the Depression and the New Deal.[19] Diverse kinds of co-operatives were founded and have continued to perform successfully in different areas: in agriculture, wholesale purchasing, telephones, and in consumer-food buying.[20][21][22]

James Warbasse, an American doctor, became the first president of the U.S. National Co-operative Business Association. He wrote extensively on co-operative history and philosophy.[23] Benjamin Ward began an important effort in co-operative economic theory in the 1950s, with Jaroslav Vanek developing a general theory.[24] David Ellerman began a line of theoretical thinking beginning with legal principles, developing especially the labor theory of property, and later reaching a treatment which evaluates the role of capital in labor managed firms using the conventional economic production formula Q = f(K, L). At one point in the 1990s he worked at the World Bank with Nobel laureate Joseph Stiglitz.[25]
Co-operatives today

Co-operative enterprises were formed successfully following Rochdale, and an international association was formed in 1895.[26] Co-operative enterprises are now widespread,[27] with one of the largest and most successful examples being the industrial Mondragón Cooperative Corporation in the Basque country of Spain. Mondragon Co-op was founded under the oppressive conditions of Fascist Franco Spain after community-based democracy-building activities of a priest, Jose Maria Arizmendiarrieta. They have become an extremely diverse network of co-operative enterprises, a huge enterprise in Spain, and a multinational concern.[28][29][30][31] Co-operatives were also successful in Yugoslavia under Tito where Workers' Councils gained a significant role in management.[32]

In many European countries, cooperative institutions have a predominant market share in the retail banking[33] and insurance businesses. There are also concrete proposals for the cooperative management of the common goods, such as the one by Initiative 136 in Greece.
An annual general meeting of a retail co-operative in England, 2005.

In the UK, co-operatives formed the Co-operative Party in the early 20th century to represent members of co-ops in Parliament. The Co-operative Party now has a permanent electoral pact with the Labour Party, and some Labour MPs are Co-operative Party members. UK co-operatives retain a significant market share in food retail, insurance, banking, funeral services, and the travel industry in many parts of the country.

Denmark has had a strong cooperative movement.

In Germany, the rebuilding of the country after World War II created a legislative opportunity in which politician Hans Boeckler significantly lobbied for the Co-Determination ("Mitbestimmung") policies which were established, requiring large companies to include a Workers' Council in the Board of Directors.[34] These policies have had some influence on European Union policies.[35][36]

Emilia Romagna, Italy had two separate and strong co-operative traditions that resisted Cold War interference by US agencies and have worked effectively in conjunction with each other.[37]

Co-operative banks have become very successful throughout Europe, and were able to respond more effectively than most corporate banks during the 2008 mortgage-securities crisis.[38][39][40]

Renewable Energy co-operatives in Europe became important in the early development of windpower in Denmark beginning in the 1970s.[41] Germany followed in the early 1990s, first on a larger scale with wind co-ops, then with a citizen's movement which challenged the reliance on nuclear power, organized, challenged the energy monopolists there, and successfully created a successful co-op social enterprise by 1999.[42][43] A citizen's group began operating wind turbines and involving broad community ownership in the U.K. by 1995. Deregulation of the electricity markets allowed energy co-operative social entrepreneurs to begin to create alternatives to the monopolies in various countries. In France, where an enormous percentage of the power is generated by nuclear sources, this occurred after 2000.[44] In Spain, wind power was developed by corporate-led efforts, and it took longer for a renewable energy-focused social enterprise to get established.[45] Similar renewable energy co-ops around Europe have organized in a network.[46]

Asian societies have adapted the co-operative model, including some of the most successful in the world.[47][48] Nevertheless, the crises generated by traditional inequalities and the shareholder model continues to require civil society and entrepreneurial responses, such as the Citizens Coalition for Economic Justice in South Korea, the Seikatsu Club Consumer Co-operative in Japan, and the Self-Employed Women's Association in India.[49][50][51] Other noteworthy efforts include Sophon Suphapong's efforts as governor in Thailand with agricultural co-ops and Antonio Yapsutco Fortich's contributions in the Philippines helping formulate a co-operative strategy with sugar workers.[52][53]

The International Labor Organization, originally established in 1919, has a Co-operative Division.[54]

Co-operatives were brought to Latin America and developed there by 1902.[55] Substantially independent efforts to develop employee-owned enterprises or co-operatives have occurred as responses to crises, such as the systemic IMF-based default in Argentina in 2001 [56] In Brazil, the World Social Forum process lead to the articulation of Solidarity Economics, a modern, activist formulation of co-operativism, with the MST landless worker's movement demonstrating enormous courage and social entrepreneurship.[57][58] In Venezuela, the late Hugo Chávez's administration began to incentivize co-operatives, resulting in their rapid and extensive development there.[59][60]

The co-operative model has a long history in the U.S., including a factory in the 1790s, the Knights of Labor, and the Grange.[61] In Colorado, USA the Meadowlark cooperative administers the only private free land program in the United States, providing many services to its members who buy and sell together. In New York City, several food co-operatives were founded around 2010, adding to others, some existing since the 1970s.[62] The U.S. has some diverse worker co-operatives, such as a home care agency,[63] an organic bread factory co-op and an engineering firm.[64] Some have already incorporated environmental and/or Fair Trade criteria into their products, such as the aforementioned bread-maker, Organic Valley foods, and Equal Exchange.[65]

Credit unions were established in the U.S. by 1908.[66] Their member-owned, co-operative structure created stable governance structure, so that they were only slightly affected by the 2008 mortgage securities crisis.[67]

Electrical co-operatives became an important economic strategy for U.S. rural areas beginning in the 1930s, and continue to operate successfully through events such as Hurricane Sandy in 2012.[68][69] However, the majority in the U.S. demonstrate that co-operative values do not necessarily lead to a progressive social and environmental consciousness, as many remain focuses on fossil fuel and nuclear fuels.[70] Nevertheless, new generation renewable power co-operatives have begun to be organized.[71][72]

Agricultural co-operatives in the U.S. have had some mainstream success, including Welch's, Ocean Spray, and Land O'Lakes.[73][74][75]

In the United States, a co-operative association was founded by 1920. Currently there are over 29,000 co-operatives employing 2 million people with over $652 billion in annual revenue.[76] To address the need for an organization oriented to newer and smaller co-ops, the United States Federation of Worker Cooperatives was founded after 2000.

An alternative method of employee-ownership, the Employee Stock Ownership Plan (ESOP), was developed in the U.S. by Louis Kelso and advocated by Senator Russell Long to be incentivized in the ERISA law of 1974.[77] For example, a large Southeastern US supermarket chain[78] a California manufacturer, and a furniture-maker with earnings of more than $2 billion,[79] are employee-owned. Employee-owned trusts have also been developed more or less independently, for example at an established iron pipe company[80]

The Fair Trade certification movement established first in the Netherlands in 1988 with an international headquarters in Bonn nine years later requires member farmers to have established a co-operative.[81][82][83]

In 2016, UNESCO inscribed "Idea and practice of organizing shared interests in cooperatives" on the Representative List of the Intangible Cultural Heritage of Humanity[84].
See also

    Cooperatives portal 

    British co-operative movement
    Agricultural cooperatives
    Cooperative economics
    Cooperative Stock Market
    Social economy
    Solidarity economy
    Sovereigns of Industry

References

"The Shore Porters' Society: About Us – Our History". 2007. Retrieved 6 May 2008.
Fairbairn, Brett. "The Meaning of Rochdale" (PDF). Usaskstudies.coop. Archived from the original (PDF) on 2012-01-11. Retrieved 2016-12-21.
"Archived copy". Archived from the original on 2007-03-19. Retrieved 2007-05-25.
Doug Peacock. "Social strife: The birth of the co-op". Cotton Times, understanding the industrial revolution. p. 2. Archived from the original on 2008-07-25. Retrieved 2008-06-26.
Doug Peacock. "Social strife: The birth of the co-op". Cotton Times, understanding the industrial revolution. p. 3. Archived from the original on 2008-09-08. Retrieved 2008-06-26.
"Our roots". Heart of England Co-operative Society. Archived from the original on 2006-02-14. Retrieved 2008-06-26.
"History". Lothian Co-op. Archived from the original on 2007-06-08. Retrieved 2008-06-26.
David Thompson. "Cooperative Principles Then and Now". Co-operative Grocer (July–Aug 1994). National Cooperative Grocers Association, Minneapolis. Archived from the original on 2007-10-10. Retrieved 2008-06-26.
"The Co-operator". 1 January 1828 – via Google Books.
"About Us". Co-operative Retail Trading Group. 2007. Archived from the original on 2008-10-23. Retrieved 2008-05-13.
Gaffin, Jean; David Thoms (1983). Caring & Sharing: The Centenary History of the Co-operative Women's Guild. Manchester: Co-operative Union Ltd. ISBN 0851951333.
"Records of the Women's Cooperative Guild (Cooperative Women's Guild)". Hull University. Archived from the original on 21 May 2013. Retrieved 7 April 2013.
Cowan, Joss. "Co-operative Women's Guild". Archived from the original on 21 March 2012. Retrieved 7 April 2013.
"Archived copy". Archived from the original on 2014-12-17. Retrieved 2014-09-15.
"Beyond Capitalism: Leland Stanford's Forgotten Vision". Dynamics.org. Retrieved 2016-12-21.
"Heritage". Woccu.org. Retrieved 2016-12-21.
"Brigham Young, Leader of Cooperative Economics - The Boy Scout | Utah National Parks Council Official Blog". Blog.utahscouts.org. Retrieved 2016-12-21.
[1][dead link]
"History of Cooperatives". REA Energy. Retrieved 2016-12-21.
"Cooperative History | lcecnet.com". Lcecnet.coopwebbuilder.com. Retrieved 2016-12-21.
"The Food Co-op Handbook". Houghton Mifflin. 1 January 1975 – via Amazon.
Knupfer, Anne Meis (21 May 2013). "Food Co-ops in America: Communities, Consumption, and Economic Democracy". Cornell University Press – via Amazon.
"Cooperative Peace". Abob.libs.uga.edu. 1950-06-15. Archived from the original on 2014-10-20. Retrieved 2016-12-21.
"From Illyria towards capitalism: did labour-management theory teach us anything about Yugoslavia and transition in its successor states? - Free Online Library". Thefreelibrary.com. Retrieved 2016-12-21.
Ellerman, David (1 December 2004). "The Role of Capital in 'Capitalist' and in Labor-Managed Firms". SSRN 633722 Freely accessible.
Birchall, Johnston 1997 The International Co-operative Movement
"Archived copy". Archived from the original on 2011-04-10. Retrieved 2011-06-21.
"The Mondragon Experiment - Societies on DocuWatch - free streaming documentaries". Societies.docuwat.ch. 1980-11-17. Retrieved 2016-12-21.
Whyte, W.F. and K.K. Whyte, 1988, Making Mondragon.
Guadano, J.F., "Analysis of the Economic Differences Between Capitalist and Labour-Owned Enterprises", International Journal of Social Economics, 36:6, 2009, 679–691.
Clamp, C.A., "The Internalization of Mondragon", Annals of Public and Cooperative Economics, 71:4, 2000, 557–577.
J. R. Lampe, Yugoslavia as History, Cambridge University Press, 2000, pp. 255–264
Bevilacqua, E. (2012) "Co-operative Banks as Key Players." European Association of Co-op Banks
"Co-determination in Germany - Hans-Böckler-Stiftung". Boeckler.de. Retrieved 2016-12-21.
"Employee representation in Europe - Hans-Böckler-Stiftung". Boeckler.de. Retrieved 2016-12-21.
"EUR-Lex - c10805 - EN - EUR-Lex". Europa.eu. Retrieved 2016-12-21.
"University of Wisconsin Center for Cooperatives – The cooperative economics of Italy's Emilia-Romagna holds a lesson for the U.S.". uwcc.wisc.edu.
Fonteyne, W. Cooperative Banks in Europe- Policy Issues. IMF Working Paper WP/07/159, July 2007.
Capella, P. "Cooperative Banks Club Together and Thrive in Crisis", Swisster.ch, 17 October 2008
McCrae, Fran (2015-01-16). "COPAC". Copac.coop. Retrieved 2016-12-21.
Larsen, J.H.M., et al., "Experiences from Middelgrunden 40 MW Offshore Wind Farm", Copenhagen Offshore Wind Conference, DK, 26–28 October 2005.
"Ökokraftwerke: Bürger nehmen Stromversorgung selbst in die Hand - SPIEGEL ONLINE". Spiegel.de. 2007-02-21. Retrieved 2016-12-21.
"Ursula Sladek - Goldman Environmental Foundation : Goldman Environmental Foundation". Goldmanprize.org. Retrieved 2016-12-21.
"Archived copy". Archived from the original on 2014-05-29. Retrieved 2014-09-02.
"Archived copy". Archived from the original on 2014-05-29. Retrieved 2014-09-02.
"Archived copy". Archived from the original on 2014-07-04. Retrieved 2014-09-02.
"Search ica.coop | ICA: International Co-operative Alliance". Ica.coop. Retrieved 2016-12-21.
"Member Organisations | International Cooperative Alliance - Asia and Pacific". Ica-ap.coop. Retrieved 2016-12-21.
"Archived copy". Archived from the original on 2014-09-03. Retrieved 2014-09-02.
"Archived copy". Archived from the original on 2014-09-03. Retrieved 2014-09-02.
"Archived copy". Archived from the original on 2014-09-03. Retrieved 2014-09-02.
"Ramon Magsaysay Award Foundation - Awardees". Rmaf.org.ph. Retrieved 2016-12-21.
"Ramon Magsaysay Award Foundation - Awardees". Rmaf.org.ph. Retrieved 2016-12-21.
"About the department (ENTERPRISES)". Ilo.org. Retrieved 2016-12-21.
Birchall, J. (1997) The International Co-operative Movement.; ICA – Americas Region
LaVaca Collective, (2007), Sin Patron.; Klein, N and A Lewis (2006) film The Take.; Howarth, M., (2007) "...empresas recuperadas in Argentina," The UK Co-operative College/ILO Archived 2013-11-12 at the Wayback Machine.
Neiva, AC et al (2013), draft "Solidarity Finance and Public Policy."; Singer, P (2002), "The Recent Rebirth of the Solidarity Economy in Brazil" in B de SS Santos (2002), Produzir Para Viver.; "Historico" at Forum Brasileiro da Economia Solidaria (in Portuguese)
"About | Friends of the MST". Mstbrazil.org. Retrieved 2016-12-21.
Fox, M (May 26, 2006), "Venezuela's Cooperatives Take First Steps Towards a National Cooperative Movement," Venezuelanalysis.com
"Hugo by Bart Jones". AbeBooks. 2007-09-04. Retrieved 2016-12-21.
Curl, John History of Work Cooperation in America CA: Homeward Press, 1980.
Ronco, WC 1974 Food Co-ops; Knupfer, AM 2013 Food Coops in America; Greene Hill Food Co-op, Brooklyn, NY; Food Co-op Initiative
"Cooperative Home Care Associates: Participation with 1600 Employees | Grassroots Economic Organizing". Geo.coop. Retrieved 2016-12-21.
Moore, M. (2009) film Capitalism: A Love Story.
Kelly, M. (2012) Owning Our Future.
"Archived copy". Archived from the original on 2014-12-17. Retrieved 2014-09-02.
"How Credit Unions Survived The Crash By Ralph Nader". Countercurrents.org. 2009-02-27. Retrieved 2016-12-21.
Righter, R.W. “The Jacobs Brothers, Montana’s Pioneer ‘Windsmiths.’” Montana: The Magazine of Western History, 46:4, Winter, 1996.
"President Obama Hosts Meeting of Electrical Co-op Leaders". Thenews.coop. Retrieved 2016-12-21.
"Electric co-operatives switch to the future at major conference". Thenews.coop. Retrieved 2016-12-21.
"Co-op Power". Cooppower.coop. 2016-09-19. Retrieved 2016-12-21.
"Coalition brings solar energy co-ops to Baltimore area.". Baltimore Sun. 2014-07-25. Retrieved 2016-12-21.
"National Grape Cooperative Association" (PDF). Retrieved 2016-12-21.
"Tastes Good, Good For You® Cranberry Juices & Snacks". Ocean Spray. Retrieved 2016-12-21.
"Land O'Lakes Inc. - Home". Landolakesinc.com. Retrieved 2016-12-21.
"Co-op Research / Economic Impact". National Cooperative Business Association. Archived from the original on 2011-11-25.
Rosen, C et al (2005), Equity.; Greider, W. (2003), The Soul of Capitalism.
Rosen, C et al (2005), Equity.
Greider, W. (2003), The Soul of Capitalism.
Barstow, D et al (9 January 2003), "A Family's Fortune: A Legacy of Blood and Tears," The New York Times; ACIPCO Archived 2013-11-12 at the Wayback Machine.
"Archived copy". Archived from the original on 2014-09-03. Retrieved 2014-09-02.
Milford, Anna. "Coffee, Co-operatives and Competition: The Impact of Fair Trade" Archived 2015-09-22 at the Wayback Machine., Michelsen Institute of Development Studies and Human Rights.
Muriel Calo and Timothy A. Wise, "Revaluing Peasant Coffee Production: Organic and Fair Trade Markets in Mexico", Global and Environment Institute, October 2005.

    "Idea and practice of organizing shared interests in cooperatives". UNESCO. Retrieved July 11, 2017.

Further reading

    Birchall, Johnston (1997), The International Co-operative Movement.
    Curl, John (2009), For All The People: Uncovering the Hidden History of Cooperation, Cooperative Movements, and Communalism in America, PM Press.
    Derr, Jascha (2013), The cooperative movement of Brazil and South Africa
    Greider, William (2003), The Soul of Capitalism.
    Kelly, Marjorie (2012), Owning Our Future: The Emerging Ownership Revolution.
    Nadeau, E.G. & D.J. Thompson (1996), Cooperation Works!
    Thompson, D.J. (1994), Weavers of Dreams: Founders of the Modern Cooperative Movement.
    Whyte, W.F. & K.K. Whyte (1988), Making Mondragon.
    Wolff, Richard (2012), Democracy at Work: A Cure for Capitalism.

External links

    History of RECs
    Over 160 rulebooks of co-operative societies from Great Britain and Ireland, 1877–1921, are available online
    Digital Collection on the History of Cooperatives in Utah: "Extension, Enterprise, and Education: the Legacy of Co-operatives and Cooperation in Utah": Utah State University

	
This is a good article. Click here for more information.
Page semi-protected
Scotland
From Wikipedia, the free encyclopedia
This article is about the country. For other uses, see Scotland (disambiguation).
Scotland
Alba (Scottish Gaelic)
Flag of Scotland
Flag
Royal Banner of Scotland
Royal Banner
Motto: 
"In My Defens God Me Defend" (Scots)[a]
"In my defence God me defend"
Anthem: Various[b]
Predominantly "Flower of Scotland"
Location of  Scotland  (dark green)– in Europe  (green & dark grey)– in the United Kingdom  (green)
Location of  Scotland  (dark green)

– in Europe  (green & dark grey)
– in the United Kingdom  (green)
Status 	Country
Capital 	Edinburgh
Largest city 	Glasgow
55°51′N 4°16′W
Languages 	English
Recognised languages[c] 	

    British Sign Language
    Scots
    Scottish Gaelic

Ethnic groups (2011) 	

    96.0% White
    2.7% Asian
    0.7% Black
    0.4% Mixed
    0.2% Arab
    0.1% other[6]

Religion (2011) 	

    53.8% Christian
    36.7% no religion
    2.6% other
    7.0% unknown[7]

Demonym 	

    Scottish Scots[d] 

    Sovereign state
    Legal jurisdiction

	

    United Kingdom
    Scotland

Government 	Devolved parliamentary legislature within constitutional monarchy[e]
• Monarch
	Elizabeth II
• First Minister
	Nicola Sturgeon
Parliament of the United Kingdom
• Secretary of State 	David Mundell
• House of Commons 	59 MPs (of 650)
Legislature 	Scottish Parliament
Formation
• Established
	9th century (traditionally 843)
• Union with England
	1 May 1707
• Devolution
	19 November 1998
Area
• Land
	77,933 km2 (30,090 sq mi)[8]
Population
• 2016 estimate
	5,404,700[9]
• 2011 census
	5,313,600[10]
• Density
	67.5/km2 (174.8/sq mi)
GVA 	2015 estimate
 • Total 	£127 billion[11]
 • Per capita 	£23,685[11]
Note: Figures do not include revenues from adjacent North Sea oil and gas.
GDP (nominal) 	2013 estimate
• Total
	$245.267 billion[12]
• Per capita
	$45,904[12]
Note: Figures include revenues from adjacent North Sea oil and gas.
Currency 	Pound sterling (GBP; £)
Time zone 	Greenwich Mean Time (UTC⁠)
• Summer (DST)
	British Summer Time (UTC+1)
Date format 	dd/mm/yyyy (AD)
Drives on the 	left
Calling code 	+44
Patron saints 	

    Saint Andrew[1][2]
    Saint Margaret[3][4]
    Saint Columba[5]

ISO 3166 code 	GB-SCT
Website
www.scotland.org

    ^ Often shown abbreviated as "In Defens".
    ^ See National anthem of Scotland.
    ^ Both Scots and Scottish Gaelic are officially recognised as regional languages under the European Charter for Regional or Minority Languages.[13] Under the Gaelic Language (Scotland) Act 2005, Bòrd na Gàidhlig is tasked with securing Gaelic as an official language of Scotland.[14] British Sign Language is officially recognised under the British Sign Language (Scotland) Act 2015.[15]
    ^ Historically, the use of "Scotch" as an adjective comparable to "Scottish" or "Scots" was commonplace. Modern use of the term describes products of Scotland (usually food or drink-related).
    ^ The head of state of the United Kingdom is the monarch (currently Queen Elizabeth II, since 1952). Scotland has limited self-government within the UK as well as representation in the UK Parliament. It is also a UK electoral region for the European Parliament. Certain executive and legislative powers have been devolved to, respectively, the Scottish Government and the Scottish Parliament.

Scotland (/ˈskɒt.lənd/; Scots: [ˈskɔt.lənd]; Scottish Gaelic: Alba [ˈal̪ˠapə] (About this sound listen)) is a country that is part of the United Kingdom and covers the northern third of the island of Great Britain.[16][17][18] It shares a border with England to the south, and is otherwise surrounded by the Atlantic Ocean, with the North Sea to the east and the North Channel and Irish Sea to the south-west. In addition to the mainland, the country is made up of more than 790 islands,[19] including the Northern Isles and the Hebrides.

The Kingdom of Scotland emerged as an independent sovereign state in the Early Middle Ages and continued to exist until 1707. By inheritance in 1603, James VI, King of Scots, became King of England and King of Ireland, thus forming a personal union of the three kingdoms. Scotland subsequently entered into a political union with the Kingdom of England on 1 May 1707 to create the new Kingdom of Great Britain.[20][21] The union also created a new Parliament of Great Britain, which succeeded both the Parliament of Scotland and the Parliament of England. In 1801, Great Britain itself entered into a political union with the Kingdom of Ireland to create the United Kingdom of Great Britain and Ireland.[22]

Within Scotland, the monarchy of the United Kingdom has continued to use a variety of styles, titles and other royal symbols of statehood specific to the pre-union Kingdom of Scotland. The legal system within Scotland has also remained separate from those of England and Wales and Northern Ireland; Scotland constitutes a distinct jurisdiction in both public and private law.[23] The continued existence of legal, educational, religious and other institutions distinct from those in the remainder of the UK have all contributed to the continuation of Scottish culture and national identity since the 1707 union with England.[24]

In 1997, a Scottish Parliament was re-established, in the form of a devolved unicameral legislature comprising 129 members, having authority over many areas of domestic policy.[25] Scotland is represented in the United Kingdom Parliament by 59 MPs and in the European Parliament by 6 MEPs.[26] Scotland is also a member of the British–Irish Council,[27] and sends five members of the Scottish Parliament to the British–Irish Parliamentary Assembly.[28]

Contents

    1 History
        1.1 Etymology
        1.2 Early history
        1.3 Roman influence
        1.4 Middle Ages
        1.5 Early modern era
        1.6 18th century
        1.7 19th century
        1.8 Early 20th century
        1.9 Modern day
    2 Geography and natural history
        2.1 Geology and geomorphology
        2.2 Climate
        2.3 Flora and fauna
    3 Demographics
    4 Religion
    5 Politics and government
        5.1 Devolved government relations
        5.2 International diplomacy
        5.3 Constitutional changes
        5.4 Administrative subdivisions
    6 Law and criminal justice
    7 Health care
    8 Economy
        8.1 Currency
    9 Military
    10 Education
    11 Culture
        11.1 Scottish music
        11.2 Literature
        11.3 Celtic connections
        11.4 National identity
        11.5 Cuisine
    12 Media
    13 Sport
    14 Infrastructure
        14.1 Transport
        14.2 Road
        14.3 Air
        14.4 Rail
        14.5 Water
        14.6 Renewable energy
    15 See also
    16 Notes
    17 References
    18 Further reading
        18.1 Specialized monographs
    19 External links

History
Main article: History of Scotland
Etymology
Main article: Etymology of Scotland

"Scotland" comes from Scoti, the Latin name for the Gaels. The Late Latin word Scotia ("land of the Gaels") was initially used to refer to Ireland.[29] By the 11th century at the latest, Scotia was being used to refer to (Gaelic-speaking) Scotland north of the River Forth, alongside Albania or Albany, both derived from the Gaelic Alba.[30] The use of the words Scots and Scotland to encompass all of what is now Scotland became common in the Late Middle Ages.[20]
Early history
Main article: Prehistoric Scotland
See also: Timeline of prehistoric Scotland

Repeated glaciations, which covered the entire land mass of modern Scotland, destroyed any traces of human habitation that may have existed before the Mesolithic period. It is believed the first post-glacial groups of hunter-gatherers arrived in Scotland around 12,800 years ago, as the ice sheet retreated after the last glaciation.[31][32]
Scara Brae. A Neolithic settlement, located on the west coast of Mainland, Orkney.

The groups of settlers began building the first known permanent houses on Scottish soil around 9,500 years ago, and the first villages around 6,000 years ago. The well-preserved village of Skara Brae on the mainland of Orkney dates from this period. Neolithic habitation, burial and ritual sites are particularly common and well preserved in the Northern Isles and Western Isles, where a lack of trees led to most structures being built of local stone.[33]

The 2009 discovery in Scotland of a 4000-year-old tomb with burial treasures at Forteviot, near Perth, the capital of a Pictish Kingdom in the 8th and 9th centuries AD, is unrivalled anywhere in Britain. It contains the remains of an early Bronze Age ruler laid out on white quartz pebbles and birch bark. It was also discovered for the first time that early Bronze Age people placed flowers in their graves.[34][35]

Scotland may have been part of a Late Bronze Age maritime trading culture called the Atlantic Bronze Age, which included other Celtic nations, and the areas that became England, France, Spain, and Portugal.[36][37][38][39]

In the winter of 1850, a severe storm hit Scotland, causing widespread damage and over 200 deaths.[40] In the Bay of Skaill, the storm stripped the earth from a large irregular knoll, known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs.[40][41] William Watt of Skaill, the local laird, began an amateur excavation of the site, but after uncovering four houses, the work was abandoned in 1868.[41] The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts.[40] In 1924, another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated.[40] The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.[40]
Roman influence
Main article: Scotland during the Roman Empire
Tablet found at Bo'ness dated ca. AD 142 depicting Roman cavalryman trampling Caledonians. Now at the NMS

The written protohistory of Scotland began with the arrival of the Roman Empire in southern and central Great Britain, when the Romans occupied what is now England and Wales, administering it as a province called Britannia. Roman invasions and occupations of southern Scotland were a series of brief interludes.

According to the Roman historian Tacitus, the Caledonians "turned to armed resistance on a large scale", attacking Roman forts and skirmishing with their legions. In a surprise night-attack, the Caledonians very nearly wiped out the whole 9th Legion until it was saved by Agricola's cavalry.[42]

In AD 83–84, the General Gnaeus Julius Agricola defeated the Caledonians at the Battle of Mons Graupius. Tacitus wrote that, before the battle, the Caledonian leader, Calgacus, gave a rousing speech in which he called his people the "last of the free" and accused the Romans of "making the world a desert and calling it peace" (freely translated).[42] After the Roman victory, Roman forts were briefly set along the Gask Ridge close to the Highland line (only Cawdor near Inverness is known to have been constructed beyond that line). Three years after the battle, the Roman armies had withdrawn to the Southern Uplands.[43]

The Romans erected Hadrian's Wall to control tribes on both sides of the wall[44] so the Limes Britannicus became the northern border of the Roman Empire; although the army held the Antonine Wall in the Central Lowlands for two short periods – the last during the reign of Emperor Septimius Severus from 208 until 210.[45]

The Roman military occupation of a significant part of what is now northern Scotland lasted only about 40 years; although their influence on the southern section of the country, occupied by Brythonic tribes such as the Votadini and Damnonii, would still have been considerable between the first and fifth centuries. The Welsh term Hen Ogledd ("Old North") is used by scholars to describe what is now the North of England and the South of Scotland during its habitation by Brittonic-speaking people around AD 500 to 800.[44] According to writings from the 9th and 10th centuries, the Gaelic kingdom of Dál Riata was founded in the 6th century in western Scotland.[46][47] The 'traditional' view is that settlers from Ireland founded the kingdom, bringing Gaelic language and culture with them. However, some archaeologists have argued against this view, saying there is no archaeological or placename evidence for a migration or a takeover by a small group of elites.[48]
Middle Ages
Main articles: Scotland in the Early Middle Ages, Scotland in the High Middle Ages, and Scotland in the Late Middle Ages
The class I Pictish stone at Aberlemno known as Aberlemno 1 or the Serpent Stone

The Kingdom of the Picts (based in Fortriu by the 6th century) was the state that eventually became known as "Alba" or "Scotland". The development of "Pictland", according to the historical model developed by Peter Heather, was a natural response to Roman imperialism.[49] Another view places emphasis on the Battle of Dun Nechtain, and the reign of Bridei m. Beli (671–693), with another period of consolidation in the reign of Óengus mac Fergusa (732–761).[50]

The Kingdom of the Picts as it was in the early 8th century, when Bede was writing, was largely the same as the kingdom of the Scots in the reign of Alexander I (1107–1124). However, by the tenth century, the Pictish kingdom was dominated by what we can recognise as Gaelic culture, and had developed a traditional story of an Irish conquest around the ancestor of the contemporary royal dynasty, Cináed mac Ailpín (Kenneth MacAlpin).[51][52][53]

From a base of territory in eastern Scotland north of the River Forth and south of the River Oykel, the kingdom acquired control of the lands lying to the north and south. By the 12th century, the kings of Alba had added to their territories the English-speaking land in the south-east and attained overlordship of Gaelic-speaking Galloway and Norse-speaking Caithness; by the end of the 13th century, the kingdom had assumed approximately its modern borders. However, processes of cultural and economic change beginning in the 12th century ensured Scotland looked very different in the later Middle Ages.

The push for this change was the reign of David I and the Davidian Revolution. Feudalism, government reorganisation and the first legally recognised towns (called burghs) began in this period. These institutions and the immigration of French and Anglo-French knights and churchmen facilitated cultural osmosis, whereby the culture and language of the low-lying and coastal parts of the kingdom's original territory in the east became, like the newly acquired south-east, English-speaking, while the rest of the country retained the Gaelic language, apart from the Northern Isles of Orkney and Shetland, which remained under Norse rule until 1468.[54][55][56] The Scottish state entered a largely successful and stable period between the 12th and 14th centuries, there was relative peace with England, trade and educational links were well developed with the Continent and at the height of this cultural flowering John Duns Scotus was one of Europe's most important and influential philosophers.
The Wallace Monument commemorates William Wallace, the 13th-century Scottish hero.

The death of Alexander III in March 1286, followed by that of his granddaughter Margaret, Maid of Norway, broke the centuries-old succession line of Scotland's kings and shattered the 200-year golden age that began with David I. Edward I of England was asked to arbitrate between claimants for the Scottish crown, and he organised a process known as the Great Cause to identify the most legitimate claimant. John Balliol was pronounced king in the Great Hall of Berwick Castle on 17 November 1292 and inaugurated at Scone on 30 November, St. Andrew's Day. Edward I, who had coerced recognition as Lord Paramount of Scotland, the feudal superior of the realm, steadily undermined John's authority.[57] In 1294, Balliol and other Scottish lords refused Edward's demands to serve in his army against the French. Instead the Scottish parliament sent envoys to France to negotiate an alliance. Scotland and France sealed a treaty on 23 October 1295, known as the Auld Alliance (1295–1560). War ensued and King John was deposed by Edward who took personal control of Scotland. Andrew Moray and William Wallace initially emerged as the principal leaders of the resistance to English rule in what became known as the Wars of Scottish Independence (1296–1328).[58]

The nature of the struggle changed significantly when Robert the Bruce, Earl of Carrick, killed his rival John Comyn on 10 February 1306 at Greyfriars Kirk in Dumfries.[59] He was crowned king (as Robert I) less than seven weeks later. Robert I battled to restore Scottish Independence as King for over 20 years, beginning by winning Scotland back from the Norman English invaders piece by piece. Victory at the Battle of Bannockburn in 1314 proved the Scots had regained control of their kingdom. In 1315, Edward Bruce, brother of the King, was briefly appointed High King of Ireland during an ultimately unsuccessful Scottish invasion of Ireland aimed at strengthening Scotland's position in its wars against England. In 1320 the world's first documented declaration of independence, the Declaration of Arbroath, won the support of Pope John XXII, leading to the legal recognition of Scottish sovereignty by the English Crown.

However, war with England continued for several decades after the death of Bruce. A civil war between the Bruce dynasty and their long-term Comyn-Balliol rivals lasted until the middle of the 14th century. Although the Bruce dynasty was successful, David II's lack of an heir allowed his half-nephew Robert II to come to the throne and establish the Stewart Dynasty.[55][60] The Stewarts ruled Scotland for the remainder of the Middle Ages. The country they ruled experienced greater prosperity from the end of the 14th century through the Scottish Renaissance to the Reformation. This was despite continual warfare with England, the increasing division between Highlands and Lowlands, and a large number of royal minorities.[60][61]

This period was the height of the Franco-Scottish alliance. The Scots Guard – la Garde Écossaise – was founded in 1418 by Charles VII of France. The Scots soldiers of the Garde Écossaise fought alongside Joan of Arc against England during the Hundred Years War.[62] In March 1421, a Franco-Scots force under John Stewart, 2nd Earl of Buchan, and Gilbert de Lafayette, defeated a larger English army at the Battle of Baugé. Three years later, at the Battle of Verneuil, the French and Scots lost around 7000 men.[63] The Scottish intervention contributed to France's victory in the war.
Early modern era
Main article: Scotland in the Early Modern Era
James VI succeeded to the English and Irish thrones in 1603.

In 1502, James IV of Scotland signed the Treaty of Perpetual Peace with Henry VII of England. He also married Henry's daughter, Margaret Tudor, setting the stage for the Union of the Crowns. For Henry, the marriage into one of Europe's most established monarchies gave legitimacy to the new Tudor royal line.[64] A decade later, James made the fateful decision to invade England in support of France under the terms of the Auld Alliance. He was the last British monarch to die in battle, at the Battle of Flodden.[65] Within a generation the Auld Alliance was ended by the Treaty of Edinburgh. France agreed to withdraw all land and naval forces. In the same year, 1560, John Knox realised his goal of seeing Scotland become a Protestant nation and the Scottish parliament revoke papal authority in Scotland.[66] Mary, Queen of Scots, a Catholic and former queen of France, was forced to abdicate in 1567.[67]

In 1603, James VI, King of Scots inherited the thrones of the Kingdom of England and the Kingdom of Ireland, and became King James I of England and Ireland, and left Edinburgh for London.[68] With the exception of a short period under the Protectorate, Scotland remained a separate state, but there was considerable conflict between the crown and the Covenanters over the form of church government. The Glorious Revolution of 1688–89 saw the overthrow of King James VII of Scotland and II of England by the English Parliament in favour of William III and Mary II.

In common with countries such as France, Norway, Sweden and Finland, Scotland experienced famines during the 1690s. Mortality, reduced childbirths and increased emigration reduced the population of parts of the country by between 10 and 15 per cent.[69]

In 1698, the Company of Scotland attempted a project to secure a trading colony on the Isthmus of Panama. Almost every Scottish landowner who had money to spare is said to have invested in the Darien scheme. Its failure bankrupted these landowners, but not the burghs. Nevertheless, the nobles' bankruptcy, along with the threat of an English invasion, played a leading role in convincing the Scots elite to back a union with England.[70][71]

On 22 July 1706, the Treaty of Union was agreed between representatives of the Scots Parliament and the Parliament of England and the following year twin Acts of Union were passed by both parliaments to create the united Kingdom of Great Britain with effect from 1 May 1707;[21] there was popular opposition and anti-union riots in Edinburgh, Glasgow, and elsewhere.[72][73]
18th century
David Morier's depiction of the Battle of Culloden

With trade tariffs with England now abolished, trade blossomed, especially with Colonial America. The clippers belonging to the Glasgow Tobacco Lords were the fastest ships on the route to Virginia. Until the American War of Independence in 1776, Glasgow was the world's premier tobacco port, dominating world trade.[74] The disparity between the wealth of the merchant classes of the Scottish Lowlands and the ancient clans of the Scottish Highlands grew, amplifying centuries of division.

The deposed Jacobite Stuart claimants had remained popular in the Highlands and north-east, particularly amongst non-Presbyterians, including Roman Catholics and Episcopalian Protestants. However, two major Jacobite Risings launched in 1715 and 1745 failed to remove the House of Hanover from the British throne. The threat of the Jacobite movement to the United Kingdom and its monarchs effectively ended at the Battle of Culloden, Great Britain's last pitched battle. This defeat paved the way for large-scale removals of the indigenous populations of the Highlands and Islands, known as the Highland Clearances.

The Scottish Enlightenment and the Industrial Revolution made Scotland into an intellectual, commercial and industrial powerhouse[75]–so much so Voltaire said "We look to Scotland for all our ideas of civilisation."[76] With the demise of Jacobitism and the advent of the Union, thousands of Scots, mainly Lowlanders, took up numerous positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes "after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland." Davidson also states "far from being 'peripheral' to the British economy, Scotland – or more precisely, the Lowlands – lay at its core."[77]
19th century
Main article: Scotland in the modern era
Shipping on the Clyde, by John Atkinson Grimshaw, 1881

The Scottish Reform Act 1832 increased the number of Scottish MPs and widened the franchise to include more of the middle classes.[78] From the mid-century there were increasing calls for Home Rule for Scotland and the post of Secretary of State for Scotland was revived.[79] Towards the end of the century Prime Ministers of Scottish descent included William Gladstone,[80] and the Earl of Rosebery.[81] In the later 19th century the growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.[82]

Glasgow became one of the largest cities in the world, and known as "the Second City of the Empire" after London.[83] After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre.[84] The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis.[85]
Walter Scott, whose Waverley Novels helped define Scottish identity in the 19th century.

While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century,[86] disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the physicists James Clerk Maxwell and Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain.[87] In literature the most successful figure of the mid-19th century was Walter Scott. His first prose work, Waverley in 1814, is often called the first historical novel.[88] It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity.[89] In the late 19th century, a number of Scottish-born authors achieved international reputations, such as Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald.[90] Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts Movement, and Japonisme, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Proponents included architect and artist Charles Rennie Mackintosh.[91]

This period saw a process of rehabilitation for Highland culture. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe,[92][93] prompted by the popularity of Macpherson's Ossian cycle[94][95] and then Walter Scott's Waverley novels.[96] However, the Highlands remained very poor and traditional.[97] The desire to improve agriculture and profits led to the Highland Clearances, in which much of the population of the Highlands suffered forced displacement as lands were enclosed, principally so that they could be used for sheep farming. The clearances followed patterns of agricultural change throughout Britain, but were particularly notorious as a result of the late timing, the lack of legal protection for year-by-year tenants under Scots law, the abruptness of the change from the traditional clan system, and the brutality of many evictions.[98] One result was a continuous exodus from the land—to the cities, or further afield to England, Canada, America or Australia.[99] The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901.[100] Even with the development of industry there were not enough good jobs. As a result, during the period 1841–1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England.[101]
The Disruption Assembly; painted by David Octavius Hill.

After prolonged years of struggle in the Kirk, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers, known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland.[102] In the late 19th century growing divisions between fundamentalist Calvinists and theological liberals resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893.[103] Catholic Emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, mainly to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland.[103]

Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants; then from 1846 it was funding schools by direct sponsorship; and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards.[104] The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as opposed to the upper class.[105] The University of St Andrews pioneered the admission of women to Scottish universities. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.[106]
Early 20th century
Royal Scots with a captured Japanese Hinomaru Yosegaki flag, Burma, 1945.

Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, fish and money.[107] With a population of 4.8 million in 1911, Scotland sent over half a million men to the war, of whom over a quarter died in combat or from disease, and 150,000 were seriously wounded.[108] Field Marshal Sir Douglas Haig was Britain's commander on the Western Front.

The war saw the emergence of a radical movement called "Red Clydeside" led by militant trades unionists. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base among the Irish Catholic working class districts. Women were especially active in building neighbourhood solidarity on housing issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament and the mood changed to passive despair by the late 1920s.[109]

The shipbuilding industry expanded by a third and expected renewed prosperity, but instead a serious depression hit the economy by 1922 and it did not fully recover until 1939. The interwar years were marked by economic stagnation in rural and urban areas, and high unemployment.[110] Indeed, the war brought with it deep social, cultural, economic, and political dislocations. Thoughtful Scots pondered their declension, as the main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. Service abroad on behalf of the Empire lost its allure to ambitious young people, who left Scotland permanently. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.[111]

During the Second World War, Scotland was targeted by Nazi Germany largely due to its factories, ship yards and coal mines.[112] Cities such as Glasgow and Edinburgh were targeted by German bombers, as were smaller towns mostly located in the central belt of the country.[112] Perhaps the most significant air-raid in Scotland was the Clydebank Blitz of March 1941, which intended to destroy naval shipbuilding in the area.[113] 528 people were killed and 4,000 homes totally destroyed.[113]
Rudolf Hess, Deputy Führer of Nazi Germany, crashed his plane at Bonnyton Moor in the Scottish central belt in an attempt to make peace

Perhaps Scotland's most unusual wartime episode occurred in 1941 when Rudolf Hess flew to Renfrewshire, possibly intending to broker a peace deal through the Duke of Hamilton.[114] Before his departure from Germany, Hess had given his adjutant, Karlheinz Pintsch, a letter addressed to Hitler that detailed his intentions to open peace negotiations with the British. Pintsch delivered the letter to Hitler at the Berghof around noon on 11 May.[115] Albert Speer later said Hitler described Hess's departure as one of the worst personal blows of his life, as he considered it a personal betrayal.[116] Hitler worried that his allies, Italy and Japan, would perceive Hess's act as an attempt by Hitler to secretly open peace negotiations with the British.

As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian.[117] The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life.[118] As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic.[119] Shetland's relative proximity to occupied Norway resulted in the Shetland Bus by which fishing boats helped Norwegians fled the Nazis, and expeditions across the North Sea to assist resistance.[120]

Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships.[118] Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25 per cent, and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition; the average height of 13-year-olds in Glasgow increased by 2 inches.[121]
Modern day
The official reconvening of the Scottish Parliament in July 1999 with Donald Dewar, then First Minister of Scotland (left) with Queen Elizabeth II (centre) and Presiding Officer Sir David Steel (right)

After 1945, Scotland's economic situation worsened due to overseas competition, inefficient industry, and industrial disputes.[122] Only in recent decades has the country enjoyed something of a cultural and economic renaissance. Economic factors contributing to this recovery included a resurgent financial services industry, electronics manufacturing, (see Silicon Glen),[123] and the North Sea oil and gas industry.[124] The introduction in 1989 by Margaret Thatcher's government of the Community Charge (widely known as the Poll Tax) one year before the rest of Great Britain,[125] contributed to a growing movement for Scottish control over domestic affairs.[126] Following a referendum on devolution proposals in 1997, the Scotland Act 1998[127] was passed by the UK Parliament, which established a devolved Scottish Parliament and Scottish Government with responsibility for most laws specific to Scotland.[128] The Scottish Parliament was reconvened in Edinburgh on 4 July 1999.[129] The first First Minister of Scotland was Donald Dewar, who served until his sudden death in 2000.[130]

The Scottish Parliament Building at Holyrood itself did not open until October 2004, after lengthy construction delays and running over budget.[131] The Scottish Parliament has a form of proportional representation (the additional member system), which normally results in no one party having an overall majority. The pro-independence Scottish National Party led by Alex Salmond achieved this in the 2011 election, winning 69 of the 129 seats available.[132] The success of the SNP in achieving a majority in the Scottish Parliament paved the way for the September 2014 referendum on Scottish independence. The majority voted against the proposition, with 55% voting no to independence.[133] More powers, particularly in relation to taxation, were devolved to the Scottish Parliament after the referendum, following cross-party talks in the Smith Commission.
Geography and natural history
Main article: Geography of Scotland
The island of Little Cumbrae with Isle of Arran in the background (left). Traigh Seilebost Beach on the Isle of Harris (right)

The mainland of Scotland comprises the northern third of the land mass of the island of Great Britain, which lies off the north-west coast of Continental Europe. The total area is 78,772 km2 (30,414 sq mi),[134] comparable to the size of the Czech Republic. Scotland's only land border is with England, and runs for 96 kilometres (60 mi) between the basin of the River Tweed on the east coast and the Solway Firth in the west. The Atlantic Ocean borders the west coast and the North Sea is to the east. The island of Ireland lies only 21 kilometres (13 mi) from the south-western peninsula of Kintyre;[135] Norway is 305 kilometres (190 mi) to the east and the Faroes, 270 kilometres (168 mi) to the north.

The territorial extent of Scotland is generally that established by the 1237 Treaty of York between Scotland and the Kingdom of England[136] and the 1266 Treaty of Perth between Scotland and Norway.[21] Important exceptions include the Isle of Man, which having been lost to England in the 14th century is now a crown dependency outside of the United Kingdom; the island groups Orkney and Shetland, which were acquired from Norway in 1472;[134] and Berwick-upon-Tweed, lost to England in 1482.

The geographical centre of Scotland lies a few miles from the village of Newtonmore in Badenoch.[137] Rising to 1,344 metres (4,409 ft) above sea level, Scotland's highest point is the summit of Ben Nevis, in Lochaber, while Scotland's longest river, the River Tay, flows for a distance of 190 kilometres (118 mi).[138][139]
Geology and geomorphology
Main article: Geology of Scotland
The Scottish Highlands, geographically located in the north west of Scotland, is considered to have some of the world's best views

The whole of Scotland was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. From a geological perspective, the country has three main sub-divisions.

The Highlands and Islands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland largely comprises ancient rocks from the Cambrian and Precambrian, which were uplifted during the later Caledonian Orogeny. It is interspersed with igneous intrusions of a more recent age, remnants of which formed mountain massifs such as the Cairngorms and Skye Cuillins.

A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast. The Highlands are generally mountainous and the highest elevations in the British Isles are found here. Scotland has over 790 islands divided into four main groups: Shetland, Orkney, and the Inner Hebrides and Outer Hebrides. There are numerous bodies of freshwater including Loch Lomond and Loch Ness. Some parts of the coastline consist of machair, a low lying dune pasture land.

The Central Lowlands is a rift valley mainly comprising Paleozoic formations. Many of these sediments have economic significance for it is here that the coal and iron bearing rocks that fuelled Scotland's industrial revolution are found. This area has also experienced intense volcanism, Arthur's Seat in Edinburgh being the remnant of a once much larger volcano. This area is relatively low-lying, although even here hills such as the Ochils and Campsie Fells are rarely far from view.

The Southern Uplands are a range of hills almost 200 kilometres (124 mi) long, interspersed with broad valleys. They lie south of a second fault line (the Southern Uplands fault) that runs from Girvan to Dunbar.[140][141][142] The geological foundations largely comprise Silurian deposits laid down some 4–500 million years ago. The high point of the Southern Uplands is Merrick with an elevation of 843 m (2,766 ft).[20][143][144][145] The Southern Uplands is home to the UK's highest village, Wanlockhead (430 m or 1,411 ft above sea level).[142]
Climate
Tiree, one of the sunniest locations in Scotland
Main article: Climate of Scotland

The climate of Scotland is temperate and oceanic, and tends to be very changeable. As it is warmed by the Gulf Stream from the Atlantic, it has much milder winters (but cooler, wetter summers) than areas on similar latitudes, such as Labrador, southern Scandinavia, the Moscow region in Russia, and the Kamchatka Peninsula on the opposite side of Eurasia. However, temperatures are generally lower than in the rest of the UK, with the coldest ever UK temperature of −27.2 °C (−17.0 °F) recorded at Braemar in the Grampian Mountains, on 11 February 1895.[146] Winter maxima average 6 °C (43 °F) in the Lowlands, with summer maxima averaging 18 °C (64 °F). The highest temperature recorded was 32.9 °C (91.2 °F) at Greycrook, Scottish Borders on 9 August 2003.[147]

The west of Scotland is usually warmer than the east, owing to the influence of Atlantic ocean currents and the colder surface temperatures of the North Sea. Tiree, in the Inner Hebrides, is one of the sunniest places in the country: it had more than 300 hours of sunshine in May 1975.[147] Rainfall varies widely across Scotland. The western highlands of Scotland are the wettest, with annual rainfall in a few places exceeding 3,000 mm (120 in).[148] In comparison, much of lowland Scotland receives less than 800 mm (31 in) annually.[149] Heavy snowfall is not common in the lowlands, but becomes more common with altitude. Braemar has an average of 59 snow days per year,[150] while many coastal areas average fewer than 10 days of lying snow per year.[149]
Flora and fauna
A mountain hare (Lepus timidus) in Findhorn Valley, May 2004
Main articles: Fauna of Scotland and Flora of Scotland

Scotland's wildlife is typical of the north-west of Europe, although several of the larger mammals such as the lynx, brown bear, wolf, elk and walrus were hunted to extinction in historic times. There are important populations of seals and internationally significant nesting grounds for a variety of seabirds such as gannets.[151] The golden eagle is something of a national icon.[152]

On the high mountain tops, species including ptarmigan, mountain hare and stoat can be seen in their white colour phase during winter months.[153] Remnants of the native Scots pine forest exist[154] and within these areas the Scottish crossbill, the UK's only endemic bird species and vertebrate, can be found alongside capercaillie, Scottish wildcat, red squirrel and pine marten.[155][156][157] Various animals have been re-introduced, including the white-tailed sea eagle in 1975, the red kite in the 1980s,[158][159] and there have been experimental projects involving the beaver and wild boar. Today, much of the remaining native Caledonian Forest lies within the Cairngorms National Park and remnants of the forest remain at 84 locations across Scotland. On the west coast, remnants of ancient Celtic Rainforest still remain, particularly on the Taynish peninsula in Argyll, these forests are particularly rare due to high rates of deforestation throughout Scottish history.[160][161]

The flora of the country is varied incorporating both deciduous and coniferous woodland and moorland and tundra species. However, large scale commercial tree planting and the management of upland moorland habitat for the grazing of sheep and commercial field sport activities impacts upon the distribution of indigenous plants and animals.[162] The UK's tallest tree is a grand fir planted beside Loch Fyne, Argyll in the 1870s, and the Fortingall Yew may be 5,000 years old and is probably the oldest living thing in Europe.[dubious – discuss][163][164][165] Although the number of native vascular plants is low by world standards, Scotland's substantial bryophyte flora is of global importance.[166][167]
Demographics
Scottish population
by ethnic group (2011)[168]

    v t e 

	 % of total
Population 	Population
White Scottish 	84.0 	4,445,678
White Other British 	7.9 	417,109
White Irish 	1.0 	54,090
White Gypsy/Traveller 	0.1 	4,212
White Polish 	1.2 	61,201
Other White ethnic group 	1.9 	102,117
White Total 	96.0 	5,084,407
Pakistani 	0.9 	49,381
Indian 	0.6 	32,706
Bangladeshi 	0.1 	3,788
Chinese 	0.6 	33,706
Other 	0.4 	21,097
Asian 	2.7 	140,678
Caribbean 	0.1 	3,430
Black 	<0.1 	2,380
Caribbean or Black Other 	<0.1 	730
Caribbean or Black 	0.1 	6,540
African 	0.6 	29,186
African Other 	<0.1 	452
African 	0.6 	29,638
Mixed or multiple ethnic groups 	0.4 	19,815
Arab 	0.2 	9,366
Other 	0.1 	4,959
Other ethnic group 	0.3 	14,325
All population 	100.00 	5,295,403
Main article: Demography of Scotland
See also: Languages of Scotland, Religion in Scotland, and Scottish people

The population of Scotland at the 2001 Census was 5,062,011. This rose to 5,295,400, the highest ever, at the 2011 Census.[169]

In the 2011 Census, 62% of Scotland's population stated their national identity as 'Scottish only', 18% as 'Scottish and British', 8% as 'British only', and 4% chose 'other identity only'.[170]

Although Edinburgh is the capital of Scotland, the largest city is Glasgow, which has just over 584,000 inhabitants. The Greater Glasgow conurbation, with a population of almost 1.2 million, is home to nearly a quarter of Scotland's population.[171] The Central Belt is where most of the main towns and cities are located, including Glasgow, Edinburgh, Dundee and Perth. Scotland's only major city outside the Central Belt is Aberdeen.

In general, only the more accessible and larger islands remain inhabited. Currently, fewer than 90 remain inhabited. The Southern Uplands are essentially rural in nature and dominated by agriculture and forestry.[172][173] Because of housing problems in Glasgow and Edinburgh, five new towns were designated between 1947 and 1966. They are East Kilbride, Glenrothes, Cumbernauld, Livingston, and Irvine.[174]

Immigration since World War II has given Glasgow, Edinburgh and Dundee small South Asian communities.[175] In 2011, there were an estimated 49,000 ethnically Pakistani people living in Scotland, making them the largest non-White ethnic group.[6] Since the Enlargement of the European Union more people from Central and Eastern Europe have moved to Scotland, and the 2011 census indicated that 61,000 Poles live there.[6][176]
Scotland population cartogram. The size of councils is in proportion to their population.

Scotland has three officially recognised languages: English, Scots, and Scottish Gaelic.[177][178] Scottish Standard English, a variety of English as spoken in Scotland, is at one end of a bipolar linguistic continuum, with broad Scots at the other.[179] Scottish Standard English may have been influenced to varying degrees by Scots.[180][181] The 2011 census indicated that 63% of the population had "no skills in Scots".[182] Others speak Highland English. Gaelic is mostly spoken in the Western Isles, where a large proportion of people still speak it; however, nationally its use is confined to just 1% of the population.[183] The number of Gaelic speakers in Scotland dropped from 250,000 in 1881 to 60,000 in 2008.[184]

There are many more people with Scottish ancestry living abroad than the total population of Scotland. In the 2000 Census, 9.2 million Americans self-reported some degree of Scottish descent.[185] Ulster's Protestant population is mainly of lowland Scottish descent,[186] and it is estimated that there are more than 27 million descendants of the Scots-Irish migration now living in the US.[187][188] In Canada, the Scottish-Canadian community accounts for 4.7 million people.[189] About 20% of the original European settler population of New Zealand came from Scotland.[190]

In August 2012, the Scottish population reached an all-time high of 5.25 million people.[191] The reasons given were that, in Scotland, births were outnumbering the number of deaths, and immigrants were moving to Scotland from overseas. In 2011, 43,700 people moved from Wales, Northern Ireland or England to live in Scotland.[191]

The total fertility rate (TFR) in Scotland is below the replacement rate of 2.1 (the TFR was 1.73 in 2011[192]). The majority of births are to unmarried women (51.3% of births were outside of marriage in 2012[193]).
 

    v t e 

Largest cities or towns in Scotland
Scotland's Census 2011 [194]
	Rank 	Name 	Council area 	Pop. 	Rank 	Name 	Council area 	Pop. 	
Glasgow
Glasgow
Edinburgh
Edinburgh 	1 	Glasgow 	Glasgow City 	590,507 	11 	Dunfermline 	Fife 	49,706 	Aberdeen
Aberdeen
Dundee
Dundee
2 	Edinburgh 	City of Edinburgh 	459,366 	12 	Inverness 	Highland 	48,201
3 	Aberdeen 	Aberdeen City 	195,021 	13 	Perth 	Perth and Kinross 	46,970
4 	Dundee 	Dundee City 	147,285 	14 	Ayr 	South Ayrshire 	46,849
5 	Paisley 	Renfrewshire 	76,834 	15 	Kilmarnock 	East Ayrshire 	46,159
6 	East Kilbride 	South Lanarkshire 	74,395 	16 	Greenock 	Inverclyde 	44,248
7 	Livingston 	West Lothian 	56,269 	17 	Coatbridge 	North Lanarkshire 	43,841
8 	Hamilton 	South Lanarkshire 	53,188 	18 	Glenrothes 	Fife 	39,277
9 	Cumbernauld 	North Lanarkshire 	52,270 	19 	Airdrie 	North Lanarkshire 	37,132
10 	Kirkcaldy 	Fife 	49,709 	20 	Stirling 	Stirling 	36,142

Life expectancy for those born in Scotland between 2012 and 2014 is 77.1 years for males and 81.1 years for females.[195] This is the lowest of any of the four countries of the UK.[195]
Religion
Main article: Religion in Scotland
Iona Abbey, an early centre of Christianity in Scotland

Just over half (54%) of the Scottish population reported being a Christian while nearly 37% reported not having a religion in a 2011 census.[196] Since the Scottish Reformation of 1560, the national church (the Church of Scotland, also known as The Kirk) has been Protestant in classification and Reformed in theology. Since 1689 it has had a Presbyterian system of church government, and enjoys independence from the state.[20] Its membership is 398,389,[197] about 7.5% of the total population, though according to the 2014 Scottish Annual Household Survey, 27.8%, or 1.5 million adherents, identified Church of Scotland as their religion.[198] The Church operates a territorial parish structure, with every community in Scotland having a local congregation.

Scotland also has a significant Roman Catholic population, 19% professing that faith, particularly in Greater Glasgow and the north-west.[199] After the Reformation, Roman Catholicism in Scotland continued in the Highlands and some western islands like Uist and Barra, and it was strengthened during the 19th century by immigration from Ireland. Other Christian denominations in Scotland include the Free Church of Scotland, and various other Presbyterian offshoots. "Scotland's third largest church" is the Scottish Episcopal Church.[200]

Islam is the largest non-Christian religion (estimated at around 75,000, which is about 1.4% of the population),[196][201] and there are also significant Jewish, Hindu and Sikh communities, especially in Glasgow.[201] The Samyé Ling monastery near Eskdalemuir, which celebrated its 40th anniversary in 2007, is the first Buddhist monastery in western Europe.[202]
Politics and government
Main articles: Politics of Scotland, Scottish Parliament, and Scottish Government
Elizabeth II in Berlin 2015.JPG 	Nicola Sturgeon 2017a (cropped).jpg
Queen Elizabeth II
Monarch 	Nicola Sturgeon
First Minister

The head of state of the United Kingdom is the monarch, currently Queen Elizabeth II (since 1952). The regnal numbering ("Elizabeth II") caused controversy around the time of her coronation, because there had never been an Elizabeth I in Scotland. The British government stated in April 1953 that future British monarchs would be numbered according to either their English or their Scottish predecessors, whichever number would be higher.[203] For instance, any future King James would be styled James VIII—since the last Scottish King James was James VII (also James II of England, etc.)—while the next King Henry would be King Henry IX throughout the UK even though there have been no Scottish kings of that name. A legal action, MacCormick v Lord Advocate (1953 SC 396), was brought in Scotland to contest the right of the Queen to entitle herself "Elizabeth II" within Scotland, but the Crown won the case.

The monarchy of the United Kingdom continues to use a variety of styles, titles and other royal symbols of statehood specific to pre-union Scotland, including: the Royal Standard of Scotland, the Royal coat of arms used in Scotland together with its associated Royal Standard, royal titles including that of Duke of Rothesay, certain Great Officers of State, the chivalric Order of the Thistle and, since 1999, reinstating a ceremonial role for the Crown of Scotland after a 292-year hiatus.[204]

Scotland has limited self-government within the United Kingdom, as well as representation in the UK Parliament. Executive and legislative powers respectively have been devolved to the Scottish Government and the Scottish Parliament at Holyrood in Edinburgh since 1999. The UK Parliament retains control over reserved matters specified in the Scotland Act 1998, including UK taxes, social security, defence, international relations and broadcasting.[205] The Scottish Parliament has legislative authority for all other areas relating to Scotland. It initially had only a limited power to vary income tax,[206] but powers over taxation and social security were significantly expanded by the Scotland Acts of 2012 and 2016.[207]

The Scottish Parliament can give legislative consent over devolved matters back to the UK Parliament by passing a Legislative Consent Motion if United Kingdom-wide legislation is considered more appropriate for a certain issue. The programmes of legislation enacted by the Scottish Parliament have seen a divergence in the provision of public services compared to the rest of the UK. For instance, university education and care services for the elderly are free at point of use in Scotland, while fees are paid in the rest of the UK. Scotland was the first country in the UK to ban smoking in enclosed public places.[208]
Bute House is the official residence and workplace of the First Minister
Holyrood is the seat of the national parliament of Scotland

The Scottish Parliament is a unicameral legislature with 129 members (MSPs): 73 of them represent individual constituencies and are elected on a first past the post system; the other 56 are elected in eight different electoral regions by the additional member system. MSPs serve for a four-year period (exceptionally five years from 2011–16). The Parliament nominates one of its Members, who is then appointed by the Monarch to serve as First Minister. Other ministers are appointed by the First Minister and serve at his/her discretion. Together they make up the Scottish Government, the executive arm of the devolved government.[209] The Scottish Government is headed by the First Minister, who is accountable to the Scottish Parliament and is the minister of charge of the Scottish Government. The First Minister is also the political leader of Scotland. The Scottish Government also comprises the Deputy First Minister, currently John Swinney MSP, who deputises for the First Minister during a period of absence of overseas visits. Alongside the Deputy First Minister's requirements as Deputy, the minister also has a cabinet ministerial responsibility. Swinney is also currently Cabinet Secretary for Education and Skills.[210] The Scottish Government's cabinet comprises nine cabinet secretaries, who form the Cabinet of Scotland. There are also twelve other ministers, who work alongside the cabinet secretaries in their appointed areas.[211] As a result, junior ministers do not attend cabinet meetings.

In the 2016 election, the Scottish National Party (SNP) won 63 of the 129 seats available. Nicola Sturgeon, the leader of the SNP, has been the First Minister since November 2014. The Conservative Party became the largest opposition party in the 2016 elections, with the Labour Party, Liberal Democrats and the Green Party also represented in the Parliament. The next Scottish Parliament election is due to be held on 6 May 2021.

Scotland is represented in the British House of Commons by 59 MPs elected from territory-based Scottish constituencies. In the 2017 general election, the SNP won 35 of the 59 seats.[212] This represented a significant decline from the 2015 general election, when the SNP won 56 seats.[212][213] Conservative, Labour and Liberal Democrat parties also represent Scottish constituencies in the House of Commons. The next United Kingdom general election is scheduled for 5 May 2022. The Scotland Office represents the UK government in Scotland on reserved matters and represents Scottish interests within the UK government.[214] The Scotland Office is led by the Secretary of State for Scotland, who sits in the Cabinet of the United Kingdom; the incumbent is Conservative MP David Mundell.
Devolved government relations
Sturgeon welcomes Prime Minister of the United Kingdom Theresa May to Bute House, 2016

The relationships between the central UK Government and devolved governments of Scotland, Wales and Northern Ireland are based on the extra statutory principles and agreements with the main elements are set out in a Memorandum of Understanding between the UK government and the devolved governments of Scotland, Wales and Northern Ireland. The MOU lays emphasis on the principles of good communication, consultation and co-operation.[215]

Since devolution in 1999, Scotland has devolved stronger working relations across the two other devolved governments, the Welsh Government and Northern Ireland Executive. Whilst there are no formal concordats between the Scottish Government, Welsh Government and Northern Ireland Executive, ministers from each devolved government meet at various points throughout the year at various events such as the British-Irish Council and also meet to discuss matters and issues that is devolved to each government.[216] Scotland, along with the Welsh Government, British Government as well as the Northern Ireland executive, participate in the Joint Ministerial Committee (JMC) which allows each government to discuss policy issues together and work together across each government to find solutions. The Scottish Government considers the successful re-establishment of the Plenary, and establishment of the Domestic fora to be important facets of the relationship with the UK Government and the other devolved administrations.[216]

In the aftermath of the United Kingdom's decision to withdraw from the European Union in 2016, the Scottish Government has called for there to be a joint approach from each of the devolved governments. In early 2017, the devolved governments met to discuss Brexit and agree on Brexit strategies from each devolved government[217] which lead for Theresa May to issue a statement that claims that the devolved governments will not have a central role or decision making process in the Brexit process, but that the UK Government plans to "fully engage" Scotland in talks alongside the governments of Wales and Northern Ireland.[218]
International diplomacy
Former First Minister Jack McConnell welcomes then President of the United States George Bush to Glasgow Prestwick Airport at the start of the G8 Summit, July 2005

Whilst foreign policy remains a reserved matter,[219] the Scottish Government still has the power and ability to strengthen and develop Scotland, the economy and Scottish interests on the world stage and encourage foreign businesses, international devolved, regional and central governments to invest in Scotland.[220] Whilst the First Minister usually undertakes a number of foreign and international visits to promote Scotland, international relations, European and Commonwealth relations are also included within the portfolios of both the Cabinet Secretary for Culture, Tourism and External Affairs (responsible for international development)[221] and the Minister for International Development and Europe (responsible for European Union relations and international relations).[222]

During the G8 Summit in 2005, then First Minister Jack McConnell welcomed each head of government of the G8 nations to the countries Glasgow Prestwick Airport[223] on behalf of then UK Prime Minister Tony Blair. At the same time, McConnell and the then Scottish Executive pioneered the way forward to launch what would become the Scotland Malawi Partnership which co-ordinates Scottish activities to strengthen existing links with Malawi.[224] During McConnell's time as First Minister, several relations with Scotland, including Scottish and Russian relations strengthened following a visit by President of Russia Vladimir Putin to Edinburgh. McConnell, speaking at the end, highlighted that the visit by Putin was a "post devolution" step towards "Scotland regaining it's international identity".[225]

Under the Salmond administration, Scotland's trade and investment deals with countries such as China[226][227] and Canada, where Salmond established the Canada Plan 2010-2015 which aimed to strengthen "the important historical, cultural and economic links" between both Canada and Scotland.[228] To promote Scotland's interests and Scottish businesses in North America, there is a Scottish Affairs Office located in Washington D.C. with the aim to promoting Scotland in both the United States and Canada.[229]

During a 2017 visit to the United States, First Minister Nicola Sturgeon met with Jerry Brown, Governor of California, where both signed an agreement committing both the Government of California and the Scottish Government to work together to tackle climate change,[230] as well as Sturgeon signing a £6.3 million deal for Scottish investment from American businesses and firms promoting trade, tourism and innovation.[231] During an official visit to the Republic of Ireland in 2016, Sturgeon claimed that is it "important for Ireland and Scotland and the whole of the British Isles that Ireland has a strong ally in Scotland".[232] During the same engagement, Sturgeon became the first head of government to address the Seanad Éireann, the Upper House of the Irish Parliament.[232]
Constitutional changes
The debating chamber within the Scottish Parliament Building

A policy of devolution had been advocated by the three main UK parties with varying enthusiasm during recent history. A previous Labour leader. John Smith, described the revival of a Scottish parliament as the "settled will of the Scottish people".[233] The devolved Scottish Parliament was created after a referendum in 1997 found majority support for both creating the Parliament and granting it limited powers to vary income tax. The constitutional status of Scotland is nonetheless subject to ongoing debate.

The Scottish National Party (SNP), which supports Scottish independence, was first elected to form the Scottish Government in 2007. The new government established a "National Conversation" on constitutional issues, proposing a number of options such as increasing the powers of the Scottish Parliament, federalism, or a referendum on Scottish independence from the United Kingdom. In rejecting the last option, the three main opposition parties in the Scottish Parliament created a commission to investigate the distribution of powers between devolved Scottish and UK-wide bodies.[234] The Scotland Act 2012, based on proposals by the commission, was subsequently enacted devolving additional powers to the Scottish Parliament.[235]

In August 2009 the SNP proposed a bill to hold a referendum on independence in November 2010. Opposition from all other major parties led to an expected defeat.[236][237][238] After the 2011 elections gave the SNP an overall majority in the Scottish Parliament, a referendum on independence for Scotland was held on 18 September 2014.[239] The referendum resulted in a rejection of independence, by 55.3% to 44.7%.[240][241] During the campaign, the three main parties in the UK Parliament pledged to extend the powers of the Scottish Parliament.[242][243] An all-party commission chaired by Lord Smith of Kelvin was formed,[243] which led to a further devolution of powers through the Scotland Act 2016.

Following a referendum on the UK's membership of the European Union on 23 June 2016, where a UK-wide majority voted to withdraw from the EU whilst a majority within Scotland voted to remain, Scotland's First Minister, Nicola Sturgeon, announced that as a result a new independence referendum was "highly likely".[244][245]
Administrative subdivisions
Main article: Subdivisions of Scotland
Glasgow City Chambers, seat of Glasgow City Council

Historical subdivisions of Scotland included the mormaerdom, stewartry, earldom, burgh, parish, county and regions and districts. Some of these names are still sometimes used as geographical descriptors.

Modern Scotland is subdivided in various ways depending on the purpose. In local government, there have been 32 single-tier council areas since 1996,[246] whose councils are responsible for the provision of all local government services. Community councils are informal organisations that represent specific sub-divisions of a council area.

In the Scottish Parliament, there are 73 constituencies and eight regions. For the Parliament of the United Kingdom, there are 59 constituencies. Until 2013, the Scottish fire brigades and police forces were based on a system of regions introduced in 1975. For healthcare and postal districts, and a number of other governmental and non-governmental organisations such as the churches, there are other long-standing methods of subdividing Scotland for the purposes of administration.

City status in the United Kingdom is conferred by letters patent.[247] There are seven cities in Scotland: Aberdeen, Dundee, Edinburgh, Glasgow, Inverness, Stirling and Perth.[248]
Law and criminal justice
Main article: Scots law
High Court of Justiciary, Edinburgh

Scots law has a basis derived from Roman law,[249] combining features of both uncodified civil law, dating back to the Corpus Juris Civilis, and common law with medieval sources. The terms of the Treaty of Union with England in 1707 guaranteed the continued existence of a separate legal system in Scotland from that of England and Wales.[250] Prior to 1611, there were several regional law systems in Scotland, most notably Udal law in Orkney and Shetland, based on old Norse law. Various other systems derived from common Celtic or Brehon laws survived in the Highlands until the 1800s.[251]

Scots law provides for three types of courts responsible for the administration of justice: civil, criminal and heraldic. The supreme civil court is the Court of Session, although civil appeals can be taken to the Supreme Court of the United Kingdom (or before 1 October 2009, the House of Lords). The High Court of Justiciary is the supreme criminal court in Scotland. The Court of Session is housed at Parliament House, in Edinburgh, which was the home of the pre-Union Parliament of Scotland with the High Court of Justiciary and the Supreme Court of Appeal currently located at the Lawnmarket. The sheriff court is the main criminal and civil court, hearing most cases. There are 49 sheriff courts throughout the country.[252] District courts were introduced in 1975 for minor offences and small claims. These were gradually replaced by Justice of the Peace Courts from 2008 to 2010. The Court of the Lord Lyon regulates heraldry.

For many decades the Scots legal system was unique for being the only legal system without a parliament. This ended with the advent of the Scottish Parliament, which legislates for Scotland. Many features within the system have been preserved. Within criminal law, the Scots legal system is unique in having three possible verdicts: "guilty", "not guilty" and "not proven".[253] Both "not guilty" and "not proven" result in an acquittal, typically with no possibility of retrial in accordance with the rule of double jeopardy. There is however the possibility of a retrial where new evidence emerges at a later date that might have proven conclusive in the earlier trial at first instance, where the person acquitted subsequently admits the offence or where it can be proved that the acquittal was tainted by an attempt to pervert the course of justice – see the provisions of the Double Jeopardy (Scotland) Act 2011. Many laws differ between Scotland and the other parts of the United Kingdom, and many terms differ for certain legal concepts. Manslaughter, in England and Wales, is broadly similar to culpable homicide in Scotland, and arson is called wilful fire raising. Indeed, some acts considered crimes in England and Wales, such as forgery, are not so in Scotland. Procedure also differs. Scots juries, sitting in criminal cases, consist of fifteen jurors, which is three more than is typical in many countries.[254]

The Scottish Prison Service (SPS) manages the prisons in Scotland, which collectively house over 8,500 prisoners.[255] The Cabinet Secretary for Justice is responsible for the Scottish Prison Service within the Scottish Government.
Health care
Main article: Healthcare in Scotland
NHS Scotland's Queen Elizabeth University Hospital, Glasgow

Health care in Scotland is mainly provided by NHS Scotland, Scotland's public health care system. This was founded by the National Health Service (Scotland) Act 1947 (later repealed by the National Health Service (Scotland) Act 1978) that took effect on 5 July 1948 to coincide with the launch of the NHS in England and Wales. However, even prior to 1948, half of Scotland's landmass was already covered by state-funded health care, provided by the Highlands and Islands Medical Service.[256] Healthcare policy and funding is the responsibility of the Scottish Government's Health Directorates. The current Cabinet Secretary for Health and Sport is Shona Robison[257] and the Director-General (DG) Health and chief executive, NHS Scotland is Paul Gray.[258]

In 2008, the NHS in Scotland had around 158,000 staff including more than 47,500 nurses, midwives and health visitors and over 3,800 consultants. There are also more than 12,000 doctors, family practitioners and allied health professionals, including dentists, opticians and community pharmacists, who operate as independent contractors providing a range of services within the NHS in return for fees and allowances. These fees and allowances were removed in May 2010, and prescriptions are entirely free, although dentists and opticians may charge if the patient's household earns over a certain amount, about £30,000 per annum.[259]
Economy
Main article: Economy of Scotland
A drilling rig located in the North Sea
The Bank of Scotland, located in Edinburgh, is one of the oldest banks in the world

The Economy of Scotland had an estimated nominal gross domestic product (GDP) of up to £152 billion in 2015. In 2014, Scotland's per capita GDP was one of the highest in the EU.[260] Scotland has a Western-style open mixed economy closely linked with the rest of the UK and the wider world. Traditionally, the Scottish economy has been dominated by heavy industry underpinned by shipbuilding in Glasgow, coal mining and steel industries. Petroleum related industries associated with the extraction of North Sea oil have also been important employers from the 1970s, especially in the north-east of Scotland.

In February 2012, the Centre for Economics and Business Research concluded that "Scotland receives no net subsidy" from the UK, as greater per capita tax generation in Scotland balanced out greater per capita public spending.[261] More recent data, from 2012–13, show that Scotland generated 9.1% (£53.1bn; this included a geographical share of North Sea oil revenue – without it, the figures were 8.2% and £47.6bn) of the UK's tax revenues and received 9.3% (£65.2bn) of spending.[262] Scotland's public spending deficit in 2012–13 was £12bn, a £3.5bn increase on the previous year; over the same period, the UK's deficit decreased by £2.6bn.[263] Over the past thirty years, Scotland contributed a relative budget surplus[clarification needed] of almost £20billion to the UK economy.[264]

In the final quarter of 2016, the Scottish economy contracted by 0.2%;[265] the UK as a whole grew by 0.7% in the same period.[266] As of September 2015, the Scottish unemployment rate of 5.9% was above the UK rate of 5.5%, while the Scottish employment rate of 74.0% was higher than the UK figure of 73.5%.[267] De-industrialisation during the 1970s and 1980s saw a shift from a manufacturing focus towards a more service-oriented economy.
Scotland's shipbuilding industry produces world-class ships, including Queen Elizabeth 2 (pictured).

Edinburgh is the financial services centre of Scotland, with many large finance firms based there, including: Lloyds Banking Group (owners of HBOS); the Government owned Royal Bank of Scotland and Standard Life. Edinburgh was ranked 15th in the list of world financial centres in 2007, but fell to 37th in 2012, following damage to its reputation,[268] and in 2016 was ranked 56th out of 86.[269]

In 2014, total Scottish exports (excluding intra-UK trade) were estimated to be £27.5 billion.[270] Scotland's primary exports include whisky, electronics and financial services.[271] The United States, Netherlands, Germany, France and Norway constitute the country's major export markets.[271] Scotland's Gross Domestic Product (GDP), including oil and gas produced in Scottish waters, was estimated at £150 billion for the calendar year 2012.[12] If Scotland became independent, it would hold 95% of the UK's current oil and gas reserves if they were split geographically using a median line from the English-Scottish border.[citation needed] If the reserves were split by population, that figure would be reduced to 9%.[272]

Whisky is one of Scotland's more known goods of economic activity. Exports increased by 87% in the decade to 2012[273] and were valued at £4.3 billion in 2013, which was 85% of Scotland's food and drink exports.[274] It supports around 10,000 jobs directly and 25,000 indirectly.[275] It may contribute £400–682 million to Scotland, rather than several billion pounds, as more than 80% of whisky produced is owned by non-Scottish companies.[276]

A briefing published in 2002 by the Scottish Parliament Information Centre (SPICe) for the Scottish Parliament's Enterprise and Life Long Learning Committee stated that tourism accounted for up to 5% of GDP and 7.5% of employment.[277]
Currency
A £100 Sterling RBS note
Main article: Banknotes of the pound sterling § Scotland

Although the Bank of England is the central bank for the UK, three Scottish clearing banks issue Sterling banknotes: the Bank of Scotland; the Royal Bank of Scotland; and the Clydesdale Bank. The value of the Scottish banknotes in circulation in 2013 was £3.8 billion; underwritten by the Bank of England using funds deposited by each clearing bank, under the Banking Act, (2009), in order to cover the total value of such notes in circulation.[278]
Military
Main article: Military of Scotland
A Challenger 2 main battle tank of the Royal Scots Dragoon Guards

Of the money spent on UK defence, about £3.3 billion can be attributed to Scotland as of 2013. Although Scotland has a long military tradition predating the Treaty of Union with England, its armed forces now form part of the British Armed Forces, with the exception of the Atholl Highlanders, Europe's only legal private army. In 2006, the infantry regiments of the Scottish Division were amalgamated to form the Royal Regiment of Scotland. Other distinctively Scottish regiments in the British Army include the Scots Guards, the Royal Scots Dragoon Guards and the 154 (Scottish) Regiment RLC, an Army Reserve Regiment of the Royal Logistic Corps.

Because of their topography and perceived remoteness, parts of Scotland have housed many sensitive defence establishments.[279][280][281] Between 1960 and 1991, the Holy Loch was a base for the US fleet of Polaris ballistic missile submarines.[282] Today, Her Majesty's Naval Base Clyde, 25 miles (40 kilometres) north-west of Glasgow, is the base for the four Trident-armed Vanguard class ballistic missile submarines that comprise the UK's nuclear deterrent. Scapa Flow was the major Fleet base for the Royal Navy until 1956.

A single front-line Royal Air Force base is located in Scotland. RAF Lossiemouth, located in Moray, is the most northerly air defence fighter base in the United Kingdom and is home to three fast-jet squadrons equipped with the Eurofighter Typhoon.
Education
Main article: Education in Scotland
Although founded as University of the West of Scotland in 2007, it can trace its history back to 1897 as the University of Paisley
University of St Andrews is the oldest University in Scotland and third oldest in the world

The Scottish education system has always been distinct from the rest of the United Kingdom, with a characteristic emphasis on a broad education.[283] In the 15th century, the Humanist emphasis on education cumulated with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools to learn "perfyct Latyne", resulting in an increase in literacy among a male and wealthy elite.[284] In the Reformation the 1560 First Book of Discipline set out a plan for a school in every parish, but this proved financially impossible.[285] In 1616 an act in Privy council commanded every parish to establish a school.[286] By the late seventeenth century there was a largely complete network of parish schools in the lowlands, but in the Highlands basic education was still lacking in many areas.[287] Education remained a matter for the church rather than the state until the Education Act (1872).[288]

The Curriculum for Excellence, Scotland's national school curriculum, presently provides the curricular framework for children and young people from age 3 to 18.[289] All 3- and 4-year-old children in Scotland are entitled to a free nursery place. Formal primary education begins at approximately 5 years old and lasts for 7 years (P1–P7); children in Scotland study Standard Grades, or Intermediate qualifications between the ages of 14 and 16. These are being phased out and replaced by the National Qualifications of the Curriculum for Excellence. The school leaving age is 16, after which students may choose to remain at school and study for Access, Intermediate or Higher Grade and Advanced Higher qualifications. A small number of students at certain private, independent schools may follow the English system and study towards GCSEs and A and AS-Levels instead.[290]

There are fifteen Scottish universities, some of which are amongst the oldest in the world.[291][292] These include the University of St Andrews, the University of Glasgow, the University of Aberdeen and the University of Edinburgh—many of which are ranked amongst the best in the UK.[293][294] Proportionally, Scotland had more universities in QS' World University Rankings' top 100 in 2012 than any other nation.[295] The country produces 1% of the world's published research with less than 0.1% of the world's population, and higher education institutions account for 9% of Scotland's service sector exports.[296][297] Scotland's University Courts are the only bodies in Scotland authorised to award degrees.

Tuition is handled by the Student Awards Agency Scotland (SAAS), which does not charge fees to what it defines as "Young Students". Young Students are defined as those under 25, without children, marriage, civil partnership or cohabiting partner, who have not been outside of full-time education for more than three years. Fees exist for those outside the young student definition, typically from £1,200 to £1,800 for undergraduate courses, dependent on year of application and type of qualification. Postgraduate fees can be up to £3,400.[298] The system has been in place since 2007 when graduate endowments were abolished.[299] Labour's education spokesperson Rhona Brankin criticised the Scottish system for failing to address student poverty.[300] Scotland has fewer disadvantaged students than England, Wales or Northern Ireland and disadvantaged students receive around £560 a year less in financial support than their counterparts in England do.[301]

Scotland's universities are complemented in the provision of Further and Higher Education by 43 Colleges. Colleges offer National Certificates, Higher National Certificates and Higher National Diplomas. These Group Awards, alongside Scottish Vocational Qualifications, aim to ensure Scotland's population has the appropriate skills and knowledge to meet workplace needs. In 2014, research reported by the Office for National Statistics found that Scotland was the most highly educated country in Europe and among the most well-educated in the world in terms of tertiary education attainment, with roughly 40% of people in Scotland aged 16–64 educated to NVQ level 4 and above.[302] Based on the original data for EU statistical regions, all four Scottish regions ranked significantly above the European average for completion of tertiary-level education by 25- to 64-year-olds.[303]
Culture
Main articles: Culture of Scotland and National symbols of Scotland
See also: Scottish people, Music of Scotland, Scottish literature, Scottish art, Media of Scotland, and Scottish cuisine
Robert Burns, regarded as the national poet of Scotland is a well known and respect poet worldwide (left). The bagpipes are a well known symbol of Scotland and an early example of popular Scottish music (right)
Scottish music
Main article: Scottish music

Scottish music is a significant aspect of the nation's culture, with both traditional and modern influences. A famous traditional Scottish instrument is the Great Highland Bagpipe, a wind instrument consisting of three drones and a melody pipe (called the chanter), which are fed continuously by a reservoir of air in a bag. Bagpipe bands, featuring bagpipes and various types of drums, and showcasing Scottish music styles while creating new ones, have spread throughout the world. The clàrsach (harp), fiddle and accordion are also traditional Scottish instruments, the latter two heavily featured in Scottish country dance bands. There are many successful Scottish bands and individual artists in varying styles including Annie Lennox, Amy Macdonald, Runrig, Boards of Canada, Cocteau Twins, Deacon Blue, Franz Ferdinand, Susan Boyle, Emeli Sandé, Texas, The View, The Fratellis, Twin Atlantic and Biffy Clyro. Other Scottish musicians include Shirley Manson, Paolo Nutini and Calvin Harris.[304]
Literature

Scotland has a literary heritage dating back to the early Middle Ages. The earliest extant literature composed in what is now Scotland was in Brythonic speech in the 6th century, but is preserved as part of Welsh literature.[305] Later medieval literature included works in Latin,[306] Gaelic,[307] Old English[308] and French.[309] The first surviving major text in Early Scots is the 14th-century poet John Barbour's epic Brus, focusing on the life of Robert I,[310] and was soon followed by a series of vernacular romances and prose works.[311] In the 16th century the crown's patronage helped the development of Scots drama and poetry,[312] but the accession of James VI to the English throne removed a major centre of literary patronage and Scots was sidelined as a literary language.[313] Interest in Scots literature was revived in the 18th century by figures including James Macpherson, whose Ossian Cycle made him the first Scottish poet to gain an international reputation and was a major influence on the European Enlightenment.[314] It was also a major influence on Robert Burns, whom many consider the national poet,[315] and Walter Scott, whose Waverley Novels did much to define Scottish identity in the 19th century.[316] Towards the end of the Victorian era a number of Scottish-born authors achieved international reputations as writers in English, including Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald.[317] In the 20th century the Scottish Renaissance saw a surge of literary activity and attempts to reclaim the Scots language as a medium for serious literature.[318] Members of the movement were followed by a new generation of post-war poets including Edwin Morgan, who would be appointed the first Scots Makar by the inaugural Scottish government in 2004.[319] From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of writers including Irvine Welsh.[318] Scottish poets who emerged in the same period included Carol Ann Duffy, who, in May 2009, was the first Scot named UK Poet Laureate.[320]
Celtic connections
Saint Andrew depicted on a 16th-century coat of arms of the burgh of St. Andrews

As one of the Celtic nations, Scotland and Scottish culture is represented at interceltic events at home and over the world. Scotland hosts several music festivals including Celtic Connections (Glasgow), and the Hebridean Celtic Festival (Stornoway). Festivals celebrating Celtic culture, such as Festival Interceltique de Lorient (Brittany), the Pan Celtic Festival (Ireland), and the National Celtic Festival (Portarlington, Australia), feature elements of Scottish culture such as language, music and dance.[321][322][323][324][325][326][327]

The image of St. Andrew, martyred while bound to an X-shaped cross, first appeared in the Kingdom of Scotland during the reign of William I.[328] Following the death of King Alexander III in 1286 an image of Andrew was used on the seal of the Guardians of Scotland who assumed control of the kingdom during the subsequent interregnum.[329] Use of a simplified symbol associated with Saint Andrew, the saltire, has its origins in the late 14th century; the Parliament of Scotland decreeing in 1385 that Scottish soldiers should wear a white Saint Andrew's Cross on the front and back of their tunics.[330] Use of a blue background for the Saint Andrew's Cross is said to date from at least the 15th century.[331] Since 1606 the saltire has also formed part of the design of the Union Flag. There are numerous other symbols and symbolic artefacts, both official and unofficial, including the thistle, the nation's floral emblem (celebrated in the song, The Thistle o' Scotland), the Declaration of Arbroath, incorporating a statement of political independence made on 6 April 1320, the textile pattern tartan that often signifies a particular Scottish clan and the royal Lion Rampant flag.[332][333][334] Highlanders can thank James Graham, 3rd Duke of Montrose, for the repeal in 1782 of the Act of 1747 prohibiting the wearing of tartans.[335]


National identity

Although there is no official national anthem of Scotland,[336] Flower of Scotland is played on special occasions and sporting events such as football and rugby matches involving the Scotland national teams and since 2010 is also played at the Commonwealth Games after it was voted the overwhelming favourite by participating Scottish athletes.[337] Other currently less popular candidates for the National Anthem of Scotland include Scotland the Brave, Highland Cathedral, Scots Wha Hae and A Man's A Man for A' That.

St Andrew's Day, 30 November, is the national day, although Burns' Night tends to be more widely observed, particularly outside Scotland. In 2006, the Scottish Parliament passed the St. Andrew's Day Bank Holiday (Scotland) Act 2007, designating the day an official bank holiday.[338] Tartan Day is a recent innovation from Canada.

The national animal of Scotland is the unicorn, which has been a Scottish heraldic symbol since the 12th century.[339]
Cuisine
Main article: Scottish cuisine

Scottish cuisine has distinctive attributes and recipes of its own, but shares much with wider British and European cuisine as a result of local and foreign influences, both ancient and modern. Traditional Scottish dishes exist alongside international foodstuffs brought about by migration. Scotland's natural larder of game, dairy products, fish, fruit, and vegetables is the chief factor in traditional Scots cooking, with a high reliance on simplicity and a lack of spices from abroad, as these were historically rare and expensive. Irn-Bru is the most common Scottish carbonated soft drink, often described as "Scotland's other national drink" (after whisky). During the Late Middle Ages and early modern era, French cuisine played a role in Scottish cookery due to cultural exchanges brought about by the "Auld Alliance",[340] especially during the reign of Mary, Queen of Scots. Mary, on her return to Scotland, brought an entourage of French staff who are considered responsible for revolutionising Scots cooking and for some of Scotland's unique food terminology.

    Cock-a-leekie soup

    Cullen skink

    Scotch broth

    Haggis, neeps and tatties

Media
Main article: Media of Scotland
Scottish inventor John Logie Baird demonstrated the first working television system on 26 January 1926.[341]

National newspapers such as the Daily Record, The Herald, The Scotsman and The National are all produced in Scotland.[342] Important regional dailies include the Evening News in Edinburgh The Courier in Dundee in the east, and The Press and Journal serving Aberdeen and the north.[342] Scotland is represented at the Celtic Media Festival, which showcases film and television from the Celtic countries. Scottish entrants have won many awards since the festival began in 1980.[343]

Television in Scotland is largely the same as UK-wide broadcasts, however the national broadcaster is BBC Scotland, a constituent part of the British Broadcasting Corporation, the publicly funded broadcaster of the United Kingdom. It runs three national television stations, and the national radio stations, BBC Radio Scotland and BBC Radio nan Gaidheal, amongst others. Scotland also has some programming in the Gaelic language. BBC Alba is the national Gaelic-language channel. The main Scottish commercial television station is STV.
Sport
Main article: Sport in Scotland
The Old Course at St Andrews

Scotland hosts its own national sporting competitions and has independent representation at several international sporting events, including the FIFA World Cup, the Rugby Union World Cup, the Rugby League World Cup, the Cricket World Cup, the Netball World Cup and the Commonwealth Games. Scotland has its own national governing bodies, such as the Scottish Football Association (the second oldest national football association in the world)[344] and the Scottish Rugby Union. Variations of football have been played in Scotland for centuries, with the earliest reference dating back to 1424.[345] Association football is the most popular sport and the Scottish Cup is the world's oldest national trophy.[346]

Scotland contested the first ever international football game in 1872 against England.[347] The match took place at Hamilton Crescent, Glasgow, home of the West of Scotland Cricket Club. Scottish clubs have been successful in European competitions with Celtic winning the European Cup in 1967, Rangers and Aberdeen winning the UEFA Cup Winners' Cup in 1972 and 1983 respectively, and Aberdeen also winning the UEFA Super Cup in 1983.

With the modern game of golf originating in 15th century Scotland, the country is promoted as the home of golf.[348][349][350] To many golfers the Old Course in the Fife town of St. Andrews, an ancient links course dating to before 1574, is considered a site of pilgrimage.[351] In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes.[352] The world's oldest golf tournament, and golf's first major, is The Open Championship, which was first played on 17 October 1860 at Prestwick Golf Club, in Ayrshire, Scotland, with Scottish golfers winning the earliest majors.[353] There are many other famous golf courses in Scotland, including Carnoustie, Gleneagles, Muirfield, and Royal Troon. Other distinctive features of the national sporting culture include the Highland games, curling and shinty. In boxing, Scotland has had 13 world champions, including Ken Buchanan, Benny Lynch and Jim Watt.

Scotland has competed at every Commonwealth Games since 1930 and has won 356 medals in total—91 Gold, 104 Silver and 161 Bronze.[354] Edinburgh played host to the Commonwealth Games in 1970 and 1986, and most recently Glasgow in 2014.[355]
Infrastructure
Transport
Main article: Transport in Scotland
Bilingual (Gaelic/English) roadsigns are found throughout the Highlands and the Hebrides.
Road

The Scottish motorways and major trunk roads are managed by Transport Scotland. The remainder of the road network is managed by the Scottish local authorities in each of their areas.
Air
Air Scotland, founded in 2002, largely served as Scotland's flag carrier until it ceased operations in 2005

Scotland has five main international airports (Glasgow, Edinburgh, Aberdeen, Glasgow Prestwick and Inverness), which together serve 150 international destinations with a wide variety of scheduled and chartered flights.[356] GIP operates Edinburgh airport and BAA operates (Aberdeen and Glasgow International), while Highland and Islands Airports operates 11 regional airports, including Inverness, which serve the more remote locations.[357] The Scottish Government owns Glasgow Prestwick, having purchased the airport from Infratil for a nominal sum.[358]

Over the period of history, Scotland has had several national airlines that has acted as the countries flag carrier, however, most of which are now defunct. Airline companies such as Air Scotland, Caledonian Airways, Scottish Airlines and Highland Airways (founded as Air Alba), all at one stage was seen to be Scotland's national airline and flag carrier. Loganair, still in operation and mostly operations in the Scottish highlands and serving the outer islands of Scotland, is largely considered to be the modern day flag carrier of Scotland and in 2017 to honour this title, Loganair revamped and introduced new and current airlines with their updated Tartan Aircraft livery to help bring "a new Scottish identify to the skies"[359].
Rail
Domestic rail services are operated by Abellio ScotRail.

Network Rail Infrastructure Limited owns and operates the fixed infrastructure assets of the railway system in Scotland, while the Scottish Government retains overall responsibility for rail strategy and funding in Scotland.[360] Scotland's rail network has around 350 railway stations and 3,000 kilometres (1,900 mi) of track. Over 89.3 million passenger journeys are made each year.[361]

The East Coast and West Coast main railway lines connect the major cities and towns of Scotland with each other and with the rail network in England. Virgin Trains provides inter-city rail journeys between Glasgow, Edinburgh, Aberdeen and Inverness to London. Domestic rail services within Scotland are operated by ScotRail. During the time of British Rail the West Coast Main Line from London Euston to Glasgow Central was electrified in the early 1970s, followed by the East Coast Main Line in the late 1980s. British Rail created the ScotRail brand. When British Rail existed, many railway lines in Strathclyde were electrified. Strathclyde Passenger Transport Executive was at the forefront with the acclaimed "largest electrified rail network outside London". Some parts of the network are electrified, but there are no electrified lines in the Highlands, Angus, Aberdeenshire, the cities of Dundee or Aberdeen, or Perth & Kinross, and none of the islands has a rail link (although the railheads at Kyle of Lochalsh and Mallaig principally serve the islands).

The East Coast Main Line crosses the Firth of Forth by the Forth Bridge. Completed in 1890, this cantilever bridge has been described as "the one internationally recognised Scottish landmark".[362] Scotland's rail network is managed by Transport Scotland.[361]
Water
A Calmac ferry at Greenock

Regular ferry services operate between the Scottish mainland and outlying islands. Ferries serving both the inner and outer Hebrides are principally operated by the State-owned enterprise Caledonian MacBrayne.

Services to the Northern Isles are operated by Serco. Other routes, served by multiple companies, connect southwest Scotland to Northern Ireland. DFDS Seaways operate a freight-only service from Rosyth, near Edinburgh, to Zeebrugge, Belgium.

Additional routes are operated by local authorities.
Renewable energy
Main article: Renewable energy in Scotland

Increasing amounts of Scotland's electricity are generated through solar power and wind power, a sizable proportion of Scotland's electricity is generated that way.[363]
See also

    flagScotland portal flagUnited Kingdom portal Celtic Studies portal 

    Celtic languages
    Celts
    Ethnic groups in Europe
    Outline of Scotland

Notes
References

"St Andrew—Quick Facts". Scotland. org—The Official Online Gateway. Archived from the original on 11 November 2007. Retrieved 2 December 2007.
"St Andrew". Catholic Online. Retrieved 15 November 2011.
"St Margaret of Scotland". Catholic Online. Retrieved 15 November 2011.
"Patron saints". Catholic Online. Retrieved 15 November 2011.
"St Columba". Catholic Online. Retrieved 15 November 2011.
"Ethnic groups, Scotland, 2001 and 2011" (PDF). The Scottish Government. 2013. Retrieved 9 December 2013.
"Scotland's Census 2011 – Table KS209SCb" (PDF). scotlandscensus.gov.uk. Retrieved 26 March 2017.
Region and Country Profiles, Key Statistics and Profiles, October 2013, ONS. Retrieved 9 August 2015.
"Scottish population rises to new record". BBC News. BBC. 27 April 2017. Retrieved 27 April 2017.
"Population estimates by sex, age and administrative area, Scotland, 2011 and 2012". National Records of Scotland. 8 August 2013. Retrieved 8 August 2013.
Office for National Statistics. "Regional gross value added (income approach), UK: 1997 to 2015, December 2015". Retrieved 24 April 2017.
Scottish Government. "Key Economy Statistics". Retrieved 22 August 2014.
"European Charter for Regional or Minority Languages". Scottish Government. Retrieved 23 October 2011.[dead link]
Macleod, Angus "Gaelic given official status" (22 April 2005) The Times. London. Retrieved 2 August 2007.
"Scotland becomes first part of UK to recognise signing for deaf as official language". Herald Scotland. 2015. Retrieved 17 January 2016.
"The Countries of the UK". Office for National Statistics. Retrieved 24 June 2012.
"Countries within a country". 10 Downing Street. Archived from the original on 16 April 2010. Retrieved 24 August 2008. "The United Kingdom is made up of four countries: England, Scotland, Wales and Northern Ireland"
"ISO 3166-2 Newsletter Date: 28 November 2007 No I-9. "Changes in the list of subdivision names and code elements" (Page 11)" (PDF). International Organization for Standardization codes for the representation of names of countries and their subdivisions – Part 2: Country subdivision codes. Retrieved 31 May 2008. "SCT Scotland country"
"Scottish Executive Resources" (PDF). Scotland in Short. Scottish Executive. 17 February 2007. Retrieved 14 September 2006.
Keay, J. & Keay, J. (1994) Collins Encyclopaedia of Scotland. London. HarperCollins.
Mackie, J.D. (1969) A History of Scotland. London. Penguin.
"Parliament and Ireland". London: The Houses of Parliament. Retrieved 26 December 2016.
Collier, J. G. (2001) Conflict of Laws (Third edition)(pdf) Cambridge University Press. "For the purposes of the English conflict of laws, every country in the world which is not part of England and Wales is a foreign country and its foreign laws. This means that not only totally foreign independent countries such as France or Russia ... are foreign countries but also British Colonies such as the Falkland Islands. Moreover, the other parts of the United Kingdom – Scotland and Northern Ireland – are foreign countries for present purposes, as are the other British Islands, the Isle of Man, Jersey and Guernsey."
Devine, T. M. (1999), The Scottish Nation 1700–2000, P.288–289, ISBN 0-14-023004-1 "created a new and powerful local state run by the Scottish bourgeoisie and reflecting their political and religious values. It was this local state, rather than a distant and usually indifferent Westminster authority, that in effect routinely governed Scotland"
"Devolution Settlement, Scotland". gov.uk. Retrieved 7 May 2017.
"Scottish MEPs". Europarl.org.uk. Archived from the original on 1 May 2014. Retrieved 26 May 2014.
"Scotland / Alba". British-Irish Council. Retrieved 4 May 2013.
http://www.britishirish.org/members-2/
The History Of Ireland. Retrieved 17 September 2014.
Ayto, John; Ian Crofton. Brewer's Britain & Ireland: The History, Culture, Folklore and Etymology of 7500 Places in These Islands. WN. ISBN 0-304-35385-X.
The earliest known evidence is a flint arrowhead from Islay. See Moffat, Alistair (2005) Before Scotland: The Story of Scotland Before History. London. Thames & Hudson. Page 42.
Sites at Cramond dated to 8500 BC and near Kinloch, Rùm from 7700 BC provide the earliest known evidence of human occupation in Scotland. See "The Megalithic Portal and Megalith Map: Rubbish dump reveals time-capsule of Scotland's earliest settlements" megalithic.co.uk. Retrieved 10 February 2008 and Edwards, Kevin J. and Whittington, Graeme "Vegetation Change" in Edwards, Kevin J. & Ralston, Ian B.M. (Eds) (2003) Scotland After the Ice Age: Environment, Archaeology and History, 8000 BC–AD 1000. Edinburgh. Edinburgh University Press. Page 70.
Pryor, Francis (2003). Britain BC. London: HarperPerennial. pp. 98–104 & 246–250. ISBN 978-0-00-712693-4.
Keys, David (14 August 2009). "Ancient royal tomb found in Scotland". The Independent. London. Retrieved 16 August 2009.
Brophy, Kenneth; Noble, Gordon; Driscoll, Stephen (2010). "The Forteviot dagger burial". History Scotland. 10 (1): 12–13. ISSN 1475-5270.
Koch, John. "O'Donnell Lecture 2008 Appendix" (PDF). University of Wales. Retrieved 27 May 2010.
Koch, John (2009). Tartessian: Celtic from the Southwest at the Dawn of History in Acta Palaeohispanica X Palaeohispanica 9 (2009) (PDF). Palaeohispanica. pp. 339–351. ISSN 1578-5386. Retrieved 17 May 2010.
Koch, John. "New research suggests Welsh Celtic roots lie in Spain and Portugal". The Megalithic Portal. Retrieved 10 May 2010.
Cunliffe, Barry (2008). A Race Apart: Insularity and Connectivity in Proceedings of the Prehistoric Society 75, 2009, pp. 55–64. The Prehistoric Society. p. 61.
Bryson 2010
"Skara Brae: The Discovery of the Village". Retrieved 17 September 2014.
"The Romans in Scotland". BBC.
Hanson, William S. The Roman Presence: Brief Interludes, in Edwards, Kevin J. & Ralston, Ian B.M. (Eds) (2003). Scotland After the Ice Age: Environment, Archeology and History, 8000 BC—AD 1000. Edinburgh. Edinburgh University Press.
Snyder, Christopher A. (2003). The Britons. Blackwell Publishing. ISBN 0-631-22260-X.
Robertson, Anne S. (1960). The Antonine Wall. Glasgow Archaeological Society.
"Dalriada: The Land of the First Scots". BBC – Legacies. Retrieved 4 January 2014.
"Scot (ancient people)". Encyclopædia Britannica.
Campbell, Ewan. (2001). "Were the Scots Irish?" in Antiquity No. 75.
Peter Heather, "State Formation in Europe in the First Millennium A.D.", in Barbara Crawford (ed.), Scotland in Dark Ages Europe, (Aberdeen, 1994), pp. 47–63
For instance, Alex Woolf, "The Verturian Hegemony: a mirror in the North", in M. P. Brown & C. A. Farr, (eds.), Mercia: an Anglo-Saxon Kingdom in Europe, (Leicester, 2001), pp. 106–11.
Brown, Dauvit (2001). "Kenneth mac Alpin". In M. Lynch. The Oxford Companion to Scottish History. Oxford: Oxford University Press. p. 359. ISBN 978-0-19-211696-3.
Brown, Dauvit (1997). "Dunkeld and the origin of Scottish identity". Innes Review. Glasgow: Scottish Catholic Historical Association (48): 112–124. reprinted in Dauvit Broun and Thomas Owen Clancy (eds.), (1999)Spes Scotorum: Hope of Scots, Edinburgh: T.& T.Clark, pp. 95–111. ISBN 978-0-567-08682-2
Foster, Sally (1996). Picts, Gaels and Scots (Historic Scotland). London: Batsford. ISBN 978-0-7134-7485-5.
Withers, Charles, W.J. (1984). Gaelic in Scotland, 1698–1981. Edinburgh: John Donald. pp. 16–41;. ISBN 978-0-85976-097-3.
Barrow, Geoffrey, W. S. (2005) [1965]. Robert Bruce & the Community of the Realm of Scotland (4th ed.). Edinburgh University Press. ISBN 0-7486-2022-2.
Thomas Owen Clancy. "Gaelic Scotland: a brief history". Bòrd na Gàidhlig. Archived from the original on 11 September 2007. Retrieved 21 September 2007.
"Scotland Conquered, 1174–1296". National Archives.
"Scotland Regained, 1297–1328". National Archives of the United Kingdom.
Murison, A. F. (1899). King Robert the Bruce (reprint 2005 ed.). Kessinger Publishing. p. 30. ISBN 978-1-4179-1494-4.
Grant, Alexander (6 June 1991) [1984]. Independence and Nationhood: Scotland, 1306–1469 (New ed.). Edinburgh University Press. pp. 3–57. ISBN 978-0-7486-0273-5.
Wormald, Jenny (6 June 1991) [1981]. Court, Kirk and Community: Scotland (New ed.). Edinburgh University Press. ISBN 978-0-7486-0276-6.
"Medieval life Garde Ecossaise". Learning Scotland. Archived from the original on 2 January 2012.
The Illustrated Encyclopedia of Warfare. DK Publishing. 2012. p. 391.
"James IV, King of Scots 1488–1513". BBC.
"Battle of Flodden, (Sept. 9, 1513),". Encyclopædia Britannica.
"The Scottish Reformation,". BBC Scotland.
"Religion, Marriage and Power in Scotland, 1503–1603". The National Archives of the United Kingdom.
Ross, David (2002). Chronology of Scottish History. Geddes & Grosset. p. 56. ISBN 1-85534-380-0. "1603: James VI becomes James I of England in the Union of the Crowns, and leaves Edinburgh for London"
Cullen, Karen J. (15 February 2010). Famine in Scotland: The 'ill Years' of The 1690s. Edinburgh University Press. pp. 152–3. ISBN 0748638873.
"Why did the Scottish parliament accept the Treaty of Union?" (PDF). Scottish Affairs. Archived from the original (PDF) on 3 October 2011. Retrieved 1 May 2013.
"Popular Opposition to the Ratification of the Treaty of Anglo-Scottish Union in 1706–7". scottishhistorysociety.com. Scottish Historical Society. Retrieved 23 March 2017.
Devine, T. M. (1999). The Scottish Nation 1700–2000. Penguin Books. p. 9. ISBN 0-14-023004-1. "From that point on anti-union demonstrations were common in the capital. In November rioting spread to the south west, that stronghold of strict Calvinism and covenanting tradition. The Glasgow mob rose against union sympathisers in disturbances that lasted intermittently for over a month"
"Act of Union 1707 Mob unrest and disorder". London: The House of Lords. 2007. Archived from the original on 1 January 2008. Retrieved 23 December 2007.
"The Tobacco Lords: A study of the Tobacco Merchants of Glasgow and their Activities". Virginia Historical Society. JSTOR 4248011.
"Some Dates in Scottish History from 1745 to 1914 Archived 31 October 2013 at the Wayback Machine.", The University of Iowa.
"Enlightenment Scotland". Learning and Teaching Scotland.
Neil Davidson(2000). The Origins of Scottish Nationhood. London: Pluto Press. pp. 94–95.
T. M. Devine and R. J. Finlay, Scotland in the Twentieth Century (Edinburgh: Edinburgh University Press, 1996), pp. 64–5.
F. Requejo and K-J Nagel, Federalism Beyond Federations: Asymmetry and Processes of Re-symmetrization in Europe (Aldershot: Ashgate, 2011), p. 39.
R. Quinault, "Scots on Top? Tartan Power at Westminster 1707–2007", History Today, 2007 57(7): 30–36. ISSN 0018-2753 Fulltext: Ebsco.
K. Kumar, The Making of English National Identity (Cambridge: Cambridge University Press, 2003), p. 183.
D. Howell, British Workers and the Independent Labour Party, 1888–1906 (Manchester: Manchester University Press, 1984), p. 144.
J. F. MacKenzie, "The second city of the Empire: Glasgow – imperial municipality", in F. Driver and D. Gilbert, eds, Imperial Cities: Landscape, Display and Identity (2003), pp. 215–23.
J. Shields, Clyde Built: a History of Ship-Building on the River Clyde (1949).
C. H. Lee, Scotland and the United Kingdom: the Economy and the Union in the Twentieth Century (1995), p. 43.
M. Magnusson (10 November 2003), "Review of James Buchan, Capital of the Mind: how Edinburgh Changed the World", New Statesman, archived from the original on 29 May 2011
E. Wills, Scottish Firsts: a Celebration of Innovation and Achievement (Edinbugh: Mainstream, 2002).
K. S. Whetter (2008), Understanding Genre and Medieval Romance, Ashgate, p. 28
N. Davidson (2000), The Origins of Scottish Nationhood, Pluto Press, p. 136
"Cultural Profile: 19th and early 20th century developments", Visiting Arts: Scotland: Cultural Profile, archived from the original on 5 November 2011
Stephan Tschudi-Madsen, The Art Nouveau Style: a Comprehensive Guide (Courier Dover, 2002), pp. 283–4.
J. L. Roberts, The Jacobite Wars, pp. 193–5.
M. Sievers, The Highland Myth as an Invented Tradition of 18th and 19th century and Its Significance for the Image of Scotland (GRIN Verlag, 2007), pp. 22–5.
P. Morère, Scotland and France in the Enlightenment (Bucknell University Press, 2004), pp. 75–6.
William Ferguson, The identity of the Scottish Nation: an Historic Quest (Edinburgh: Edinburgh University Press, 1998), p. 227.
Divine, Scottish Nation pp. 292–95.
M. Gray, The Highland Economy, 1750–1850 (Greenwood, 1976).
E. Richards, The Highland Clearances: People, Landlords and Rural Turmoil (2008).
J. Wormald, Scotland: a History (2005), p. 229.
A. K. Cairncross, The Scottish Economy: A Statistical Account of Scottish Life by Members of the Staff of Glasgow University (Glasgow: Glasgow University Press, 1953), p. 10.
R. A. Houston and W. W. Knox, eds, The New Penguin History of Scotland (Penguin, 2001), p. xxxii.
G. Robb, "Popular Religion and the Christianization of the Scottish Highlands in the Eighteenth and Nineteenth Centuries", Journal of Religious History, 1990, 16(1): 18–34.
J. T. Koch, Celtic Culture: a Historical Encyclopedia, Volumes 1–5 (ABC-CLIO, 2006), pp. 416–7.
T. M. Devine, The Scottish Nation, pp. 91–100.
Paul L. Robertson, "The Development of an Urban University: Glasgow, 1860–1914", History of Education Quarterly, Winter 1990, vol. 30 (1), pp. 47–78.
M. F. Rayner-Canham and G. Rayner-Canham, Chemistry was Their Life: Pioneering British Women Chemists, 1880–1949, (Imperial College Press, 2008), p. 264.
Richard J. Finlay, Modern Scotland 1914–2000 (2006), pp 1–33
R. A. Houston and W.W. J. Knox, eds. The New Penguin History of Scotland (2001) p 426.[1] Niall Ferguson points out in "The Pity of War" that the proportion of enlisted Scots who died was third highest in the war behind Serbia and Turkey and a much higher proportion than in other parts of the UK.[2][3]
Iain McLean, The Legend of Red Clydeside (1983)
Finlay, Modern Scotland 1914–2000 (2006), pp 34–72
Richard J. Finlay, "National identity in Crisis: Politicians, Intellectuals and the 'End of Scotland', 1920–1939," History, June 1994, Vol. 79 Issue 256, pp 242–59
http://www.bbc.co.uk/schools/primaryhistory/world_war2/scotlands_blitz/
http://www.bbc.co.uk/scotland/landscapes/clydebank_blitz/
J. Leasor Rudolf Hess: The Uninvited Envoy (Kelly Bray: House of Stratus, 2001), ISBN 0-7551-0041-7, p. 15.
Evans 2008, p. 168.
Sereny 1996, p. 240.
P. Wykeham, Fighter Command (Manchester: Ayer, rpt., 1979), ISBN 0-405-12209-8, p. 87.
J. Buchanan, Scotland (Langenscheidt, 3rd edn., 2003), ISBN 981-234-950-2, p. 51.
J. Creswell, Sea Warfare 1939–1945 (Berkeley, University of California Press, 2nd edn., 1967), p. 52.
D. Howarth, The Shetland Bus: A WWII Epic of Escape, Survival, and Adventure (Guilford, DE: Lyons Press, 2008), ISBN 1-59921-321-4.
T. M. Devine, The Scottish Nation, 1700–2000 (London: Penguin Books, 2001), ISBN 0-14-100234-4, pp. 549–50.
Harvie, Christopher No Gods and Precious Few Heroes (Edward Arnold, 1989) pp 54–63.
Stewart, Heather (6 May 2007). "Celtic Tiger Burns Brighter at Holyrood". The Guardian.
"National Planning Framework for Scotland". Retrieved 17 September 2014.
Torrance, David (30 March 2009). "Modern myth of a poll tax test-bed lives on". The Scotsman. Retrieved 19 September 2017.
"The poll tax in Scotland 20 years on". BBC News. BBC. 1 April 2009. Retrieved 17 September 2014.
"The Scotland Act 1998" Office of Public Sector Information. Retrieved 22 April 2008.
"Devolution > Scottish responsibilities" Scottish Government publication, (web-page last updated November 2010)
http://news.bbc.co.uk/1/hi/special_report/1999/06/99/scottish_parliament_opening/382490.stm
http://www.independent.co.uk/news/uk/politics/donald-dewar-dies-after-fall-634695.html
http://news.bbc.co.uk/1/hi/scotland/3719396.stm
https://www.theguardian.com/politics/2011/may/06/scottish-elections-salmond-historic-victory-snp
http://www.bbc.co.uk/news/events/scotland-decides/results
Whitaker's Almanack (1991) London. J. Whitaker and Sons.
North Channel, Encyclopædia Britannica. Retrieved 2 May 2016.
"Uniting the Kingdoms?". Retrieved 17 September 2014.
See "Centre of Scotland" Newtonmore.com. Retrieved 7 September 2012.
Keay, J. & Keay, J. (1994) Collins Encyclopaedia of Scotland. London. HarperCollins. Pages 734 and 930.
"Tay". Encarta. Archived from the original on 17 May 2008. Retrieved 21 March 2008.
"Southern Uplands". Tiscali.co.uk. 16 November 1990. Archived from the original on 28 November 2004. Retrieved 11 June 2009.
"Education Scotland – Standard Grade Bitesize Revision – Ask a Teacher – Geography – Physical – Question From PN". BBC. Retrieved 11 June 2009.
"Scotland Today " ITKT". Intheknowtraveler.com. 28 December 2006. Archived from the original on 6 January 2007. Retrieved 11 June 2009.
Murray, W.H. (1973) The Islands of Western Scotland. London. Eyre Methuen ISBN 978-0-413-30380-6
Murray, W.H. (1968) The Companion Guide to the West Highlands of Scotland. London. Collins. ISBN 0-00-211135-7
Johnstone, Scott et al. (1990) The Corbetts and Other Scottish Hills. Edinburgh. Scottish Mountaineering Trust. Page 9.
"BBC Weather: UK Records". BBC.co.uk. Archived from the original on 2 December 2010. Retrieved 21 September 2007. The same temperature was also recorded in Braemar on 10 January 1982 and at Altnaharra, Highland, on 30 December 1995.
"Weather extremes". Met Office. Retrieved 23 March 2017.
"Western Scotland: climate". Archived from the original on 8 October 2014. Retrieved 17 September 2014.
"Eastern Scotland: climate". Archived from the original on 8 October 2014. Retrieved 17 September 2014.
"Scottish Weather Part One". BBC. Archived from the original on 26 January 2011. Retrieved 21 September 2007.
Fraser Darling, F. & Boyd, J.M. (1969) Natural History in the Highlands and Islands. London. Bloomsbury.
Benvie, Neil (2004) Scotland's Wildlife. London. Aurum Press. ISBN 1-85410-978-2 p. 12.
"State of the Park Report. Chapter 2: Natural Resources"(pdf) (2006) Cairngorms National Park Authority. Retrieved 14 October 2007.
Preston, C.D., Pearman, D.A., & Dines, T.D. (2002) New Atlas of the British and Irish Flora. Oxford University Press.
Gooders, J. (1994) Field Guide to the Birds of Britain and Ireland. London. Kingfisher.
Matthews, L.H. (1968) British Mammals. London. Bloomsbury.
WM Adams (2003). Future nature:a vision for conservation. p. 30. ISBN 978-1-85383-998-6. Retrieved 10 January 2011.
"East Scotland Sea Eagles" RSPB. Retrieved 3 January 2014.
Ross, John (29 December 2006). "Mass slaughter of the red kites". The Scotsman. Edinburgh, UK.
Ross, David (26 November 2009) "Wild Boar: our new eco warriors" The Herald. Glasgow.
"Beavers return after 400-year gap". BBC News. 29 May 2009. Retrieved 5 December 2009.
Integrated Upland Management for Wildlife, Field Sports, Agriculture & Public Enjoyment (pdf) (September 1999) Scottish Natural Heritage. Retrieved 14 October 2007.
"The Fortingall Yew". Archived from the original on 6 December 2008. Retrieved 17 September 2014.
"Scotland remains home to Britain's tallest tree as Dughall Mor reaches new heights". Forestry Commission. Archived from the original on 3 October 2012. Retrieved 26 April 2008.
Copping, Jasper (4 June 2011) "Britain's record-breaking trees identified" London. The Telegraph. Retrieved 10 July 2011.
"Why Scotland has so many mosses and liverworts". Retrieved 17 September 2014.
"Bryology (mosses, liverworts and hornworts)". Retrieved 17 September 2014.
"Scotland's Census 2011 - National Records of Scotland Table KS201SC - Ethnic group - Release 3A". National Records for Scotland. 2014-02-27. Retrieved 2014-02-27.[dead link]
"Scotland's Population at its Highest Ever". National Records of Scotland. 30 April 2015. Retrieved 12 February 2015.
Census 2011: Detailed characteristics on Ethnicity, Identity, Language and Religion in Scotland – Release 3A. Scotland Census 2011. Retrieved 20 September 2014.
"Did You Know?—Scotland's Cities". Retrieved 17 September 2014.
Clapperton, C.M. (ed) (1983) Scotland: A New Study. London. David & Charles.
Miller, J. (2004) Inverness. Edinburgh. Birlinn. ISBN 978-1-84158-296-2
"New Towns". Retrieved 17 September 2014.
"Scotland speaks Urdu". Retrieved 17 September 2014.
The Pole Position (6 August 2005). Glasgow. Sunday Herald newspaper.
Gaelic Language Plan, www.gov.scot. Retrieved 2 October 2014.
Scots Language Policy, www.gov.scot. Retrieved 2 October 2014.
Stuart-Smith J. Scottish English: Phonology in Varieties of English: The British Isles, Kortman & Upton (Eds), Mouton de Gruyter, New York 2008. p.47
Stuart-Smith J. Scottish English: Phonology in Varieties of English: The British Isles, Kortman & Upton (Eds), Mouton de Gruyter, New York 2008. p.48
Macafee C. Scots in Encyclopedia of Language and Linguistics, Vol. 11, Elsevier, Oxford, 2005. p.33
"Scotland's Census 2011". National Records of Scotland. Retrieved 27 May 2014.
Kenneth MacKinnon. "A Century on the Census—Gaelic in Twentieth Century Focus". University of Glasgow. Archived from the original on 5 September 2007. Retrieved 26 September 2007.
"Can TV's evolution ignite a Gaelic revolution?". The Scotsman. 16 September 2008.
The US Census 2000. The [4] American Community Survey 2004 by the US Census Bureau estimates 5,752,571 people claiming Scottish ancestry and 5,323,888 people claiming Scotch-Irish ancestry. "Archived copy". Archived from the original on 8 January 2012. Retrieved 5 February 2016.
"The Scotch-Irish". American Heritage Magazine. 22 (1). December 1970. Archived from the original on 20 October 2010.
"Born Fighting: How the Scots-Irish Shaped America". Powells.com. 12 August 2009. Retrieved 30 April 2010.
"Scots-Irish By Alister McReynolds, writer and lecturer in Ulster-Scots studies". Nitakeacloserlook.gov.uk. Archived from the original on 16 February 2009. Retrieved 30 April 2010.
"2006 Canadian Census". 2 April 2008. Retrieved 17 September 2014.
Linguistic Archaeology: The Scottish Input to New Zealand English Phonology Trudgill et al. Journal of English Linguistics.2003; 31: 103–124
"Scotland's population reaches record of high of 5.25 million". The Courier. 3 August 2012. Retrieved 3 January 2014.
"Scotland's Population 2011: The Registrar General's Annual Review of Demographic Trends 157th Edition". Gro-gov.scot. Retrieved 1 May 2013.
"Table Q1: Births, stillbirths, deaths, marriages and civil partnerships, numbers and rates, Scotland, quarterly, 2002 to 2012" (PDF). General Register Office for Scotland. Retrieved 1 May 2013.
http://www.scotlandscensus.gov.uk/ods-analyser/jsf/tableView/crosstabTableView.xhtml 2011 Census population data for localities in Scotland. Retrieved 10 July 2014.
Life Expectancy for Areas within Scotland 2012-2014 (PDF) (Report). National Records of Scotland. 13 October 2015. p. 5. Retrieved 22 March 2017.
"Scotland's Census 2011" (PDF). National Records of Scotland. Retrieved 11 August 2016.
"Church of Scotland ‘struggling to stay alive’". scotsman.com.
"Survey indicates 1.5 million Scots identify with Church". www.churchofscotland.org.uk. Retrieved 29 September 2016.
Andrew Collier, "Scotland's Confident Catholics," The Tablet 10 January 2009, 16.
"Scottish Episcopal Church could be first in UK to conduct same-sex weddings". Scottish Legal News. 20 May 2016. Retrieved 1 October 2016.
"Analysis of Religion in the 2001 Census". General Register Office for Scotland. Retrieved 26 September 2007.
"In the Scottish Lowlands, Europe's first Buddhist monastery turns 40". Retrieved 17 September 2014.
HC Deb vol 514 cc 199-201, 15 April 1953, Prime Minister Winston Churchill
"Opening of Parliament: Procession of the Crown of Scotland". Scottish Parliament. Retrieved 9 July 2016.
"Government of Scotland Facts". Archived from the original on 3 May 2010. Retrieved 17 September 2014.
"Brown opens door to Holyrood tax powers". Sunday Herald. 16 February 2008. Retrieved 4 January 2014.
Fraser, Douglas (2 February 2016). "Scotland's tax powers: What it has and what's coming?". BBC News. BBC. Retrieved 27 April 2017.
BBC Scotland News Online "Scotland begins pub smoking ban", BBC Scotland News, 26 March 2006. Retrieved 17 July 2006.
"People: Who runs the Scottish Government". Scottish Government. 21 November 2014. Retrieved 11 January 2015.
"Deputy First Minister". Gov.scot. 24 May 2016. Retrieved 11 August 2016.
"The Scottish Government". Beta.gov.scot. Retrieved 11 August 2016.
"General election 2017: SNP lose a third of seats amid Tory surge". BBC News. BBC. 9 June 2017. Retrieved 20 June 2017.
"Election 2015: SNP wins 56 of 59 seats in Scots landslide". BBC News. BBC. Retrieved 17 May 2015.
"Scotland Office Charter". Scotland Office website. 9 August 2004. Archived from the original on 30 October 2007. Retrieved 22 December 2007.
https://www.gov.uk/guidance/devolution-of-powers-to-scotland-wales-and-northern-ireland
http://www.gov.scot/topics/archive/About-Archive/11556
http://www.bbc.co.uk/news/uk-scotland-scotland-politics-38670128
https://www.independent.co.uk/news/uk/politics/brexit-latest-devolved-government-scotland-northern-ireland-wales-eu-negotiation-talks-article-50-a7552421.html
http://www.parliament.scot/visitandlearn/Education/18642.aspx
http://www.gov.scot/Topics/International
https://beta.gov.scot/about/who-runs-government/cabinet-and-ministers/cabinet-secretary-culture-tourism-external-affairs/
https://beta.gov.scot/about/who-runs-government/cabinet-and-ministers/minister-international-development-europe/
http://news.bbc.co.uk/1/hi/scotland/4646129.stm
https://www.scotland-malawipartnership.org/who-we-are/about-us/
http://news.bbc.co.uk/1/hi/world/europe/3018692.stm
http://www.bbc.co.uk/news/uk-scotland-scotland-politics-24787204
http://www.gov.scot/Topics/International/Asia/china-1-1/visitchina2011
http://www.gov.scot/Topics/International/Americas/north-america/canadaplan
http://www.gov.scot/Topics/International/Americas/north-america/sao
http://www.bbc.co.uk/news/uk-scotland-scotland-politics-39485807
http://www.dailyrecord.co.uk/news/scottish-news/nicola-sturgeon-nets-63million-deal-10146762
https://firstminister.gov.scot/first-minister-in-dublin-day-2/
Cavanagh, Michael (2001) The Campaigns for a Scottish Parliament. University of Strathclyde. Retrieved 12 April 2008.
"Party people confront new realities". BBC News. BBC. Retrieved 18 January 2008.
"Commons clears transfer of power". The Herald. Glasgow. January 2011. Retrieved 4 October 2011.
"Referendum Bill". Official website, About > Programme for Government > 2009–10 > Summaries of Bills > Referendum Bill. Scottish Government. 2 September 2009. Archived from the original on 10 September 2009. Retrieved 10 September 2009.
MacLeod, Angus (3 September 2009). "Salmond to push ahead with referendum Bill". The Times. London. Archived from the original on 10 September 2009. Retrieved 10 September 2009.
"Scottish independence plan 'an election issue'". BBC News. 6 September 2010.
Black, Andrew (21 March 2013). "Scottish independence: Referendum to be held on 18 September, 2014". London: BBC News. Retrieved 21 March 2013.
"Scotland votes no: the union has survived, but the questions for the left are profound". The Guardian. 19 September 2014.
Indyref. "Scotland decides". BBC. Retrieved 19 September 2014.
Scottish Independence Referendum: statement by the Prime Minister, UK Government
Scottish referendum: Who is Lord Smith of Kelvin?, BBC News
"Scottish Leader Nicola Sturgeon Announces Plans for Second Independence Referendum". Time. 24 June 2016. Retrieved 24 June 2016.
"Brexit: Nicola Sturgeon says second Scottish independence vote 'highly likely'". BBC News. 24 June 2016. Retrieved 24 June 2016.
"Local Government etc. (Scotland) Act 1994" Archived 1 March 2010 at the Wayback Machine. Office of Public Sector Information. Retrieved 26 September 2007.
"City status". Retrieved 17 September 2014.
"UK Cities". Retrieved 17 September 2014.
"History of the Faculty of Law". The University of Edinburgh School of Law. Archived from the original on 22 November 2007. Retrieved 22 October 2007.
The Articles: legal and miscellaneous, UK Parliament House of Lords (2007). "Article 19: The Scottish legal system and its courts was to remain unchanged":"Act of Union 1707". House of Lords. Archived from the original on 14 November 2007. Retrieved 22 October 2007.
"Law and institutions, Gaelic" & "Law and lawyers" in M. Lynch (ed.), The Oxford Companion to Scottish History, (Oxford, 2001), pp. 381–382 & 382–386. Udal Law remains relevant to land law in Orkney and Shetland: "A General History of Scots Law (20th century)" (PDF). Law Society of Scotland. Archived from the original (PDF) on 25 September 2007. Retrieved 20 September 2007.
"Court Information" www.scotcourts.gov.uk. Retrieved 26 September 207. Archived 20 March 2015 at the Wayback Machine.
"The case for keeping 'not proven' verdict". Retrieved 17 September 2014.
"Scotland's unique 15-strong juries will not be abolished". The Scotsman. 11 May 2009. Retrieved 13 March 2017.
"Prisoner Population". Sps.gov.uk. Retrieved 8 July 2009.
Highlands and Islands Medical Service (HIMS) www.60yearsofnhsscotland.co.uk. Retrieved 28 July 2008.
"Cabinet and ministers - gov.scot". beta.gov.scot. Retrieved 11 August 2017.
"Strategic Board of the Scottish Government". Scottish Government. Retrieved 8 June 2014.
"About the NHS in Scotland". Archived from the original on 28 June 2014. Retrieved 17 September 2014.
http://www.telegraph.co.uk/finance/economics/11084406/The-Scottish-economy-in-ten-essential-charts.html
Centre for Economics & Business Research. "How money in some regions subsidises others". Archived from the original on 12 October 2013. Retrieved 31 July 2013.
"Government Expenditure & Revenue Scotland 2012–13". p. 4. Retrieved 12 March 2014.
Johnson, Simon (12 March 2014) "Scots Each Receive £1,300 More Spending Despite Oil Tax Drop". The Daily Telegraph.
Scottish Government. "Scotland's Balance Sheet" (PDF). Retrieved 12 June 2013.
"Scotland's GDP 2016 Q4" (5 April 2017). Scottish Government.
BBC. "Scottish economic output falls by 0.2%". Retrieved 7 April 2017.
Scottish Office. "Scottish Labour Market Statistics September 2015". Retrieved 15 January 2016.
Askeland, Erikka (20 March 2012) "Scots Cities Slide down Chart of the World's Top Financial Centres". The Scotsman.
"The Global Financial Centres Index 19". Long Finance. March 2016.
Scottish Government. "Export Statistics Scotland – Publication". Retrieved 14 December 2014.
"Economy Statistics". The Scottish Government. Retrieved 26 May 2014.
Macalister, Terry (2 March 2012). "Who would get the oil revenues if Scotland became independent?". The Guardian. Retrieved 14 October 2012.
"Scotch Whisky Exports Hit Record Level". Scotch Whisky Association. 2 April 2013. Retrieved 12 June 2013.
"Scotch Whisky Exports Remain Flat". BBC News. Retrieved 17 September 2014.
"Scotch Whisky Briefing 2014". Scotch Whisky Association. Retrieved 30 May 2014.
Carrell, Severin; Griffiths, Ian; Terry Macalister, Terry (29 May 2014). "New Doubt Cast over Alex Salmond's Claims of Scottish Wealth". The Guardian. Retrieved 30 May 2014.
"The Economics of Tourism" (PDF). SPICe. 2002. Archived from the original (PDF) on 6 November 2005. Retrieved 22 October 2007.
"Scottish Banknotes: The Treasury's Symbolic Hostage in the Independence Debate". The Guardian. Retrieved 26 May 2014.
The large number of military bases in Scotland led some to use the euphemism "Fortress Scotland". See Spaven, Malcolm (1983) Fortress Scotland. London. Pluto Press in association with Scottish CND.
"Pensioner, 94, in nuclear protest". Retrieved 17 September 2014.
"Reprieve for RAF Lossiemouth base". Retrieved 17 September 2014.
"Dunoon and the US Navy". Retrieved 17 September 2014.
"A Guide to Education and Training in Scotland – "the broad education long regarded as characteristic of Scotland"". Scottish Government. Retrieved 18 October 2007.
P. J. Bawcutt and J. H. Williams, A Companion to Medieval Scottish Poetry (Woodbridge: Brewer, 2006), ISBN 1-84384-096-0, pp. 29–30.
R. A. Houston, Scottish Literacy and the Scottish Identity: Illiteracy and Society in Scotland and Northern England, 1600–1800 (Cambridge: Cambridge University Press, 2002), ISBN 0-521-89088-8, p. 5.
"School education prior to 1873", Scottish Archive Network, 2010, archived from the original on 2 July 2011
R. Anderson, "The history of Scottish Education pre-1980", in T. G. K. Bryce and W. M. Humes, eds, Scottish Education: Post-Devolution (Edinburgh: Edinburgh University Press, 2nd edn., 2003), ISBN 0-7486-1625-X, pp. 219–28.
"Schools and schooling" in M. Lynch (ed.), The Oxford Companion to Scottish History, (Oxford, 2001), pp. 561–563.
"Curriculum for Excellence – Aims, Purposes and Principles". Scottish Government. Archived from the original on 1 August 2010.
"The Scottish Exam System". Archived from the original on 14 February 2008. Retrieved 17 September 2014.
"Welcome to the Carnegie Trust for the Universities of Scotland". Carnegie Trust for the Universities of Scotland. Archived from the original on 11 October 2007. Retrieved 18 October 2007.
"Understanding Scottish Qualifications". Scottish Agricultural College. Archived from the original on 22 May 2012. Retrieved 18 October 2007.
"RAE 2008: results for UK universities". The Guardian. London. 18 December 2008. Retrieved 11 June 2009.
Foster, Patrick. "The Times Good University Guide 2009 – league table". The Times. London. Retrieved 30 April 2010.
"Scotland tops global university rankings". Newsnet Scotland. 11 September 2012. Archived from the original on 9 March 2013. Retrieved 11 January 2013.
"A Framework for Higher Education in Scotland: Higher Education Review Phase 2". Scottish Government. Retrieved 18 October 2007.
"What is higher education?" (PDF). Universities Scotland. Archived from the original (PDF) on 16 March 2004. Retrieved 18 October 2007.
http://www.saas.gov.uk/_forms/fees_student.pdf
"Scottish Government - Graduate endowment scrapped". Retrieved 29 October 2014.
"MSPs vote to scrap endowment fee". BBC News. 2008-02-28. Retrieved 2011-02-12.
Cite error: The named reference NS122015 was invoked but never defined (see the help page).
ITV (5 June 2014). "Scotland 'most highly educated country in Europe'". Retrieved 8 June 2014.
"Tertiary educational attainment, age group 25–64 by sex and NUTS 2 regions". Eurostat. 2014. Retrieved 8 June 2014.
"Best Scottish Band of All Time". The List. Retrieved 2 August 2006.
R. T. Lambdin and L. C. Lambdin, Encyclopedia of Medieval Literature (London: Greenwood, 2000), ISBN 0-313-30054-2, p. 508.
I. Brown, T. Owen Clancy, M. Pittock, S. Manning, eds, The Edinburgh History of Scottish Literature: From Columba to the Union, until 1707 (Edinburgh: Edinburgh University Press, 2007), ISBN 0-7486-1615-2, p. 94.
J. T. Koch, Celtic Culture: a Historical Encyclopedia (ABC-CLIO, 2006), ISBN 1-85109-440-7, p. 999.
E. M. Treharne, Old and Middle English c.890-c.1400: an Anthology (Wiley-Blackwell, 2004), ISBN 1-4051-1313-8, p. 108.
M. Fry, Edinburgh (London: Pan Macmillan, 2011), ISBN 0-330-53997-3.
N. Jayapalan, History of English Literature (Atlantic, 2001), ISBN 81-269-0041-5, p. 23.
J. Wormald, Court, Kirk, and Community: Scotland, 1470–1625 (Edinburgh: Edinburgh University Press, 1991), ISBN 0-7486-0276-3, pp. 60–7.
I. Brown, T. Owen Clancy, M. Pittock, S. Manning, eds, The Edinburgh History of Scottish Literature: From Columba to the Union, until 1707 (Edinburgh: Edinburgh University Press, 2007), ISBN 0-7486-1615-2, pp. 256–7.
R. D. S. Jack, "Poetry under King James VI", in C. Cairns, ed., The History of Scottish Literature (Aberdeen University Press, 1988), vol. 1, ISBN 0-08-037728-9, pp. 137–8.
J. Buchan (2003). Crowded with Genius. Harper Collins. p. 163. ISBN 0-06-055888-1.
L. McIlvanney (Spring 2005). "Hugh Blair, Robert Burns, and the Invention of Scottish Literature". Eighteenth-Century Life. 29 (2): 25–46. doi:10.1215/00982601-29-2-25.
N. Davidson (2000). The Origins of Scottish Nationhood. Pluto Press. p. 136. ISBN 0-7453-1608-5.
"Cultural Profile: 19th and early 20th century developments". Visiting Arts: Scotland: Cultural Profile. Archived from the original on 5 November 2011.
"The Scottish 'Renaissance' and beyond". Visiting Arts: Scotland: Cultural Profile. Archived from the original on 5 November 2011.
"The Scots Makar". The Scottish Government. 16 February 2004. Archived from the original on 5 November 2011. Retrieved 28 October 2007.
"Duffy reacts to new Laureate post". BBC News. 1 May 2009. Archived from the original on 5 November 2011.
Harvey, David; Jones, Rhys; McInroy, Neil; et al., eds. (2002). Celtic geographies: old culture, new times. Stroud, Gloucestershire: Routledge. p. 142. ISBN 978-0-415-22396-6.
Pittock, Murray (1999). Celtic identity and the British image. Manchester: Manchester University Press. pp. 1–5. ISBN 0-7190-5826-0.
"Celtic connections:Scotland's premier winter music festival". Celtic connections website. Celtic Connections. 2010. Retrieved 23 January 2010.
"'Hebridean Celtic Festival 2010 – the biggest homecoming party of the year". Hebridean Celtic Festival website. Hebridean Celtic Festival. 2009. Retrieved 23 January 2010.
"Site Officiel du Festival Interceltique de Lorient". Festival Interceltique de Lorient website. Festival Interceltique de Lorient. 2009. Archived from the original on 5 March 2010. Retrieved 23 January 2010.
"Welcome to the Pan Celtic 2010 Home Page". Pan Celtic Festival 2010 website. Fáilte Ireland. 2010. Retrieved 26 January 2010.
"About the Festival". National Celtic Festival website. National Celtic Festival. 2009. Archived from the original on 19 January 2012. Retrieved 23 January 2010.
"Feature: Saint Andrew seals Scotland's independence" Archived 16 September 2013 at the Wayback Machine., The National Archives of Scotland, 28 November 2007, retrieved 12 September 2009.
"Feature: Saint Andrew seals Scotland's independence". The National Archives of Scotland. 28 November 2007. Archived from the original on 16 September 2013. Retrieved 9 December 2009.
Dickinson, Donaldson, Milne (eds.), A Source Book Of Scottish History, Nelson and Sons Ltd, Edinburgh 1952, p.205
G. Bartram, www.flaginstitute.org British Flags & Emblems Archived 9 November 2012 at the Wayback Machine. (Edinburgh: Tuckwell Press, 2004), ISBN 1-86232-297-X, p. 10.
"National identity" in M. Lynch (ed.), The Oxford Companion to Scottish History, (Oxford, 2001), pp. 437–444.
Keay, J. & Keay, J. (1994) Collins Encyclopaedia of Scotland. London. HarperCollins. Page 936.
"Symbols of Scotland—Index". Retrieved 17 September 2014.
Bain, Robert (1959). Margaret O. MacDougall (ed.), ed. Clans & Tartans of Scotland (revised). P.E. Stewart-Blacker (heralidic advisor), forward by The R. Hon. C/refountess of Erroll. William Collins Sons & Co., Ltd. p. 108.
"Action call over national anthem". BBC News. 21 March 2006. Retrieved 3 November 2011.
"Games team picks new Scots anthem". BBC. 9 January 2010.
"Explanatory Notes to St. Andrew's Day Bank Holiday (Scotland) Act 2007" Office of Public Sector Information. Retrieved 22 September 2007.
"Scottish fact of the week: Scotland's official animal, the Unicorn". Retrieved 17 September 2014.
Gail Kilgore. "The Auld Alliance and its Influence on Scottish Cuisine". Retrieved 29 July 2006.
"Who invented the television? How people reacted to John Logie Baird's creation 90 years ago". The Telegraph. 26 January 2016.
"Newspapers and National Identity in Scotland" (PDF). IFLA University of Stirling. Retrieved 12 December 2006.
"About Us::Celtic Media Festival". Celtic Media Festival website. Celtic Media Festival. 2014. Retrieved 3 January 2014.
Soccer in South Asia: Empire, Nation, Diaspora. By James Mills, Paul Dimeo: Page 18 – Oldest Football Association is England's FA, then Scotland and third oldest is the Indian FA.
Gerhardt, W. "The colourful history of a fascinating game. More than 2000 Years of Football". FIFA. Archived from the original on 10 August 2006. Retrieved 11 August 2006.
"Official site of the Tennents Scottish Cup". The Tennents Scottish Cup. Retrieved 10 December 2006.
Paul Mitchell. "The first international football match". BBC. Retrieved 21 September 2014.
"Scotland is the home of golf". PGA Tour official website. Archived from the original on 28 August 2008. Retrieved 4 December 2008. "Scotland is the home of golf..."
"The Home of Golf". Scottish Government. Retrieved 4 December 2008. "The Royal & Ancient and three public sector agencies are to continue using the Open Championship to promote Scotland as the worldwide home of golf."
Keay (1994) op cit page 839. "In 1834 the Royal and Ancient Golf Club declared St. Andrews 'the Alma Mater of golf'".
Cochrane, Alistair (ed) Science and Golf IV: proceedings of the World Scientific Congress of Golf. Page 849. Routledge.
Forrest L. Richardson (2002). "Routing the Golf Course: The Art & Science That Forms the Golf Journey". p. 46. John Wiley & Sons
The Open Championship – More Scottish than British Archived 2 October 2012 at the Wayback Machine. PGA Tour. Retrieved 23 September 2011
"Medal Tally". Retrieved 17 September 2014.
"Overview and History". Retrieved 17 September 2014.
The Scotsman 27 March 2007. "Special Report—Business Class"
"Highlands and Islands Airports – Airport Information". Retrieved 17 September 2014.
"Prestwick Airport to be nationalised in bid to safeguard jobs". The Herald. 8 October 2013. Retrieved 8 October 2013.
http://www.telegraph.co.uk/travel/destinations/europe/united-kingdom/scotland/articles/scotland-to-get-its-own-national-airline/
"Disaggregating Network Rail's expenditure and revenue allowance and future price control framework: a consultation (June 2005)" Office of Rail Regulation. Retrieved 2 November 2007.
"Rail". www.transport.gov.scot. Transport Scotland. Retrieved 15 December 2016.
Keay, J. & Keay, J. (1994) Collins Encyclopaedia of Scotland. London. HarperCollins.ISBN 0-00-255082-2

    'Extraordinary' month for Scottish renewable energy BBC

Further reading

    Devine, T. M. [1999] (2000). The Scottish Nation 1700–2000 (New Ed. edition). London:Penguin. ISBN 0-14-023004-1
    Donnachie, Ian and George Hewitt. Dictionary of Scottish History. (2001). 384 pp.
    Keay, John, and Julia Keay. Collins Encyclopedia of Scotland (2nd ed. 2001), 1101pp; 4000 articles; emphasis on history
    Koch, J. T. Celtic Culture: a Historical Encyclopedia (ABC-CLIO, 2006), ISBN 1-85109-440-7, 999pp
    Tabraham, Chris, and Colin Baxter. The Illustrated History of Scotland (2004) excerpt and text search
    Trevor-Roper, Hugh, The Invention of Scotland: Myth and History, Yale, 2008, ISBN 0-300-13686-2
    Watson, Fiona, Scotland; From Prehistory to the Present. Tempus, 2003. 286 pp.
    Wilson, Neil. Lonely Planet Scotland (2013) excerpt and text search[dead link]
    Wormald, Jenny, The New History of Scotland (2005) excerpt and text search

Specialized monographs

    Brown, Dauvit, (1999) Anglo-French acculturation and the Irish element in Scottish Identity in Smith, Brendan (ed.), Insular Responses to Medieval European Change, Cambridge University Press, pp. 135–53
    Brown, Michael (2004) The Wars of Scotland, 1214–1371, Edinburgh University Press., pp. 157–254
    Dumville, David N. (2001). "St Cathróe of Metz and the Hagiography of Exoticism". Irish Hagiography: Saints and Scholars. Dublin: Four Courts Press. pp. 172–176. ISBN 978-1-85182-486-1.
    Flom, George Tobias. Scandinavian influence on Southern Lowland Scotch. A Contribution to the Study of the Linguistic Relations of English and Scandinavian (Columbia University Press, New York. 1900)
    Herbert, Maire (2000). "Rí Érenn, Rí Alban, kingship and identity in the ninth and tenth centuries". In Simon Taylor (ed.). Kings, Clerics and Chronicles in Scotland, 500–1297. Dublin: Four Courts Press. pp. 63–72. ISBN 1-85182-516-9.
    MacLeod, Wilson (2004) Divided Gaels: Gaelic Cultural Identities in Scotland and Ireland: c.1200–1650. Oxford University Press.
    Pope, Robert (ed.), Religion and National Identity: Wales and Scotland, c.1700–2000 (University of Wales Press, 2001)
    Sharp, L. W. The Expansion of the English Language in Scotland, (Cambridge University PhD thesis, 1927), pp. 102–325;

External links
	Wikimedia Commons has media related to Scotland.
	Wikivoyage has a travel guide for Scotland.
Find more about
Scotland
at Wikipedia's sister projects

    Definitions from Wiktionary
    Media from Commons
    News from Wikinews
    Quotations from Wikiquote
    Texts from Wikisource
    Textbooks from Wikibooks
    Travel guide from Wikivoyage
    Learning resources from Wikiversity
    Data from Wikidata

    Visit Scotland, official site of Scotland's national tourist board.
    Maps and digital collections at the National Library of Scotland.
    National Archives of Scotland, official site of the National Archives of Scotland.
    Scotland at DMOZ
    Scottish Census Results On Line, official government site for Scotland's census results.
    Scottish economic statistics from the Scottish Government.
    Scottish Government, official site of the Scottish Government.
    Scotland.org, the official online gateway to Scotland managed by the Scottish Government.
    Scottish Parliament, official site of the Scottish Parliament.
    ScotlandsPeople, official government resource for Scottish genealogy.
    statistics.gov.scot, open access to a range of official statistics about Scotland including small area statistics.
    Gazetteer for Scotland, an extensive guide to the places and people of Scotland by the Royal Scottish Geographical Society and University of Edinburgh.
    Streets of Scotland, photos from Scotland's streets.
    Geographic data related to Scotland at OpenStreetMap

[show]
Links to related articles
Authority control 	

    WorldCat Identities VIAF: 258766271 LCCN: n79123936 GND: 4053233-1 NDL: 00571670 


	
This is a good article. Click here for more information.
V. Gordon Childe
From Wikipedia, the free encyclopedia
  (Redirected from Vere Gordon Childe)
V. Gordon Childe
Gordon Childe.jpg
Vere Gordon Childe in the 1930s
Born 	Vere Gordon Childe
14 April 1892
Sydney, New South Wales, Australia
Died 	19 October 1957 (aged 65)
Blackheath, New South Wales, Australia
Alma mater 	

University of Sydney
Oxford University
Occupation 	Archaeologist
Philologist
Known for 	Excavating Skara Brae
Marxist archaeological theory

Vere Gordon Childe (14 April 1892 – 19 October 1957), better known as V. Gordon Childe, was an Australian archaeologist and philologist who specialized in the study of European prehistory. Working most of his life as an academic in the United Kingdom for the University of Edinburgh and then the Institute of Archaeology, London, he wrote twenty-six books and was an early proponent of culture-historical archaeology and Marxist archaeology.

Born in Sydney, New South Wales to a middle-class family of English descent, Childe studied Classics at the University of Sydney before moving to England to study Classical archaeology at the University of Oxford. Here, he embraced the socialist movement and campaigned against the First World War, viewing it as a conflict waged by competing imperialists to the detriment of Europe's working class. Returning to Australia in 1917, he was prevented from working in academia because of his socialist activism, instead working for the Australian Labor Party as the private secretary of politician John Storey. Growing critical of Labor, he authored an analysis of their policies and joined the far-left Industrial Workers of the World. Emigrating to London in 1921, he became librarian of the Royal Anthropological Institute and continued his research into European prehistory through various journeys across the continent, publishing his findings in academic papers and books. In doing so he introduced the continental European concept of an archaeological culture to the British archaeological community.

From 1927 through to 1946 he worked as the Abercromby Professor of Archaeology at the University of Edinburgh, and then from 1947 to 1957 as director of the Institute of Archaeology, London. During this period he oversaw excavation of a number of archaeological sites in Scotland and Northern Ireland, focusing in particular on the society of Neolithic Orkney by excavating the settlement of Skara Brae and the chambered tombs of Maeshowe and Quoyness. Throughout, he continued to publish prolifically, producing excavation reports, journal articles, and books. With Stuart Piggott and Grahame Clark he co-founded The Prehistoric Society in 1934, becoming its first president. Remaining committed to his socialist ideals, he embraced Marxism, and used Marxist ideas as an interpretative framework for archaeological data. He also became a noted sympathiser with the Soviet Union and visited the country on a number of occasions, although grew sceptical of Soviet foreign policy following the Hungarian Revolution of 1956. His beliefs resulted in him being legally barred from entering the United States, despite being repeatedly invited to lecture there. Upon retirement, he returned to Australia's Blue Mountains, there committing suicide.

Widely regarded as one of the most important archaeologists and prehistorians of his generation, he became known as the "great synthesizer" for his work in synthesizing regional research into a broader picture of Near Eastern and European prehistory. He was also renowned for his emphasis on the role of revolutionary technological and economic developments in human society, such as the Neolithic Revolution and the Urban Revolution, in this manner being influenced by Marxist ideas on societal development.

Contents

    1 Early life
        1.1 Childhood: 1892–1910
        1.2 University in Sydney and Oxford: 1911–1917
        1.3 Early career in Australia: 1918–1921
        1.4 London and early books: 1922–1926
    2 Later life
        2.1 Abercromby Professor of Archaeology: 1927–1946
            2.1.1 Excavations
            2.1.2 Publications
        2.2 Institute of Archaeology, London: 1946–1956
        2.3 Retirement and death: 1957
    3 Archaeological methodology and theory
        3.1 Culture-historical archaeology
        3.2 Marxist archaeology
        3.3 The Neolithic and Urban Revolutions
        3.4 Influence on processual and post-processual archaeology
    4 Personal life
    5 Legacy and influence
        5.1 Academic publications
        5.2 Popular culture
    6 Bibliography
    7 References
        7.1 Footnotes
        7.2 Bibliography
    8 Further reading

Early life
Childhood: 1892–1910

Childe was born on 14 April 1892 in Sydney, New South Wales.[1] He was the only surviving child of the Reverend Stephen Henry (1844–1923) and Harriet Eliza Childe (1853–1910), a middle-class couple of English descent.[2] Stephen Childe was a second-generation Anglican priest, ordained into the Church of England in 1867 after gaining a BA from the University of Cambridge. Becoming a teacher, in 1871 he married Mary Ellen Latchford, together having five children.[3] They moved to Australia in 1878. It was here that Mary died, and in 1886 Stephen married Harriet, an Englishwoman from a wealthy background who had moved to Australia as a child.[4] Gordon Childe was raised alongside five half-siblings at his father's palatial country house, the Chalet Fontenelle, in the township of Wentworth Falls in the Blue Mountains west of Sydney.[5] Reverend Childe worked as the minister for St. Thomas' Parish, but proved unpopular, arguing with his congregation and taking unscheduled holidays.[5]

A sickly child, Gordon Childe was educated at home for a number of years, before gaining a private school education in North Sydney.[6] In 1907, he began attending Sydney Church of England Grammar School, gaining his Junior Matriculation in 1909 and Senior Matriculation in 1910. At school he studied ancient history, French, Greek, Latin, geometry, algebra and trigonometry, achieving good marks in all subjects, but was bullied because of his strange appearance and unathletic physique.[7] In July 1910 his mother died; his father soon took Monica Gardiner as his third wife.[8] Childe's relationship with his father was strained, particularly following his mother's death, and they disagreed on the subject of religion and politics, with the Reverend being a devout Christian and conservative while his son was an atheist and socialist.[8]
University in Sydney and Oxford: 1911–1917

Childe studied for a degree in Classics at the University of Sydney in 1911; although focusing on the study of written sources, he first came across classical archaeology through the works of archaeologists Heinrich Schliemann and Arthur Evans.[9] At university, he became an active member of the Debating Society, at one point arguing in favour of the proposition that "socialism is desirable". Increasingly interested in socialism, he read the works of Marxism's founders Karl Marx and Friedrich Engels, as well as those of philosopher G. W. F. Hegel, whose ideas on dialectics heavily influenced Marxist theory.[10] Also while there, he became a great friend of fellow undergraduate Herbert Vere Evatt, with whom he remained in contact throughout his life.[11] Ending his studies in 1913, Childe graduated the following year with various honours and prizes, including Professor Francis Anderson's prize for Philosophy.[12]
"My Oxford training was in the Classical tradition to which bronzes, terracottas and pottery (at least if painted) were respectable while stone and bone tools were banausic."
— Gordon Childe, 1957.[13]

Wishing to continue his education, he gained a £200 Cooper Graduate Scholarship in Classics, allowing him to afford the tuition fees at Queen's College, a part of the University of Oxford, England. He set sail for Britain aboard the SS Orsova in August 1914, shortly after the outbreak of World War I.[14] At Queen's, Childe was entered for a diploma in classical archaeology followed by a Literae Humaniores degree, although he never completed the former.[15] Whilst there, he studied under John Beazley and Arthur Evans, the latter acting as Childe's supervisor.[16] In 1915, he published his first academic paper, "On the Date and Origin of Minyan Ware", which appeared in the Journal of Hellenic Studies, and the following year produced his B.Litt. thesis, "The Influence of Indo-Europeans in Prehistoric Greece", displaying his interest in combining philological and archaeological evidence.[17]

At Oxford he became actively involved with the socialist movement, antagonising the conservative university authorities. Becoming a noted member of the left-wing reformist Oxford University Fabian Society, then at the height of its power and membership, he was there in 1915 when it changed its name to the Oxford University Socialist Society, following a split from the Fabian Society.[18] His best friend and flatmate was Rajani Palme Dutt, a British citizen born to an Indian father and Swedish mother, who was a fervent socialist and Marxist. The two often got drunk and tested each other's knowledge about classical history late at night.[19] With Britain in the midst of World War I, many socialists refused to fight for the British Army despite the government imposed conscription. They believed that the war was being waged in the interests of the ruling classes of the European imperialist nations at the expense of the working classes, and that class war was the only conflict that they should be concerned with. Dutt was imprisoned for refusing to fight, and Childe campaigned for his release and the release of other socialists and pacifist conscientious objectors. Childe was never required to enlist in the army, most likely because of his poor health and eyesight.[20] The authorities were concerned by his anti-war sentiments; the intelligence agency MI5 opening a file on him, his mail was intercepted, and he was kept under observation.[21]
Early career in Australia: 1918–1921
From 1919 to 1921, Childe worked for the leftist politician John Storey as his personal assistant.

Childe returned to Australia in August 1917,[22] and being a known socialist agitator, he was soon placed under surveillance by the security services, who intercepted all of his mail.[23] In 1918 he took up the post of Senior Resident Tutor at St Andrew's College, Sydney University, getting involved in Sydney's socialist and anti-conscription movement. In Easter 1918 he spoke at the Third Inter-State Peace Conference, an event organised by the Australian Union of Democratic Control for the Avoidance of War, a group opposed to the plans by Prime Minister Billy Hughes and the centre-right Nationalist Party of Australia to introduce conscription. The conference had a prominent socialist emphasis, and its report argued that the best hope for the end to international war was the "abolition of the Capitalist System". News of Childe's participation reached the Principal of St Andrew's College. Under pressure from the university authorities, he forced Childe to resign despite much opposition from staff.[24]

With his good academic reputation, several staff members provided him with work as a tutor in Ancient History in the Department of Tutorial Classes, but he was prevented from doing so by the Chancellor of the University, Sir William Cullen, who feared that Childe would propagate socialism to students.[25] This infringement of Childe's civil rights was condemned in the leftist community, and the issue was brought up in the Parliament of Australia by centre-left politicians William McKell and T.J. Smith.[26] Moving to Maryborough, Queensland, in October 1918 Childe took up employment teaching Latin at the Maryborough Grammar School. Here too his political affiliations became known, and he was subject to an opposition campaign from local conservative groups and the Maryborough Chronicle, resulting in abuse from disobedient pupils. He soon resigned.[27]

Realising that an academic career would be barred from him by the right wing university authorities, Childe turned to getting a job within the leftist movement. In August 1919, he became private secretary and speech writer to politician John Storey, a prominent member of the centre-left Australian Labor Party then in opposition to New South Wales' Nationalist government. Representing the Sydney suburb of Balmain on the New South Wales Legislative Assembly, Storey became state premier in 1920 when Labor achieved an electoral victory there.[28] Working within the Labor Party allowed Childe to gain an "unrivalled grasp of its structure and history", enabling him to write a book on the subject, How Labour Governs (1923). The greater his involvement, the more Childe became critical of Labor, believing that they betrayed their socialist ideals once they gained political power and moved to a centrist, pro-capitalist stance.[29] He joined the Industrial Workers of the World, which in Australia served mostly as a centre of radical labourers within existing unions, and at the time was banned by the government as a political threat.[29] In 1921 Childe was sent to London by Storey, in order to keep the British press updated about developments in New South Wales, but in December Storey died, and a few days later the New South Wales elections restored a Nationalist government under the premiership of George Fuller. Fuller thought Childe's job unnecessary, and in early 1922 terminated his employment.[30]
London and early books: 1922–1926

Unable to find an academic job in Australia, Childe remained in Britain, renting a room in Bloomsbury, Central London, and spending much time studying at the British Museum and the Royal Anthropological Institute library.[31] An active member of the London socialist movement, he associated with leftists at the 1917 Club in Gerrard Street, Soho, and befriended members of the Marxist Communist Party of Great Britain (CPGB), contributing to their publication, Labour Monthly; however he had not yet openly embraced Marxism.[32][33] Having earned a reputation as an excellent prehistorian, he was invited to other parts of Europe in order to study prehistoric artefacts. In 1922 he travelled to Vienna, Austria to examine unpublished material about the painted Neolithic pottery from Schipenitz, Bukovina held in the Prehistoric Department of the Natural History Museum; he published his findings in the 1923 Journal of the Royal Anthropological Institute.[34] Childe used this excursion to visit a number of museums in Czechoslovakia and Hungary, bringing them to the attention of British archaeologists in a 1922 article published in Man.[35] Returning to London, in 1922 Childe became a private secretary for three Members of Parliament, including John Hope Simpson and Frank Gray, both members of the centre-left Liberal Party. Supplementing this income, Childe worked as a translator for the publishers Kegan Paul, Trench, Trübner & Co and occasionally lectured in prehistory at the London School of Economics.[36]
"As the [Australian] Labour Party, starting with a band of inspired Socialists, degenerated into a vast machine for capturing political power, but did not know how to use that political power except for the profit of individuals; so the [ One Big Union ] will, in all likelihood, become just a gigantic apparatus for the glorification of a few bosses. Such is the history of all Labour organizations in Australia, and that is not because they are Australian, but because they are Labour."
— Gordon Childe, How Labour Governs, 1923.[37]

In 1923 his first book, How Labour Governs, was published by the London Labour Company. Examining the Australian Labor Party and its wider connection with the Australian labour movement, it reflect Childe's disillusionment with the party, believing that the politicians that it managed to get elected had abandoned their socialist ideals in favour of personal comfort.[38] Childe's biographer Sally Green noted that How Labour Governs was of particular significance at the time because it was published just as the British Labour Party was emerging as a major player in British politics, threatening the two-party dominance of the Conservatives and Liberals; in 1924 they were elected to power.[39] Childe had planned a sequel expanding on his ideas, but it was never published.[40]

In May 1923 he visited continental Europe, journeying to the museums in Lausanne, Bern and Zürich to study their prehistoric artefact collections; that year he became a member of the Royal Anthropological Institute. In 1925, the Institute offered him one of the only archaeological jobs then available in Britain, and he became their librarian, in doing so cementing connections with scholars across Europe.[41] This job meant that he came into contact with many of Britain's archaeologists, of whom there were relatively few during the 1920s; he developed a great friendship with O. G. S. Crawford, the Archaeological Officer to the Ordnance Survey, influencing the latter's move toward socialism and Marxism.[42]

In 1925, Kegan Paul, Trench, Trübner & Co published Childe's second book, The Dawn of European Civilisation, in which he synthesised the varied data about European prehistory that he had been exploring for many years. An important work, it was released when the few archaeologists across Europe were amateur and focused purely on studying their locality; The Dawn was a rare example that looked at the larger picture across the continent. Its importance was also due to the fact that it introduced the concept of the archaeological culture into Britain from continental scholarship, thereby aiding in the development of culture-historical archaeology.[43] Childe later stated that the book "aimed at distilling from archaeological remains a preliterate substitute for the conventional politico-military history with cultures, instead of statesmen, as actors, and migrations in place of battles."[44] In 1926 he published a successor, The Aryans: A Study of Indo-European Origins, exploring the theory that civilisation diffused northward and westward into Europe from the Near East via an Indo-European linguistic group known as the Aryans; with the ensuing racial use of the term "Aryan" by the German Nazi Party, Childe avoided mention of the book.[45] In these works, Childe accepted a moderate diffusionism, believing that although most cultural traits spread from one society to another, it was possible for the same traits to develop independently in different places, a theory at odds with the hyper-diffusionism of Sir Grafton Elliot Smith.[46]
Later life
Abercromby Professor of Archaeology: 1927–1946
"Because the early Hindus and Persians did really call themselves Aryans, this term was adopted by some nineteenth-century philologists to designate the speakers of the 'parent tongue'. It is now applied scientifically only to the Hindus, Iranian peoples and the rulers of Mitanni whose linguistic ancestors spoke closely related dialects and even worshipped common deities. As used by Nazis and anti-semites generally, the term "Aryan" means as little as the words "Bolshie" and "Red" in the mouths of crusted tories."
— Gordon Childe criticising the Nazi conception of an Aryan race, What Happened in History, 1942.[47]

In 1927, Childe was offered the newly created post of Abercromby Professor of Archaeology at Scotland's University of Edinburgh (at what is now the School of History, Classics and Archaeology), established by deed poll in the bequest of prehistorian Lord John Abercromby. Although sad at leaving London, Childe took the prestigious position, moving to Edinburgh in September 1927.[48] Aged 35, Childe became the "only academic prehistorian in a teaching post in Scotland", and was disliked by many Scottish archaeologists, who viewed him as an outsider with no specialism in Scottish prehistory; this hostility intensified, and he wrote to a friend, remarking that "I live here in an atmosphere of hatred and envy."[49] He nevertheless made friends in Edinburgh, including Sir W. Lindsay Scott, Alexander Curle, J.G. Callender, Walter Grant and Charles Galton Darwin, becoming godfather to the latter's youngest son.[50] Initially lodging at Liberton, he moved into the semi-residential Hotel de Vere in Eglington Crescent.[51]

At Edinburgh University, Childe focused on research, and although reportedly very kind towards his students, had difficulty speaking to large audiences; he organised the BSc degree course so that it began studying the Iron Age, progressing chronologically backward to the Palaeolithic, confusing many students.[52] Founding the Edinburgh League of Prehistorians, he took his more enthusiastic students on excavations and invited guest lecturers to visit.[53] Involving them in experimental archaeology, of which he was an early proponent, in 1937 he performed experiments to understand the vitrification process that had occurred at several Iron Age forts in northern Britain.[54]

Regularly travelling to London to visit friends, one notable comrade was Stuart Piggott, another influential British archaeologist who succeeded Childe as Abercromby Professor at Edinburgh.[55] Another was Grahame Clark, whom Childe befriended and encouraged in his research.[56] The trio were elected onto the committee of the Prehistoric Society of East Anglia. At Clark's suggestion, in 1935 they used their influence to convert it into a nationwide organisation, the Prehistoric Society, of which Childe was elected president.[57] Membership of the group then grew rapidly; in 1935 it had 353 members, and this had increased to 668 in 1938.[58]

Often attending conferences across Europe, Childe became fluent in several languages, and in 1935 first visited the Soviet Union, spending 12 days in Leningrad and Moscow; impressed with the socialist state, he was particularly interested in the role of Soviet archaeology. Returning to Britain, he became a vocal Soviet sympathiser who avidly read the CPGB's Daily Worker, although he was heavily critical of certain Soviet government policies, in particular the Molotov–Ribbentrop Pact with Nazi Germany.[59] His socialist convictions led to an early denunciation of European fascism, and he was outraged by the Nazi co-option of prehistoric archaeology to glorify their own conceptions of an Aryan racial heritage.[60] Supportive of the British government's decision to fight the fascist powers in the Second World War, he made the decision to commit suicide should the Nazis conquer Britain.[61] Though opposing fascist Germany and Italy, he also criticised the imperialist, capitalist governments of the United Kingdom and United States: he often described the latter as being full of "loathsome fascist hyenas".[62] Nevertheless, in summer 1939 he visited the U.S., lecturing at the University of Harvard, University of California, Berkeley, and University of Pennsylvania.[63]
Excavations
Neolithic dwellings at Skara Brae in Orkney, the site excavated by Childe in 1927–30.

Childe's university position meant that he was obliged to undertake archaeological excavations, something he loathed and believed that he did poorly.[64] Students agreed, but recognised his "genius for interpreting evidence".[65] Unlike many contemporaries, he was scrupulous with writing up and publishing his findings, producing almost annual reports for the Proceedings of the Society of Antiquaries of Scotland, unusually ensuring that he acknowledged the help of every digger.[53]

His best known excavation was undertaken from 1928 to 1930 at Skara Brae in the Orkney Islands. Uncovering a well-preserved Neolithic village, in 1931 he published the excavation results in a book titled Skara Brae. He nevertheless made an error of interpretation, erroneously attributing the site to the Iron Age.[66] Getting on particularly well with the locals, it is reported that to them "he was every inch the professor" because of his eccentric appearance and habits.[67] In 1932, Childe, collaborating with anthropologist C. Daryll Forde, excavated two Iron Age hillforts at Earn's Hugh on the Berwickshire coast,[68] while in June 1935 he excavated a promontory fort at Larriban near to Knocksoghey in Northern Ireland.[69] Together with Wallace Thorneycroft, another Fellow of the Society of Antiquaries of Scotland, Childe excavated two vitrified Iron Age forts in Scotland, at Finavon, Angus (1933–34) and at Rahoy, Argyllshire (1936–37).[70] In 1938, he and Walter Grant oversaw excavations at the Neolithic settlement of Rinyo; excavation ceased during the Second World War, but resumed in 1946.[71]
Publications

Childe continued writing and publishing books on archaeology, beginning with a series of works following on from The Dawn of European Civilisation and The Aryans by compiling and synthesising data from across Europe. First was The Most Ancient Near East (1928), which assembled information from across Mesopotamia and India, setting a background from which the spread of farming and other technologies into Europe could be understood.[72] This was followed by The Danube in Prehistory (1929) which examined the archaeology along the Danube river, recognising it as the natural boundary dividing the Near East from Europe; Childe believed that it was via the Danube that new technologies travelled westward in prehistory. The book introduced the concept of an archaeological culture to Britain from Germany, revolutionising the theoretical approach of British archaeology.[73]
"We find certain types of remains – pots, implements, ornaments, burial rites, house forms – constantly recurring together. Such a complex of regularly associated traits we shall term a 'cultural group' or just a 'culture'. We assume that such a complex is the material expression of what today would be called a people."
— Gordon Childe, The Danube in Prehistory, 1929.[74]

Childe's next work, The Bronze Age (1930), dealt with the titular Bronze Age in Europe, and displayed his increasing acceptance of Marxist theory in understanding how society functioned and changed. He believed that metal was the first indispensable article of commerce, and that metal-smiths were therefore full-time professionals who lived off the social surplus.[75] Within a matter of years he had followed this up with a string of further works: The Forest Cultures of Northern Europe: A Study in Evolution and Diffusion (1931) and The Continental Affinities of British Neolithic Pottery (1932).

In 1933, Childe travelled to Asia, visiting Iraq – a place he thought "great fun" – and India, which he felt was "detestable" due to the hot weather and extreme poverty. Touring archaeological sites in the two countries, he opined that much of what he had written in The Most Ancient Near East was outdated, going on to produce New Light on the Most Ancient Near East (1935), applying his Marxist-influenced ideas about the economy to his conclusions.[76]

After publishing Prehistory of Scotland (1935), Childe produced one of the defining books of his career, Man Makes Himself (1936). Influenced by Marxist views of history, Childe argued that the usual distinction between (pre-literate) prehistory and (literate) history was a false dichotomy and that human society has progressed through a series of technological, economic and social revolutions. These included the Neolithic Revolution, when hunter-gatherers began settling in permanent farming communities, through to the Urban Revolution, when society progressed from a series of small towns through to the first cities, and right up to more recent times, when the Industrial Revolution changed the nature of production.[77]

With the Second World War's outbreak, Childe was unable to travel across Europe, instead focusing on writing Prehistoric Communities of the British Isles (1940).[78] Childe's pessimism surrounding the war's outcome led him to believe that "European Civilization – Capitalist and Stalinist alike – was irrevocably headed for a Dark Age."[79] In this state of mind he produced a sequel to Man Makes Himself titled What Happened in History (1942), a synthesis of human history from the Palaeolithic through to the fall of the Roman Empire. Although Oxford University Press offered to publish the work, he released it through Penguin Books because they could sell it at a cheaper price, something he believed pivotal in providing knowledge for "the masses."[80] This was followed by two short works, Progress and Archaeology (1944) and The Story of Tools (1944), the latter being explicitly Marxist and written for the Young Communist League.[81]
Institute of Archaeology, London: 1946–1956
The Neolithic passage tomb of Maes Howe on Mainland, Orkney, excavated by Childe from 1954–55.

In 1946, Childe left Edinburgh to take up the position as Director and Professor of European Prehistory at the Institute of Archaeology (IOA) in London. Anxious to return to the capital, he had kept silent over his disapproval of government policies so that he would not be prevented from getting the job.[82] He took up residence at Lawn Road Flats near to Hampstead.[83]

Located in St John's Lodge in the Inner Circle of Regent's Park, the IOA was founded in 1937, largely by archaeologist Mortimer Wheeler, but until 1946 relied primarily upon volunteer lecturers.[84] Childe's relationship with the conservative Wheeler was strained, the latter being intolerant of the shortcomings of others, something Childe made an effort never to be.[85] He was popular among students, who saw him as a kindly eccentric; they commissioned a bust of Childe from Marjorie Maitland Howard. His lecturing was nevertheless considered poor, as he often mumbled and walked into an adjacent room to find something while continuing to talk. He consistently referred to the socialist states of eastern Europe by their full official titles, and called towns by their Slavonic rather than Germanic names, further confusing his students.[86] He was deemed better at giving tutorials and seminars, where he devoted more time to interacting with his students.[87] As Director, Childe was not obliged to excavate, though he did undertake projects at the Orkney Neolithic burial tombs of Quoyness (1951) and Maes Howe (1954–55).[88]

In 1949 he and O.G.S. Crawford resigned as Fellows of the Society of Antiquaries in protest at the election of James Mann to the Presidency following the retirement of Cyril Fox. They believed that Mann, Keeper of the Tower's Armouries at the Tower of London, was a poor choice and that Wheeler, an actual prehistorian, should have won the election.[89] In 1952 a group of British Marxist historians began publishing the periodical Past & Present, with Childe joining the editorial board.[90] He also became a board member for The Modern Quarterly (later The Marxist Quarterly) during the early 1950s, working alongside old friend, the Communist leader, Rajani Palme Dutt, chairman of the board.[91] He authored occasional articles for Palme Dutt's socialist journal, the Labour Monthly, but disagreed with him over the Hungarian Revolution of 1956; Palme Dutt defended the Soviet Union's decision to quash the revolution using military force, but like many western socialists, Childe strongly disagreed. The event made Childe abandon faith in the Soviet leadership, but not in socialism and Marxism.[92] Childe retained a love of the Soviet Union, having visiting on multiple occasions and having been involved with CPBG satellite body the Society for Cultural Relations with the USSR, and serving as President of their National History and Archaeology Section from the early 1950s until his death.[93]

In April 1956, Childe was awarded the Gold Medal of the Society of Antiquaries for his services to archaeology.[94] He was invited to lecture in the U.S. on multiple occasions, by Robert Braidwood, William Duncan Strong, and Leslie White, but was barred from entering the country due to his socialist beliefs.[95] Whilst working at the Institute, Childe continued writing and publishing books dealing with archaeology and prehistory. History (1947) continued his belief that prehistory and literate history must be viewed together, and adopted a Marxist view of history, whilst Prehistoric Migrations (1950) displayed his views on moderate diffusionism.[96] In 1946 he had also published a paper in the Southwestern Journal of Anthropology, titled "Archaeology and Anthropology" which argued that the two disciplines must be used in tandem, something that would be widely accepted in the decades following his death.[97]
Retirement and death: 1957

In the summer of 1956, Childe retired as IOA Director a year prematurely. European archaeology had rapidly expanded during the 1950s, leading to increasing specialisation and making the synthesising that Childe was known for increasingly difficult.[98] That year, the Institute was moving to Gordon Square, Bloomsbury, and Childe wanted to give his successor, W.F. Grimes, a fresh start in the new surroundings.[99] To commemorate his achievements, the Proceedings of the Prehistoric Society published a Festschrift edition on the last day of his Directorship containing contributions from friends and colleagues from all over the world, something that touched Childe deeply.[99] Upon his retirement, he told many friends that he planned to return to Australia, visit his relatives, and then commit suicide; he was terrified of becoming old, senile, and a burden on society, and suspected that he had cancer.[100] Subsequent commentators have suggested that a core reason for his suicidal desires was his loss of faith in Marxism following the Hungarian Revolution and Premier Nikita Khrushchev's denouncement of Joseph Stalin,[101] although Bruce Trigger noted that while Childe was critical of the Soviet Union's foreign policy, he never saw the state and Marxism as "synonymous", thereby dismissing this explanation.[102]
A view of Grose Valley from Govetts Leap, the site where Childe chose to end his life.

Sorting out his affairs, Childe donated most of his library and all of his estate to the Institute.[103] After a holiday visiting archaeological sites in Gibraltar and Spain in February 1957, he sailed to Australia, reaching Sydney on his 65th birthday. Here, the University of Sydney, which had once barred him from working there, awarded him an honorary degree.[104] Travelling around the country for six months, visiting family members and old friends, he was unimpressed by Australian society, believing it reactionary, increasingly suburban and uneducated.[105] Looking into Australian prehistory, he found it a lucrative field for research,[106] and lectured to archaeological and leftist groups on this and other topics, taking to Australian radio to attack academic racism towards Indigenous Australians.[107]

Writing personal letters to many friends,[108] he sent one to Grimes, requesting that it not be opened until 1968. In it, he described how he feared old age, and stated his intention to take his own life, remarking that "Life ends best when one is happy and strong."[109] On 19 October 1957, Childe went to the area of Govett's Leap in Blackheath in the Blue Mountains where he had grown up. Leaving his hat, spectacles, compass, pipe and Mackintosh atop the cliffs, he fell 1000 feet (300 m) to his death.[110] A coroner ruled his death as accidental, although in the 1980s the Grimes letter saw publication, allowing for recognition of his suicide.[111] His remains were cremated at the Northern Suburbs Crematorium, and his name added to a small family plaque in the Crematorium Gardens.[112] Following his death, an "unprecedented" level of tributes and memorials were issued by the archaeological community,[113] all testifying to his status as Europe's "greatest prehistorian and a wonderful human being."[114]
Archaeological methodology and theory
"By far the most important source [of Childe's thinking], especially in the early stages of his career, was the highly developed western European archaeology, which had been established as a scientific discipline for over a century. His research and publications took the form mainly of contributions to the development of that tradition. His thinking was also influenced, however, by ideas that he derived from Soviet archaeology and American anthropology as well as from more remote disciplines. He had a subsidiary interest in philosophy and politics, and was more concerned than were most archaeologists of his time with justifying the social value of archaeology."
— Bruce Trigger, 1980.[115]

Childe has been considered the principal contributor to archaeological methodology in the first part of the 20th century.[116] His theoretical approach blended together Marxism, diffusionism, and functionalism.[117] Childe was critical of the evolutionary archaeology which was dominant during the 19th century. He believed that those archaeologists who adhered to it placed a greater emphasis on artefacts themselves rather than their makers.[118] He recognised flaws in the technological-based three-age system first developed by Danish antiquarian Christian Jürgensen Thomsen, rejecting its evolutionary chronology that divided prehistory into the Stone Age, Bronze Age, and Iron Age by noting that many of the world's societies were still effectively Stone Age in their technology.[119] He nevertheless saw it as a useful model for analysing socio-economic development when combined with a Marxist model.[120] He therefore used technological criteria for dividing up prehistory into three ages, but instead used economic criteria for sub-dividing the Stone Age into the Palaeolithic and Neolithic, rejecting the concept of the Mesolithic as useless.[121]
Culture-historical archaeology

Childe was a proponent of the culture-historical approach to archaeology, coming to be seen as one of its "founders and chief exponents".[122] Culture-historical archaeology revolved around the concept of "culture", which it had adopted from anthropology. This has been seen as "a major turning point in the history of the discipline", allowing archaeologists to look at the past through a spatial dynamic rather than simply a temporal one.[123] Childe adopted the concept of "culture" from German philologist and archaeologist Gustav Kossinna, before using it in three of his books – The Dawn of European Civilisation (1925), The Aryans (1926) and The Most Ancient East (1928) – without defining it.[124] He proceeded to give it a specifically archaeological definition in The Danube in Prehistory (1929).[125] There, he defined a "culture" as being a set of "regularly associated traits" in the material culture – i.e. "pots, implements, ornaments, burial rites, house forms" – that are repeatedly found across a certain area. He stated that in this respect a "culture" was the archaeological equivalent of a "people". Childe's use of the term was non-racial, and he considered a "people" to be a social grouping, not a biological race.[126] He opposed the equation of archaeological cultures with biological races, as various nationalists were doing in Europe at the time, and was a vociferous critic of Nazi uses of archaeology, arguing that the Jewish people were not a distinct biological race but a socio-cultural grouping.[127]

In 1935, he suggested that culture worked as a "living functioning organism", emphasising the adaptive potential of material culture; in this he was influenced by anthropological functionalism.[128] However, by the late 1940s he came to question the utility of "culture" as an archaeological concept, and therefore the validity of the culture-historical approach.[129] McNairn suggested that this was because the term had become popular across the social sciences in reference to all learned modes of behaviour, and not just material culture as Childe had first used it.[130] He accepted that archaeologists defined "cultures" based on a subjective selection of material criteria; this view later came to be widely adopted by archaeologists like Colin Renfrew.[131]
Marxist archaeology
"To me Marxism means effectively a way of approach to and a methodological device for the interpretation of historical and archaeological material and I accept it because and in so far as it works. To the average communist and anti-communist alike... Marxism means a set of dogmas – the words of the master from which as among mediaeval schoolmen, one must deduce truths which the scientist hopes to infer from experiment and observation."
— Gordon Childe, in letter to Rajani Palme Dutt, 1938.[132]

Childe has typically been seen as a Marxist archaeologist, being the first archaeologist in the West to use Marxist theory in his work.[133] McNairn noted that Marxism was "a major intellectual force in Childe's thought",[134] while Trigger stated that Childe identified with Marx's theories "both emotionally and intellectually".[135] Biographer Sally Green noted that Childe's beliefs were "never dogmatic, always idiosyncratic" and "continually changing throughout his life" but that "Marxist views on a model of the past" were accepted by Childe because they offer "a structural analysis of culture in terms of economy, sociology and ideology, and a principle for cultural change through economy."[136] She noted that "Childe's Marxism" often differed from the orthodox Marxism of his contemporaries because he made reference to the original texts of Hegel, Karl Marx and Friedrich Engels rather than later interpretations and because he was selective in his use of their writings.[136] Similarly, McNairn considered Childe's Marxism to be "an individual interpretation" that differed from "popular or orthodox" conceptions of Marxism.[137]

Marxist archaeology had developed in the Soviet Union in 1929, when young archaeologist Vladislav I. Ravdonikas published a report titled "For a Soviet history of material culture". Criticising the archaeological discipline as inherently bourgeois and therefore anti-socialist, it called for the adoption of a pro-socialist, explicitly Marxist approach to archaeology that was a part of the academic reforms instituted under the administration of Premier Joseph Stalin.[138] Although influenced by Soviet archaeology, Childe maintained a sceptical approach to much of it, disapproving of Soviet archaeologists' tendencies to assume their conclusions in advance of analysing the data, something he recognised as being encouraged by the Soviet government.[139]
"The Marxist view of history and prehistory is admittedly material determinist and materialist. But its determinism does not mean mechanism. The Marxist account is in fact termed 'dialectical materialism.' It is deterministic in as much as it assumes that the historical process is not a mere succession of inexplicable or miraculous happenings, but that all the constituent events are interrelated and form an intelligible pattern."
— Gordon Childe, 1979 [1949].[140]

Childe was heavily critical of the "Marrist" trend in Soviet archaeology, based on the theories of Georgian philologist Nicholas Marr, which rejected diffusionism in favour of unilinear evolutionism; instead, Childe saw diffusionism as a key part of historical development.[141] Childe did not publicly air these criticisms of his Soviet colleagues, perhaps so as not to offend his communist friends or to provide support for right-wing archaeologists.[142] Instead, he publicly praised the Soviet system of archaeology and heritage management, contrasting it favourably with that in Britain because it encouraged collaboration rather than competition between archaeologists.[143] After first visiting the country in 1935, he returned in 1945, 1953, and 1956, befriending many Soviet archaeologists, but shortly before his suicide sent a letter to the Soviet archaeological community stating that he was "extremely disappointed" that they had methodologically fallen behind Western Europe and North America.[144]

Other Marxists, such as George Derwent Thomson and Neil Faulkner, have argued that Childe's archaeological work should not correctly be considered Marxist because he failed to take into account the existence of class struggle as an instrument of social change, something which was a core tenet of Marxist thought.[145] While class struggle was not a factor he considered in his archaeological work, Childe did accept that historians and archaeologists typically interpreted the past through their own class interests, and that most of his contemporaries were producing studies with an innate bourgeois agenda.[146] Childe also diverged from orthodox Marxism by not employing dialectics in his methodology.[147] Furthermore, he denied Marxism's ability to predict the future development of human society, and did not consider humanity's development into pure communism to be inevitable, instead opining that society could fossilize or become extinct instead.[148]
The Neolithic and Urban Revolutions

Influenced by Marxism, Childe argued that society experienced widescale changes in relatively short periods of time, citing the Industrial Revolution as a modern example.[149] He first introduced these ideas of "revolutions" in 1935, as part of his functional-economic interpretation of the three-age system. Here, he argued for a "Neolithic Revolution" which initiated the Neolithic era, and also believed that there were others that marked the start of the Bronze Age and the Iron Age.[149] However the following year, in Man Makes Himself, he combined these Bronze and Iron Age Revolutions into a singular "Urban Revolution," which corresponded largely to Lewis H. Morgan's concept of "civilization."[150]

For Childe, the Neolithic Revolution was a radical period in which humans – who had formerly been hunter-gatherers – began cultivating plants and breeding animals for food, allowing for greater control of the food supply and population growth.[151] He believed that the Urban Revolution was largely caused by the development of bronze metallurgy, and in a 1950 paper proposed ten traits that he believed were present in the oldest cities: they were larger than earlier settlements, they contained full-time craft specialists, the surplus was collected together to a god or king, they witnessed monumental architecture, there was an unequal distribution of social surplus, writing was invented, the sciences developed, naturalistic art developed, trade with foreign areas increased, and the state organisation was based on residence rather than kinship.[152] Childe also believed that the Urban Revolution had a negative side, in that it led to increased social stratification into classes and the oppression of the majority by a power elite.[153] Childe's concept of "revolutions" were not universally adopted in archaeology, with many believing that the term "revolution" was misleading because the processes of agricultural and urban development were gradual transformations.[154]
Influence on processual and post-processual archaeology

Through his work, Gordon Childe contributed to two of the major theoretical movements in Anglo-American archaeology, processualism and post-processualism.[155] Prominent processual archaeologist Colin Renfrew described him as "one of the fathers of processual thought" due to his "development of economic and social themes in prehistory",[156] an idea echoed by Marxist archaeologist Neil Faulkner.[157] Trigger argued that Childe's work foreshadowed processual thought in two clear ways; first by emphasising the role of change in societal development, and second by adhering to a strictly materialist view of the past. Both of these arose from Childe's Marxist beliefs.[158] However, most American processualists ignored Childe's work, seeing him as particularist and irrelevant in their search for generalised laws of societal behaviour.[159] In keeping with Marxist thought, Childe did not agree that such generalised laws existed, believing that behaviour was conditioned by socio-economic factors and was not universal.[160]

Peter Ucko, who was one of Childe's successors as director of the Institute of Archaeology, highlighted that in his writings, Childe accepted the subjectivity of archaeological interpretation, something which was in stark contrast to the processualists' insistence that archaeological interpretation could be objective.[161] This acceptance of subjectivity led Trigger to comment that Childe was a "prototypical post-processual archaeologist".[155]
Personal life
The bronze bust of Childe by Marjorie Maitland Howard.[162] It has been kept in the library of the Institute of Archaeology since 1958.[163]

Biographer Sally Green found no evidence that Childe ever had a serious relationship with anyone; she assumed him to be heterosexual because she found no evidence of same-sex attraction.[164] He had many friends of both sexes, although remained "awkward and uncouth, without any social graces".[164] He enjoyed interacting and socialising with students, often inviting them to dine with him, despite finding it difficult relating to other humans.[165] He was shy, and often hid his personal feelings.[166] He could speak a number of European languages, having taught himself in early life when he was travelling across much of the continent.[167]

Childe believed that the study of the past could offer guidance for how humans should act in the present and future.[168] A socialist from his undergraduate days,[169] Childe was an atheist and critic of religion, viewing it as a false consciousness based in superstition.[170] In History (1947) he commented that "Magic is a way of making people believe they are going to get what they want, whereas religion is a system for persuading them that they ought to want what they get."[171] Archaeologist Colin Renfrew noted that Childe was liberal minded on social issues, but thought that although Childe deplored racism, he did not entirely escape the pervasive 19th century view on distinct differences between different races.[172]

Childe was fond of driving cars, enjoying the "feeling of power" he got from them.[173] He often told a story about how he had raced at high speed down Piccadilly, London at 3 o'clock in the morning for the sheer enjoyment of it, only to be pulled over by a policeman for such illegal activity.[174] He loved practical jokes, and allegedly kept a halfpenny in his pocket to trick pickpockets. On another occasion he played a joke on the delegates at a Prehistoric Society conference by lecturing them on a theory that the Neolithic monument of Woodhenge had been constructed as an imitation of Stonehenge by a nouveau riche chieftain. Several audience members failed to realise that he was being tongue in cheek.[175]

Childe's other hobbies included walking in the British hillsides, attending classical music concerts, and playing the card game contract bridge.[174] Fond of poetry, his favourite poet was John Keats, although his favourite poems were William Wordsworth's "Ode to Duty" and Robert Browning's "A Grammarian's Funeral".[174] He was not particularly interested in reading novels but his favourite was D. H. Lawrence's Kangaroo (1923), a book echoing many of Childe's own feelings about Australia.[174] He was a fan of good quality food and drink, and frequented a number of restaurants.[176] Known for his battered, tatty attire, Childe always wore his wide-brimmed black hat, which he had purchased from a hatter in Jermyn Street, central London, as well as a tie, which was usually red, a colour chosen to symbolise his socialist beliefs. He regularly wore a black Mackintosh raincoat, often carrying it over his arm or draped over his shoulders like a cape. In summer he frequently wore shorts with socks, sock suspenders, and large boots.[177]
Legacy and influence

On his death, Childe was praised by his colleague Stuart Piggott as "the greatest prehistorian in Britain and probably the world".[113] The archaeologist Randall H. McGuire later described him as "probably the best known and most cited archaeologist of the twentieth century",[133] an idea echoed by Bruce Trigger,[166] while Barbara McNairn labelled him "one of the most outstanding and influential figures in the discipline".[178] The archaeologist Brian Fagan described his books as "simple, well-written narratives" which became "archaeological canon between the 1930s and early 1960s".[56] By 1956, he was cited as the most translated Australian author in history, having seen his books published in such languages as Chinese, Czech, Dutch, French, German, Hindi, Hungarian, Italian, Japanese, Polish, Russian, Spanish, Sweden and Turkish.[113] The archaeologists David Lewis-Williams and David Pearce considered Childe to be "probably the most written about" archaeologist in history, commenting that his books were still "required reading" for those in the discipline in 2005.[179]
"The most original and useful contributions that I may have made to prehistory are certainly not novel data rescued by brilliant excavation from the soil or by patient research from dusty museum cases, nor yet well founded chronological schemes nor freshly defined cultures, but rather interpretative concepts and methods of explanation."
— Gordon Childe, 1958.[13]

Childe is primarily respected for developing a synthesis of European and Near Eastern prehistory at a time when most archaeologists were focused on regional sites and sequences,[180] gaining the moniker of "the Great Synthesizer".[156] Since his death, this framework has been heavily revised following the discovery of radiocarbon dating,[181] while his interpretations have been "largely rejected".[182] Various archaeologists have debated and disagreed over the importance of various different parts of Childe's work.[183] Childe himself believed that his primary contribution to archaeology was in his interpretative frameworks, an analysis supported by Alison Ravetz and Peter Gathercole.[183] Childe's theoretical work had been largely ignored in his lifetime,[184] and remained forgotten in the decades after his death, although would see a resurgence in the late 1990s and early 2000s.[185] It remained best known in Latin America, where Marxism remained a core theoretical current in the archaeological community throughout the latter 20th century.[186]

Despite his global influence, Childe's oeuvre was poorly understood in the United States, where his work on European prehistory had never become well known.[187] As a result, in the U.S. he erroneously gained the reputation of being a Near Eastern specialist, where he was regarded by anthropologists as one of the founders of neo-evolutionism, alongside Julian Steward and Leslie White,[188] despite the fact that his approach was "more subtle and nuanced" than theirs.[189] Nevertheless, Bruce Trigger believed that it was an American archaeologist, Robert McCormick Adams, Jr., who did the most to develop Childe's "most innovative ideas" after the latter's death.[185]
Academic publications
"While he may not have provided answers that modern archaeologists find satisfactory, [Childe] challenged colleagues of his own and succeeding decades by constructing a vision of archaeology that was as broad as that of other social sciences, but which also took account of the particular strengths and limitations of archaeological data."
— Bruce Trigger, 1994[155]

Following his death, various articles were published that examined Childe's work from a historical perspective.[113] In 1980, Bruce Trigger published Gordon Childe: Revolutions in Archaeology, which studied the influences that extended over Childe's archaeological thought.[190] That year, Barbara McNairn published The Method and Theory of V. Gordon Childe, examining his methodological and theoretical approaches to the discipline.[191] The following year, Sally Green's Prehistorian: A Biography of V. Gordon Childe, was published, in which she described him as "the most eminent and influential scholar of European prehistory in the twentieth century".[192] Peter Gathercole thought the work of Trigger, McNairn and Green to have been "extremely important",[155] while Ruth Tringham considered them part of a "let's-get-to-know-Childe-better" movement, expressing her opinion that they were all worth reading.[193]

In July 1986, a colloquium devoted to Childe's work was held in Mexico City, marking the 50th anniversary of Man Makes Himself's publication.[186] In May 1992, a conference marking his centenary was held at the UCL Institute of Archaeology in London, co-sponsored by the Institute and the Prehistoric Society, both organisations that he had formerly headed.[163] The proceedings of the conference were subsequently published in a 1994 volume edited by Institute director David R. Harris, The Archaeology of V. Gordon Childe: Contemporary Perspectives. Harris stated that the book was designed to "demonstrate the dynamic qualities of Childe's thought, the breadth and depth of his scholarship, and the continuing relevance of his work to contemporary issues in archaeology."[194] In 1995, another anthology based on a conference was published. Titled Childe and Australia: Archaeology, Politics and Ideas, it was edited by Peter Gathercole, T.H. Irving, and Gregory Melleuish.[195] Further papers would appear on the subject of Childe in ensuing years, looking at such subjects as his personal correspondences,[196] and final resting place.[197]
Popular culture

Childe is referenced in the American blockbuster film Indiana Jones and the Kingdom of the Crystal Skull (2008). Directed by Steven Spielberg and George Lucas, the motion picture was the fourth film in the Indiana Jones series that dealt with the eponymous fictional archaeologist and university professor. In the film, Jones is heard advising one of his students that to understand the concept of diffusion he must read the works of Childe.[198]
Bibliography
Title 	Year 	Publisher
How Labour Governs: A Study of Workers' Representation in Australia 	1923 	The Labour Publishing Company (London)
The Dawn of European Civilization 	1925 	Kegan Paul (London)
The Aryans: A Study of Indo-European Origins 	1926 	Kegan Paul (London)
The Most Ancient East: The Oriental Prelude to European Prehistory 	1929 	Kegan Paul (London)
The Danube in Prehistory 	1929 	Oxford University Press (Oxford)
The Bronze Age 	1930 	Cambridge University Press (Cambridge)
Skara Brae: A Pictish Village in Orkney 	1931 	Kegan Paul (London)
The Forest Cultures of Northern Europe: A Study in Evolution and Diffusion 	1931 	Royal Anthropological Institute of Great Britain and Ireland (London)
The Continental Affinities of British Neolithic Pottery 	1932 	Royal Anthropological Institute of Great Britain and Ireland (London)
New Light on the Most Ancient East: The Oriental Prelude to European Prehistory 	1935 	Kegal Paul (London)
The Prehistory of Scotland 	1935 	Kegan Paul (London)
Man Makes Himself 	1936, slightly revised 1941, 1951 	Watts (London)
Prehistoric Communities of the British Isles 	1940, second edition 1947 	Chambers (London)
What Happened in History 	1942 	Penguin Books (Harmondsworth)
The Story of Tools 	1944 	Cobbett (London)
Progress and Archaeology 	1944 	Watts (London)
History 	1947 	Cobbett (London)
Social Worlds of Knowledge 	1949 	Oxford University Press (London)
Prehistoric Migrations in Europe 	1950 	Aschehaug (Oslo)
Magic, Craftsmanship and Science 	1950 	Liverpool University Press (Liverpool)
Social Evolution 	1951 	Schuman (New York)
Illustrated Guide to Ancient Monuments: Vol. VI Scotland 	1952 	Her Majesty's Stationery Office (London)
Society and Knowledge: The Growth of Human Traditions 	1956 	Harper (New York)
Piecing Together the Past: The Interpretation of Archeological Data 	1956 	Routledge and Kegan Paul (London)
A Short Introduction to Archaeology 	1956 	Muller (London)
The Prehistory of European Society 	1958 	Penguin (Harmondsworth)
References
Footnotes

Trigger 1980, p. 9; Green 1981, p. 1.
Green 1981, p. 1.
Trigger 1980, p. 32; Green 1981, pp. 3–4.
Trigger 1980, p. 32; Green 1981, p. 4.
Green 1981, p. 5.
Green 1981, p. 7.
Trigger 1980, p. 32; Green 1981, p. 8.
Green 1981, pp. 8–9.
Trigger 1980, p. 32; Green 1981, p. 9.
Green 1981, p. 10.
Mulvaney 1994, p. 56.
Trigger 1980, p. 9, 32; Green 1981, pp. 9-11.
Childe 1958, p. 69.
Trigger 1980, p. 32; Green 1981, pp. 12-13; Champion 2009, pp. 12–13, 19–20.
Trigger 1980, p. 9; Green 1981, pp. 14-15; Champion 2009, p. 20.
Trigger 1980, p. 9; Green 1981, pp. 14-15.
Trigger 1980, p. 33; Green 1981, pp. 17-18; Champion 2009, pp. 20, 21.
Trigger 1980, pp. 9, 33; Green 1981, pp. 18-19.
Trigger 1980, p. 33; Green 1981, pp. 21-22.
Green 1981, pp. 22–24; Champion 2009, pp. 26–27.
Champion 2009, pp. 27–28.
Trigger 1980, p. 33; Green 1981, p. 26-27; Mulvaney 1994, p. 57.
Mulvaney 1994, p. 57.
Green 1981, pp. 27–28; Mulvaney 1994, p. 59.
Green 1981, pp. 29–30; Mulvaney 1994, p. 61.
Mulvaney 1994, p. 61.
Trigger 1980, p. 33; Green 1981, p. 26-27; Mulvaney 1994, p. 63; Evans 1995, pp. 7–15.
Trigger 1980, p. 34; Green 1981, pp. 31-35; Mulvaney 1994, p. 66.
Green 1981, p. 35-36.
Trigger 1980, p. 34; Green 1981, pp. 37-40; Mulvaney 1994, p. 55.
Trigger 1980, p. 9; Green 1981, p. 40.
Green 1981, p. 41.
Playford 1963.
Childe 1923.
Green 1981, pp. 43–44.
Green 1981, pp. 47–48.
Childe 1964, p. 181.
Trigger 1980, p. 34; Green 1981, pp. 46–47.
Green 1981, pp. 46-47.
Irving 1995, pp. 82–94.
Trigger 1980, pp. 35–36; Green 1981, pp. 48–49.
Green 1981, pp. 49–50; Hauser 2008, pp. 110, 172.
Trigger 1980, pp. 37–40; Green 1981, pp. 50–51; Trigger 2007, pp. 242–245.
Childe 1958, p. 70.
Trigger 1980, p. 37-40; McNairn 1980, pp. 12–14.
Trigger 1980, p. 44-49; McNairn 1980, p. 7; Green 1981, pp. 52–53.
Childe 1942, p. 150.
Trigger 1980, pp. 60-61; Green 1981, pp. 56–57; Richards 1995.
Green 1981, pp. 58–59.
Green 1981, pp. 59–60.
Green 1981, pp. 72–73.
Green 1981, pp. 60–61.
Green 1981, p. 67.
Green 1981, pp. 62–63.
Green 1981, pp. 73–74.
Fagan 2001, p. 178.
Green 1981, pp. 93–94; Fagan 2001, pp. 62–63.
Fagan 2001, p. 63.
Green 1981, pp. 76–77; Trigger 1994, pp. 17, 20; Klein 1994, p. 76.
Trigger 1994, p. 17; Green 1981, pp. 85–86.
Green 1981, p. 87; Pearce 1995, p. 131.
Green 1981, p. 86.
Pearce 1995, pp. 130, 132.
Green 1981, p. 64.
Green 1981, p. 66.
Green 1981, pp. 68–71; Richards 1995, p. 119–122.
Green 1981, p. 69.
Green 1981, pp. 66–67.
Green 1981, p. 68.
Green 1981, p. 63.
Richards 1995, pp. 123–125.
Trigger 1980, pp. 61–67; McNairn 1980, pp. 21–24; Green 1981, p. 90.
Trigger 1980, pp. 56–60; Green 1981, pp. 90–91.
Childe 1929, pp. v–vi.
Trigger 1980, pp. 67–74; McNairn 1980, pp. 24–26; Green 1981, p. 92.
McNairn 1980, pp. 26–27; Green 1981, p. 93.
Trigger 1980, pp. 83–87, 104–110; McNairn 1980, pp. 27–30; Green 1981, pp. 96–97.
Trigger 1980, pp. 83–87; Green 1981, p. 97.
Green 1981, pp. 97–98.
Trigger 1980, pp. 110–114; McNairn 1980, pp. 33–38; Green 1981, pp. 97–98.
Trigger 1980, pp. 114–117, 151; Green 1981, pp. 99–100.
Trigger 1980, p. 125; Green 1981, p. 105.
Green 1981, pp. 117–118.
Green 1981, p. 106.
Green 1981.
Green 1981, pp. 110–111.
Green 1981, p. 113.
Green 1981, p. 112; Richards 1995, p. 125.
Green 1981, p. 118..
Green 1981, pp. 119–120; Pearce 1995, p. 141.
Green 1981, pp. 119–120.
Green 1981, p. 121.
Trigger 1980, pp. 124–125; Green 1981, pp. 122–123.
Green 1981. pp. 142–143.
Pearce 1995, p. 133.
Trigger 1980, pp. 154–155; Green 1981, pp. 127 and 130.
Green 1981, p. 129.
Trigger 1980, p. 166; Green 1981, p. 126.
Green 1981, p. 142.
Green 1981, pp. 143–144.
Trigger 1980, pp. 166–167; Faulkner 2007, p. 115.
Trigger 1980, p. 167.
Green 1981, p. 144; Barton 2000, p. 769.
Trigger 1980, p. 166; Green 1981, pp. 145–146.
Green 1981, p. 147.
Green 1981, p. 149.
Green 1981, pp. 150–151.
Green 1981, pp. 151–152.
Green 1981, pp. 152–154.
Trigger 1980, p. 166; Green 1981, pp. 154.
Green 1981, p. 154; Barton 2000, p. 769.
Barton 2000, pp. 769–770.
Trigger 1980, p. 11.
Tringham 1983, p. 85.
Trigger 1980, pp. 12–13.
McNairn 1980, p. 2.
McNairn 1980, p. 166.
Trigger 2007, p. 247.
McNairn 1980, pp. 77–78.
McNairn 1980, pp. 78–79.
McNairn 1980, pp. 81–82.
Trigger 1994, pp. 11, 24.
McNairn 1980, pp. 47–48.
McNairn 1980, pp. 48–49.
McNairn 1980, p. 46.
McNairn 1980, p. 50; Harris 1994, p. 3.
McNairn 1980, pp. 49–51.
McNairn 1980, p. 53.
McNairn 1980, p. 59; Harris 1994, p. 4.
McNairn 1980, p. 59.
McNairn 1980, pp. 60–61.
Gathercole 1995, p. 97.
McGuire 1992, p. 69.
McNairn 1980, p. 150.
Trigger 1980, p. 169.
Green 1981, p. 79.
McNairn 1980, p. 66.
Trigger 2007, pp. 326–340.
Trigger 1980, p. 177; Trigger 1994, p. 18.
Childe 1979, p. 93.
Trigger 1994, p. 18; McNairn 1980, pp. 157, 166.
Trigger 1994, pp. 19, 31–32.
McNairn 1980, p. 164.
Klein 1994, p. 76, 80–87.
Thomson 1949; Trigger 1980, p. 175; Faulkner 2007, pp. 97–101.
McNairn 1980, p. 160.
McNairn 1980, p. 134.
McNairn 1980, pp. 127, 159; Trigger 1994, p. 21.
McNairn 1980, p. 91.
McNairn 1980, pp. 91–92.
McNairn 1980, pp. 92–95.
McNairn 1980, pp. 98–102.
McNairn 1980, p. 103.
Maddock 1995, p. 114.
Trigger 1994, p. 24.
Renfrew 1994, p. 123.
Faulkner 2007, p. 100.
Trigger 1980, p. 181.
Tringham 1983, p. 93.
Tringham 1983, p. 94.
Ucko 1990, p. xiii.
"From the archives: Women of the early institute", Elizabeth Pye, Archaeology International, No. 18 (2015), pp. 131-133.
Harris 1994, p. vii.
Green 1981, p. 20.
Green 1981, p. 72.
Trigger 1994, p. 9.
Green 1981, pp. 124–125.
Rowlands 1994, p. 35.
Trigger 1994, p. 17.
McNairn 1980, p. 117; Trigger 1994, p. 22.
Childe 1947, p. 37.
Renfrew 1994, p. 130.
Trigger 1980, p. 18; Green 1981, p. 72.
Green 1981, p. 73.
Trigger 1980, p. 18; Green 1981, pp. 114–115.
Green 1981, p. 117.
Trigger 1980, p. 18; Green 1981, p. 76.
McNairn 1980, p. 1.
Lewis-Williams & Pearce 2005, p. 19.
Harris 1994, p. 1.
Tringham 1983, p. 87; Harris 1994, p. 2.
Trigger 1994, p. 10.
Trigger 1980, p. 13.
McNairn 1980, p. 3; Tringham 1983, p. 86.
Trigger 2007, pp. 352–353.
Flannery 1994, p. 102.
Flannery 1994, p. 101.
Trigger 1980, pp. 10–11; Harris 1994, p. 2.
Trigger 1994, p. 19.
Trigger 1980, p. 12.
McNairn 1980.
Green 1981, p. xix.
Tringham 1983, p. 87.
Harris 1994, p. 6.
Gathercole, Irving & Melleuish 1995.
Stevenson 2011.
Barton 2000.

    Rose 2008.

Bibliography

    Barton, Huw (2000). "In Memoriam V. Gordon Childe". Antiquity. 74: 769–770. doi:10.1017/S0003598X00060361.

    Champion, Timothy (2009). "Childe and Oxford". European Journal of Archaeology. 12: 11–33. doi:10.1177/1461957109339689.

    Childe, V. Gordon (1923). "Schipenitz: A Late Neolithic Station with Painted Pottery in Bukovina". Journal of the Royal Anthropological Institute. 53: 263–288. JSTOR 2843571. doi:10.2307/2843571.

     ———  (1929). The Danube in Prehistory. Oxford: Clarendon Press.

     ———  (1942). What Happened in History. Harmondsworth and New York: Penguin Books Ltd.

     ———  (1947). History. Cobbett Press. OCLC 613254303.

     ———  (1950). "The Urban Revolution". The Town Planning Review. 21 (1): 3–17. JSTOR 40102108. doi:10.3828/tpr.21.1.k853061t614q42qh.

     ———  (1958). "Retrospect". Antiquity. 32. pp. 69–74. doi:10.1017/S0003598X0003845X.

     ———  (1964) [1923]. How Labour Governs. Melbourne: Melbourne University Press.

     ———  (1979) [1949]. "Prehistory and Marxism". Antiquity. 53: 93–95. doi:10.1017/S0003598X00042265.

    Evans, Raymond (1995). ""Social Passion": Vere Gordon Childe in Queensland, 1918–19". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 1–26. ISBN 0-7022-2613-0.

    Faulkner, Neil (2007). "Gordon Childe and Marxist Archaeology". International Socialism. 116. London: Socialist Worker's Party. pp. 81–106.

    Fagan, Brian (2001). Grahame Clark: An Intellectual Biography of an Archaeologist. Boulder, CO: Westview Press. ISBN 0-8133-3602-3.

    Flannery, Kent V. (1994). "Childe the Evolutionist: A Perspective from Nuclear America". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 101–119. ISBN 1-85728-220-5.

    Gathercole, Peter (1995). "The Relationship Between Vere Gordon Childe's Political and Academic Thought – and Practice". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 95–107. ISBN 0-7022-2613-0.

    Gathercole, Peter; Irving, T.H. and Melleuish, Gregory, eds. (1995). Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. ISBN 0-7022-2613-0.

    Green, Sally (1981). Prehistorian: A Biography of V. Gordon Childe. Bradford-on-Avon, Wiltshire: Moonraker Press. ISBN 978-0239002068.

    Greene, Kevin (1999). "V. Gordon Childe and the Vocabulary of Revolutionary Change". Antiquity. 73: 97–109. doi:10.1017/S0003598X00087871.

    Harris, David R., eds. (1994). The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. ISBN 1-85728-220-5.

     ———  (1994). "Preface". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. vii–ix. ISBN 1-85728-220-5.

     ———  (1994). "Introduction". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 1–7. ISBN 1-85728-220-5.

    Hauser, Kitty (2008). Bloody Old Britain: O.G.S. Crawford and the Archaeology of Modern Life. London: Granta. ISBN 978-1847080776.

    Klein, Leo S. (1994). "Childe and Soviet Archaeology: A Romance". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 75–93. ISBN 1-85728-220-5.

    Irving, T.H. (1995). "On the Work of Labour Governments: Vere Gordon Childe's Plans for Volume Two of How Labour Governs". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 82–94. ISBN 0-7022-2613-0.

    Lewis-Williams, David; Pearce, David (2005). Inside the Neolithic Mind: Consciousness, Cosmos and the Realm of the Gods. London: Thames and Hudson. ISBN 978-0500051382.

    Maddock, Kenneth (1995). "Prehistory, Power and Pessimism". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 107–117. ISBN 0-7022-2613-0.

    McGuire, Randall G. (1992). A Marxist Archaeology. San Diego: Academic Press Inc. ISBN 978-0124840782.

    McNairn, Barbara (1980). The Method and Theory of V. Gordon Childe. Edinburgh: Edinburgh University. ISBN 978-0852243893.

    Mulvaney, John (1994). ""Another University Man Gone Wrong": V. Gordon Childe 1892–1922". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 55–73. ISBN 1-85728-220-5.

    Pearce, William J. (1988). "Vere Gordon Childe and American Anthropology". Journal of Anthropological Research. 44. pp. 417–433. JSTOR 3630507.

     ———  (1995). "Vere Gordon Childe and the Cold War". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 128–45. ISBN 0-7022-2613-0.
    Playford, J. D. (1963). "Labour Monthly (London), 1921-1962". Labour History. Australian Society for the Study of Labour History (5): 57–59. JSTOR 27507733. doi:10.2307/27507733.

    Renfrew, Colin (1987). Archaeology and Language: The Puzzle of Indo-European Origins. Jonathan Cape Ltd. ISBN 978-0224024952.

     ———  (1994). "Concluding Remarks: Childe and the Study of Culture Process". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 121–133. ISBN 1-85728-220-5.

    Renfrew, Colin; Bahn, Paul (2004). Archaeology: Theories, Methods and Practice (fourth ed.). London: Thames and Hudson. ISBN 978-0500284414.

    Richards, Colin (1995). "Vere Gordon Childe at Skara Brae and Rinyo: Research and Redemption". In Gathercole, Peter; Irving, T.H., and Melleuish, Gregory. Childe and Australia: Archaeology, Politics and Ideas. St Lucia: University of Queensland Press. pp. 118–127. ISBN 0-7022-2613-0.

    Rose, Mark (20 May 2008). "The Man in the Fedora". Archaeology Magazine. Archived from the original on 8 November 2013.

    Rowlands, Michael (1994). "Childe and the Archaeology of Freedom". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 35–54. ISBN 1-85728-220-5.

    Stevenson, Alice (2011). ""Yours (Unusually) Cheerfully, Gordon": Vere Gordon Childe's Letters to R.B.K. Stevenson". Antiquity. 85. pp. 1454–1462. doi:10.1017/S0003598X00062189.

    Thomson, George (1949). "Review of V.G. Childe's History". The Modern Quarterly. 4: 266–269.

    Trigger, Bruce (1980). Gordon Childe: Revolutions in Archaeology. London: Thames & Hudson. ISBN 978-0231050388.

     ———  (1984). "Childe and Soviet Archaeology". Australian Archaeology. 18. pp. 1–16. JSTOR 40286871.

     ———  (1994). "Childe's Relevance to the 1990s". In Harris, David R. The Archaeology of V. Gordon Childe: Contemporary Perspectives. London: UCL Press. pp. 9–34. ISBN 1-85728-220-5.

     ———  (2007). A History of Archaeological Thought (second ed.). New York: Cambridge University Press. ISBN 978-0521600491.

    Tringham, Ruth (1983). "V. Gordon Childe 25 Years After: His Relevance for the Archaeology of the Eighties: A Review Article". Journal of Field Archaeology. 10 (1): 85–100. JSTOR 529750. doi:10.1179/009346983791504381.

    Ucko, Peter (1990). "Foreword". In Peter Gathercole and David Lowenthal. The Politics of the Past. London: Unwin Hyman. pp. ix–xxi. ISBN 978-0415095549.

Further reading

    Allen, Jim (1967). "Aspects of V. Gordon Childe". Labour History. 12: 52–59. JSTOR 27507861.
    Beilharz, Peter (1991). "The Vere Gordon Childe Centenary Conference". Labour History. Australian Society for the Study of Labour History. 60: 108–112. JSTOR 27509051.
    Brothwell, Don (2009). "Childe, His Student, and Archaeological Science: An Epilogue". European Journal of Archaeology. 12. London: Maney. pp. 193–202.
    Coningham, Robin; Manuel, Mark (2009). "Priest-Kings or Puritans? Childe and Willing Subordination in the Indus". European Journal of Archaeology. London: Maney. 12: 167–180. doi:10.1177/1461957109339691.
    Díaz-Andreu, Margarita (2009). "Childe and the International Congresses of Archaeology". European Journal of Archaeology. London: Maney. 12: 91–122. doi:10.1177/1461957109339693.
    Gathercole, Peter (1971). "'Patterns in Prehistory': An Examination of the Later Thinking of V. Gordon Childe". World Archaeology. London: Routledge. 3 (2): 225–232. JSTOR 124074. doi:10.1080/00438243.1969.9979503.
    Gathercole, Peter (2009). "Childe, Marxism, and Knowledge". European Journal of Archaeology. London: Maney. 12: 181–191. doi:10.1177/1461957109339695.
    Goody, Jack (2006). "Gordon Childe, the Urban Revolution, and the Haute Cuisine: An Anthropo-Archaeological View of Modern History". Comparative Studies in Society and History. Cambridge University Press. 48 (3): 508–519. JSTOR 3879435. doi:10.1017/s0010417506000211.
    Harris, David (2009). "'A new Professor of a Somewhat Obscure Subject': V. Gordon Childe at the London Institute of Archaeology, 1946–1956". European Journal of Archaeology. London: Maney. 12: 123–144. doi:10.1177/1461957109339697.
    László, Attila (2009). "The Young Gordon Childe and Transylvanian Archaeology: The Archaeological Correspondence Between Childe and Ferenc László". European Journal of Archaeology. London: Maney. 12: 35–46. doi:10.1177/1461957109339700.
    Ralston, Ian (2009). "Gordon Childe and Scottish Archaeology: The Edinburgh Years 1927–1946". European Journal of Archaeology. London: Maney. 12: 47–90. doi:10.1177/1461957109339702.
    Sherratt, Andrew (1989). "V. Gordon Childe: Archaeology and Intellectual History". Past & Present. Oxford: Oxford University Press. 125: 151–185. JSTOR 650864. doi:10.1093/past/125.1.151.
    Sherratt, Andrew (1990). "Gordon Childe: Paradigms and Patterns in Prehistory". Australian Archaeology. Australian Archaeological Association. 30: 3–13. JSTOR 40286976. doi:10.1080/03122417.1990.11681358.
    Smith, Michael E. (2009). "V. Gordon Childe and the Urban Revolution: An Historical Perspective on a Revolution in Urban Studies". The Town Planning Review. 80. pp. 3–29.

Authority control 	

    WorldCat Identities VIAF: 76388231 LCCN: n79120914 ISNI: 0000 0001 2028 7780 GND: 119233010 SUDOC: 032257058 BNF: cb12332246x (data) BPN: 51367472 NDL: 00435902 BNE: XX848904 IATH: w6p30drw 

Categories:

    1892 births1957 deathsPeople educated at Sydney Church of England Grammar SchoolUniversity of Sydney alumniAlumni of The Queen's College, OxfordAcademics of the University of EdinburghPeople associated with the University of Edinburgh School of History, Classics and ArchaeologyAustralian archaeologistsAustralian socialistsAustralian Marxist historiansTheorists on Western civilizationPrehistoriansIndustrial Workers of the World membersPeople associated with the UCL Institute of ArchaeologySuicides by jumping in AustraliaSuicides in New South Wales



	
Neolithic Revolution
From Wikipedia, the free encyclopedia
This article is about the introduction of agriculture during the Stone Age. For later historical breakthroughs in agriculture, see Agricultural revolution (disambiguation).
A Sumerian harvester's sickle - dated to 3,000 BC

The Neolithic Revolution, Neolithic Demographic Transition, or Agricultural Revolution, was the wide-scale transition of many human cultures from a lifestyle of hunting and gathering to one of agriculture and settlement, making possible an increasingly larger population.[1] These settled communities permitted humans to observe and experiment with plants to learn how they grew and developed.[2] This new knowledge led to the domestication of plants.[2][3]

Archaeological data indicates that the domestication of various types of plants and animals happened in separate locations worldwide, starting in the geological epoch of the Holocene[4] around 12,500 years ago.[5] It was the world's first historically verifiable revolution in agriculture. The Neolithic Revolution greatly narrowed the diversity of foods available, with a switch to agriculture which led to a downturn in human nutrition.[6]

The Neolithic Revolution involved far more than the adoption of a limited set of food-producing techniques. During the next millennia it would transform the small and mobile groups of hunter-gatherers that had hitherto dominated human pre-history into sedentary (non-nomadic) societies based in built-up villages and towns. These societies radically modified their natural environment by means of specialized food-crop cultivation, with activities such as irrigation and deforestation which allowed the production of surplus food.

These developments provided the basis for densely populated settlements, specialization and division of labour, more trade, the development of non-portable art and architecture, centralized administrations and political structures, hierarchical ideologies, depersonalized systems of knowledge (e.g. writing), and property ownership. The earliest known civilization developed in Sumer in southern Mesopotamia (c. 5,500 BP); its emergence also heralded the beginning of the Bronze Age.[7]

The relationship of the above-mentioned Neolithic characteristics to the onset of agriculture, their sequence of emergence, and empirical relation to each other at various Neolithic sites remains the subject of academic debate, and varies from place to place, rather than being the outcome of universal laws of social evolution.[8][9] The Levant saw the earliest developments of the Neolithic Revolution from around 10,000 BC, followed by sites in the wider Fertile Crescent. The Neolithic Revolution "inspired some of the most important developments in human history including the invention of the wheel, the planting of the first cereal crops and the development of cursive script, mathematics, astronomy and agriculture."[10][11]

Contents

    1 Agricultural transition
    2 Domestication of plants
        2.1 In the Fertile Crescent
        2.2 In China
        2.3 In Europe
        2.4 In Africa
        2.5 In the Americas
        2.6 In New Guinea
    3 Domestication of animals
        3.1 Domestication of animals in the Middle East
    4 Consequences
        4.1 Social change
        4.2 Subsequent revolutions
        4.3 Disease
        4.4 Technology
    5 Archaeogenetics
    6 See also
    7 References
    8 Bibliography
    9 External links

Agricultural transition
See also: Ancient grains
Map of the world showing approximate centers of origin of agriculture and its spread in prehistory: the Fertile Crescent (11,000 BP), the Yangtze and Yellow River basins (9,000 BP) and the New Guinea Highlands (9,000–6,000 BP), Central Mexico (5,000–4,000 BP), Northern South America (5,000–4,000 BP), sub-Saharan Africa (5,000–4,000 BP, exact location unknown), eastern North America (4,000–3,000 BP).[12]
Knap of Howar farmstead on a site occupied from 3,700 BC to 2,800 BC

The term Neolithic Revolution was coined in 1923 by V. Gordon Childe to describe the first in a series of agricultural revolutions in Middle Eastern history. The period is described as a "revolution" to denote its importance, and the great significance and degree of change affecting the communities in which new agricultural practices were gradually adopted and refined.

The beginning of this process in different regions has been dated from 10,000 to 8,000 BC in the Fertile Crescent[5][13] and perhaps 8000 BC in the Kuk Early Agricultural Site of Melanesia.[14][15] This transition everywhere seems associated with a change from a largely nomadic hunter-gatherer way of life to a more settled, agrarian-based one, with the inception of the domestication of various plant and animal species—depending on the species locally available, and probably also influenced by local culture. Recent archaeological research suggests that in some regions such as the Southeast Asian peninsula, the transition from hunter-gatherer to agriculturalist was not linear, but region-specific.[16]

There are several competing (but not mutually exclusive) theories as to the factors that drove populations to take up agriculture. The most prominent of these are:

    The Oasis Theory, originally proposed by Raphael Pumpelly in 1908, popularized by V. Gordon Childe in 1928 and summarised in Childe's book Man Makes Himself.[17] This theory maintains that as the climate got drier due to the Atlantic depressions shifting northward, communities contracted to oases where they were forced into close association with animals, which were then domesticated together with planting of seeds. However, today this theory has little support amongst archaeologists because subsequent climate data suggests that the region was getting wetter rather than drier.[18]
    The Hilly Flanks hypothesis, proposed by Robert Braidwood in 1948, suggests that agriculture began in the hilly flanks of the Taurus and Zagros mountains, where the climate was not drier as Childe had believed, and fertile land supported a variety of plants and animals amenable to domestication.[19]
    The Feasting model by Brian Hayden[20] suggests that agriculture was driven by ostentatious displays of power, such as giving feasts, to exert dominance. This required assembling large quantities of food, which drove agricultural technology.
    The Demographic theories proposed by Carl Sauer[21] and adapted by Lewis Binford[22] and Kent Flannery posit an increasingly sedentary population that expanded up to the carrying capacity of the local environment and required more food than could be gathered. Various social and economic factors helped drive the need for food.
    The evolutionary/intentionality theory, developed by David Rindos[23] and others, views agriculture as an evolutionary adaptation of plants and humans. Starting with domestication by protection of wild plants, it led to specialization of location and then full-fledged domestication.
    Peter Richerson, Robert Boyd, and Robert Bettinger[24] make a case for the development of agriculture coinciding with an increasingly stable climate at the beginning of the Holocene. Ronald Wright's book and Massey Lecture Series A Short History of Progress[25] popularized this hypothesis.
    The postulated Younger Dryas impact event, claimed to be in part responsible for megafauna extinction and ending the last glacial period, could have provided circumstances that required the evolution of agricultural societies for humanity to survive.[26] The agrarian revolution itself is a reflection of typical overpopulation by certain species following initial events during extinction eras; this overpopulation itself ultimately propagates the extinction event.
    Leonid Grinin argues that whatever plants were cultivated, the independent invention of agriculture always took place in special natural environments (e.g., South-East Asia). It is supposed that the cultivation of cereals started somewhere in the Near East: in the hills of Palestine or Egypt. So Grinin dates the beginning of the agricultural revolution within the interval 12,000 to 9,000 BP, though in some cases the first cultivated plants or domesticated animals' bones are even of a more ancient age of 14–15 thousand years ago.[27]
    Andrew Moore suggested that the Neolithic Revolution originated over long periods of development in the Levant, possibly beginning during the Epipaleolithic. In "A Reassessment of the Neolithic Revolution", Frank Hole further expanded the relationship between plant and animal domestication. He suggested the events could have occurred independently over different periods of time, in as yet unexplored locations. He noted that no transition site had been found documenting the shift from what he termed immediate and delayed return social systems. He noted that the full range of domesticated animals (goats, sheep, cattle and pigs) were not found until the sixth millennium at Tell Ramad. Hole concluded that "close attention should be paid in future investigations to the western margins of the Euphrates basin, perhaps as far south as the Arabian Peninsula, especially where wadis carrying Pleistocene rainfall runoff flowed."[28]

Domestication of plants
Neolithic grindstone or quern for processing grain

Once agriculture started gaining momentum, around 9000 BC, human activity resulted in the selective breeding of cereal grasses (beginning with emmer, einkorn and barley), and not simply of those that would favour greater caloric returns through larger seeds. Plants with traits such as small seeds or bitter taste would have been seen as undesirable. Plants that rapidly shed their seeds on maturity tended not to be gathered at harvest, therefore not stored and not seeded the following season; years of harvesting selected for strains that retained their edible seeds longer.

Several plant species, the "pioneer crops" or Neolithic founder crops, were identified by Daniel Zohary, who highlighted the importance of the three cereals, and suggested that domestication of flax, peas, chickpeas, bitter vetch and lentils came a little later. Based on analysis of the genes of domesticated plants, he preferred theories of a single, or at most a very small number of domestication events for each taxon that spread in an arc from the Levantine corridor around the Fertile Crescent and later into Europe.[29][30] Gordon Hillman and Stuart Davies carried out experiments with wild wheat varieties to show that the process of domestication would have occurred over a relatively short period of between 20 and 200 years.[31] Some of these pioneering attempts failed at first and crops were abandoned, sometimes to be taken up again and successfully domesticated thousands of years later: rye, tried and abandoned in Neolithic Anatolia, made its way to Europe as weed seeds and was successfully domesticated in Europe, thousands of years after the earliest agriculture.[32] Wild lentils presented a different problem: most of the wild seeds do not germinate in the first year; the first evidence of lentil domestication, breaking dormancy in their first year, was found in the early Neolithic at Jerf el Ahmar (in modern Syria), and quickly spread south to the Netiv HaGdud site in the Jordan Valley.[32] This process of domestication allowed the founder crops to adapt and eventually become larger, more easily harvested, more dependable[clarification needed] in storage and more useful to the human population.
An "Orange slice" sickle blade element with inverse, discontinuous retouch on each side, not denticulated. Found in large quantities at Qaraoun II and often with Heavy Neolithic tools in the flint workshops of the Beqaa Valley in Lebanon. Suggested by James Mellaart to be older than the Pottery Neolithic of Byblos (around 8,400 cal. BP).

Selectively propagated figs, wild barley and wild oats were cultivated at the early Neolithic site of Gilgal I, where in 2006[33] archaeologists found caches of seeds of each in quantities too large to be accounted for even by intensive gathering, at strata datable to c. 11,000 years ago. Some of the plants tried and then abandoned during the Neolithic period in the Ancient Near East, at sites like Gilgal, were later successfully domesticated in other parts of the world.

Once early farmers perfected their agricultural techniques like irrigation, their crops would yield surpluses that needed storage. Most hunter gatherers could not easily store food for long due to their migratory lifestyle, whereas those with a sedentary dwelling could store their surplus grain. Eventually granaries were developed that allowed villages to store their seeds longer. So with more food, the population expanded and communities developed specialized workers and more advanced tools.

The process was not as linear as was once thought, but a more complicated effort, which was undertaken by different human populations in different regions in many different ways.
In the Fertile Crescent
Clay human figurine (Fertility goddess) Tappeh Sarab, Kermanshah ca. 7000–6100 BCE

Early agriculture is believed to have originated and become widespread in Southwest Asia around 10,000–9,000 BP, though earlier individual sites have been identified. The Fertile Crescent region of Southwest Asia is the centre of domestication for three cereals (einkorn wheat, emmer wheat and barley), four legumes (lentil, pea, bitter vetch and chickpea) and flax.[34] The Mediterranean climate consists of a long dry season with a short period of rain, which may have favored small plants with large seeds, like wheat and barley.[citation needed] The Fertile Crescent also had a large area of varied geographical settings and altitudes and this variety may have made agriculture more profitable for former hunter-gatherers in this region in comparison with other areas with a similar climate .[citation needed]

Finds of large quantities of seeds and a grinding stone at the paleolithic site of Ohalo II in the vicinity of the Sea of Galilee, dated to around 19,400 BP has shown some of the earliest evidence for advanced planning of plant food consumption and suggests that humans at Ohalo II processed the grain before consumption.[35][36] Tell Aswad is oldest site of agriculture with domesticated emmer wheat dated to 8800 BC.[37][38] Soon after came hulled, two-row barley found domesticated earliest at Jericho in the Jordan valley and Iraq ed-Dubb in Jordan.[39] Other sites in the Levantine corridor that show the first evidence of agriculture include Wadi Faynan 16 and Netiv Hagdud.[5] Jacques Cauvin noted that the settlers of Aswad did not domesticate on site, but "arrived, perhaps from the neighbouring Anti-Lebanon, already equipped with the seed for planting".[40] The Heavy Neolithic Qaraoun culture has been identified at around fifty sites in Lebanon around the source springs of the River Jordan, however the dating of the culture has never been reliably determined.[41][42]
In China

Northern China appears to have been the domestication center for foxtail millet (Setaria italica) and broomcorn millet (Panicum miliaceum) with evidence of domestication of these species approximately 8,000 years ago.[43] These species were subsequently widely cultivated in the Yellow River basin (7,500 years ago).[43] Rice was domesticated in southern China later on.[43] Soybean was domesticated in northern China 4,500 years ago.[44] Orange and peach also originated in China. They were cultivated around 2500 BC.[45][46]
In Europe
Szentgyörgyvölgy cow – 4500 BC
Tilling with Hungarian Grey cattle

The fertile Carpathian Basin was the place where Europeans survived the Ice Age. The territory between the Danube and the Tisza rivers was a powerhouse of agricultural knowledge.[citation needed]
In Africa
File:Nile-River1.ogvPlay media
Nile River Valley, Egypt

On the African continent, three areas have been identified as independently developing agriculture: the Ethiopian highlands, the Sahel and West Africa.[47] By contrast, Agriculture in the Nile River Valley is thought to have developed from the original Neolithic Revolution in the Fertile Crescent. Many grinding stones are found with the early Egyptian Sebilian and Mechian cultures and evidence has been found of a neolithic domesticated crop-based economy dating around 7,000 BP.[48][49] Unlike the Middle East, this evidence appears as a "false dawn" to agriculture, as the sites were later abandoned, and permanent farming then was delayed until 6,500 BP with the Tasian and Badarian cultures and the arrival of crops and animals from the Near East.

Bananas and plantains, which were first domesticated in Southeast Asia, most likely Papua New Guinea, were re-domesticated in Africa possibly as early as 5,000 years ago. Asian yams and taro were also cultivated in Africa.[47]

The most famous crop domesticated in the Ethiopian highlands is coffee. In addition, khat, ensete, noog, teff and finger millet were also domesticated in the Ethiopian highlands. Crops domesticated in the Sahel region include sorghum and pearl millet. The kola nut was first domesticated in West Africa. Other crops domesticated in West Africa include African rice, yams and the oil palm.[47]

Agriculture spread to Central and Southern Africa in the Bantu expansion during the 1st millennium BC to 1st millennium AD.
In the Americas
Further information: New World Crops, Ancient Pueblo Peoples, Oasisamerica, and Proto-Uto-Aztecan

Maize (corn), beans and squash were among the earliest crops domesticated in Mesoamerica, with maize beginning about 7500 BC, squash, as early as 8000 to 6000 BC and beans by no later than 4000 BC. Potatoes and manioc were domesticated in South America. In what is now the eastern United States, Native Americans domesticated sunflower, sumpweed and goosefoot around 2500 BC. At Guilá Naquitz cave in the Mexican highlands, fragments of maize pollen, bottle gourd and pepo squash were recovered and variously dated between 8000 and 7000 BC. In this area of the world people relied on hunting and gathering for several millennia to come. Sedentary village life based on farming did not develop until the second millennium BC, referred to as the formative period.[50]
In New Guinea

Evidence of drainage ditches at Kuk Swamp on the borders of the Western and Southern Highlands of Papua New Guinea shows evidence of the cultivation of taro and a variety of other crops, dating back to 11,000 BP. Two potentially significant economic species, taro (Colocasia esculenta) and yam (Dioscorea sp.), have been identified dating at least to 10,200 calibrated years before present (cal BP). Further evidence of bananas and sugarcane dates to 6,950 to 6,440 BP. This was at the altitudinal limits of these crops, and it has been suggested that cultivation in more favourable ranges in the lowlands may have been even earlier. CSIRO has found evidence that taro was introduced into the Solomon Islands for human use, from 28,000 years ago, making taro cultivation the earliest crop in the world.[51][52] It seems to have resulted in the spread of the Trans–New Guinea languages from New Guinea east into the Solomon Islands and west into Timor and adjacent areas of Indonesia. This seems to confirm the theories of Carl Sauer who, in "Agricultural Origins and Dispersals", suggested as early as 1952 that this region was a centre of early agriculture.
Domestication of animals
Further information: Domestication

When hunter-gathering began to be replaced by sedentary food production it became more profitable to keep animals close at hand.[citation needed] Therefore, it became necessary to bring animals permanently to their settlements, although in many cases there was a distinction between relatively sedentary farmers and nomadic herders.[53][original research?] The animals' size, temperament, diet, mating patterns, and life span were factors in the desire and success in domesticating animals. Animals that provided milk, such as cows and goats, offered a source of protein that was renewable and therefore quite valuable. The animal’s ability as a worker (for example ploughing or towing), as well as a food source, also had to be taken into account. Besides being a direct source of food, certain animals could provide leather, wool, hides, and fertilizer. Some of the earliest domesticated animals included dogs (East Asia, about 15,000 years ago),[54] sheep, goats, cows, and pigs.
Domestication of animals in the Middle East
Dromedary camel caravan in Algeria

The Middle East served as the source for many animals that could be domesticated, such as sheep, goats and pigs. This area was also the first region to domesticate the dromedary camel. Henri Fleisch discovered and termed the Shepherd Neolithic flint industry from the Bekaa Valley in Lebanon and suggested that it could have been used by the earliest nomadic shepherds. He dated this industry to the Epipaleolithic or Pre-Pottery Neolithic as it is evidently not Paleolithic, Mesolithic or even Pottery Neolithic.[42][55] The presence of these animals gave the region a large advantage in cultural and economic development. As the climate in the Middle East changed and became drier, many of the farmers were forced to leave, taking their domesticated animals with them. It was this massive emigration from the Middle East that would later help distribute these animals to the rest of Afroeurasia. This emigration was mainly on an east-west axis of similar climates, as crops usually have a narrow optimal climatic range outside of which they cannot grow for reasons of light or rain changes. For instance, wheat does not normally grow in tropical climates, just like tropical crops such as bananas do not grow in colder climates. Some authors, like Jared Diamond, have postulated that this East-West axis is the main reason why plant and animal domestication spread so quickly from the Fertile Crescent to the rest of Eurasia and North Africa, while it did not reach through the North-South axis of Africa to reach the Mediterranean climates of South Africa, where temperate crops were successfully imported by ships in the last 500 years.[56] Similarly, the African Zebu of central Africa and the domesticated bovines of the fertile-crescent — separated by the dry sahara desert — were not introduced into each other's region.
Consequences
Social change

Despite the significant technological advance, the Neolithic revolution did not lead immediately to a rapid growth of population. Its benefits appear to have been offset by various adverse effects, mostly diseases and warfare.[57]
World population (estimated) did not rise for a few millennia after the Neolithic revolution.

It has long been taken for granted that the introduction of agriculture had been an unequivocal progress. This is now questioned in view of findings by archaeologists and paleopathologists showing that nutritional standards of Neolithic populations were generally inferior to that of hunter-gatherers, and that their life expectancy may well have been shorter too, in part due to diseases and harder work. Hunter-gatherers must have covered their food needs with about 20 hours work a week, while agriculture required much more and was at least as uncertain. The hunter-gatherers' diet was more varied and balanced than what agriculture later allowed. Average height went down from 5'10" (178 cm) for men and 5'6" (168 cm) for women to 5'5" (165 cm) and 5'1" (155 cm), respectively, and it took until the twentieth century for average human height to come back to the pre-Neolithic Revolution levels.[58] Agriculturalists had more anaemias and vitamin deficiencies, more spinal deformations and more dental pathologies.[59]

However, the decrease in individual nutrition was accompanied by an increase in population.

The traditional view is that agricultural food production supported a denser population, which in turn supported larger sedentary communities, the accumulation of goods and tools, and specialization in diverse forms of new labor. The development of larger societies led to the development of different means of decision making and to governmental organization. Food surpluses made possible the development of a social elite who were not otherwise engaged in agriculture, industry or commerce, but dominated their communities by other means and monopolized decision-making.[60] Jared Diamond (in The World Until Yesterday) identifies the availability of milk and/or cereal grains as permitting mothers to raise both an older (e.g. 3 or 4 year old) child and a younger child concurrently, whereas this was not possible previously. The result is that a population can significantly more-rapidly increase its size than would otherwise be the case, resources permitting.

Recent analyses point out that agriculture also brought about deep social divisions and in particular encouraged inequality between the sexes.[61]
Subsequent revolutions
Domesticated cow being milked in Ancient Egypt

Andrew Sherratt has argued that following upon the Neolithic Revolution was a second phase of discovery that he refers to as the secondary products revolution. Animals, it appears, were first domesticated purely as a source of meat.[62] The Secondary Products Revolution occurred when it was recognised that animals also provided a number of other useful products. These included:

    hides and skins (from undomesticated animals)
    manure for soil conditioning (from all domesticated animals)
    wool (from sheep, llamas, alpacas, and Angora goats)
    milk (from goats, cattle, yaks, sheep, horses and camels)
    traction (from oxen, onagers, donkeys, horses, camels and dogs)
    guarding and herding assistance (dogs)

Sherratt argued that this phase in agricultural development enabled humans to make use of the energy possibilities of their animals in new ways, and permitted permanent intensive subsistence farming and crop production, and the opening up of heavier soils for farming. It also made possible nomadic pastoralism in semi arid areas, along the margins of deserts, and eventually led to the domestication of both the dromedary and Bactrian camel.[62] Overgrazing of these areas, particularly by herds of goats, greatly extended the areal extent of deserts. Living in one spot would have more easily permitted the accrual of personal possessions and an attachment to certain areas of land. From such a position, it is argued[by whom?], prehistoric people were able to stockpile food to survive lean times and trade unwanted surpluses with others. Once trade and a secure food supply were established, populations could grow, and society would have diversified into food producers and artisans, who could afford to develop their trade by virtue of the free time they enjoyed because of a surplus of food. The artisans, in turn, were able to develop technology such as metal weapons. Such relative complexity would have required some form of social organisation to work efficiently, so it is likely that populations that had such organisation, perhaps such as that provided by religion, were better prepared and more successful. In addition, the denser populations could form and support legions of professional soldiers. Also, during this time property ownership became increasingly important to all people. Ultimately, Childe argued that this growing social complexity, all rooted in the original decision to settle, led to a second Urban Revolution in which the first cities were built.[citation needed]
Disease

Throughout the development of sedentary societies, disease spread more rapidly than it had during the time in which hunter-gatherer societies existed. Inadequate sanitary practices and the domestication of animals may explain the rise in deaths and sickness following the Neolithic Revolution, as diseases jumped from the animal to the human population. Some examples of infectious diseases spread from animals to humans are influenza, smallpox, and measles.[63] In concordance with a process of natural selection, the humans who first domesticated the big mammals quickly built up immunities to the diseases as within each generation the individuals with better immunities had better chances of survival. In their approximately 10,000 years of shared proximity with animals, such as cows, Eurasians and Africans became more resistant to those diseases compared with the indigenous populations encountered outside Eurasia and Africa.[64] For instance, the population of most Caribbean and several Pacific Islands have been completely wiped out by diseases. 90% or more of many populations of the Americas were wiped out by European and African diseases before recorded contact with European explorers or colonists. Some cultures like the Inca Empire did have a large domestic mammal, the llama, but llama milk was not drunk, nor did llamas live in a closed space with humans, so the risk of contagion was limited. According to bioarchaeological research, the effects of agriculture on physical and dental health in Southeast Asian rice farming societies from 4000 to 1500 B.P. was not detrimental to the same extent as in other world regions.[65]
Technology

In his book Guns, Germs, and Steel, Jared Diamond argues that Europeans and East Asians benefited from an advantageous geographical location that afforded them a head start in the Neolithic Revolution. Both shared the temperate climate ideal for the first agricultural settings, both were near a number of easily domesticable plant and animal species, and both were safer from attacks of other people than civilizations in the middle part of the Eurasian continent. Being among the first to adopt agriculture and sedentary lifestyles, and neighboring other early agricultural societies with whom they could compete and trade, both Europeans and East Asians were also among the first to benefit from technologies such as firearms and steel swords.[66]
Archaeogenetics

The dispersal of Neolithic culture from the Middle East has recently been associated with the distribution of human genetic markers. In Europe, the spread of the Neolithic culture has been associated with distribution of the E1b1b lineages and Haplogroup J that are thought to have arrived in Europe from North Africa and the Near East respectively.[67][68] In Africa, the spread of farming, and notably the Bantu expansion, is associated with the dispersal of Y-chromosome haplogroup E1b1a from West Africa.[67]
See also

    Anthropocene
    Aşıklı Höyük, in Anatolia
    Natufians, a settled culture preceding agriculture
    Behavioral modernity
    Original affluent society
    Haplogroup G (Y-DNA)
    Haplogroup J2 (Y-DNA)
    Haplogroup K (mtDNA)
    Neolithic tomb
    Surplus product
    Göbekli Tepe
    Mehrgarh, a Neolithic site in Balochistan

References

Jean-Pierre Bocquet-Appel (July 29, 2011). "When the World's Population Took Off: The Springboard of the Neolithic Demographic Transition". Science. 333 (6042): 560–561. Bibcode:2011Sci...333..560B. PMID 21798934. doi:10.1126/science.1208880. Retrieved June 10, 2012.
Pollard, Rosenberg, and Tigor (2015). Worlds together, worlds apart concise edition vol.1. New York: W.W. Norton & Company. p. 23. ISBN 9780393250930.
Compare:Lewin, Roger (2009) [1984]. "35: The origin of agriculture and the first villagers". Human Evolution: An Illustrated Introduction (5 ed.). Malden, Massachusetts: John Wiley & Sons. p. 250. ISBN 9781405156141. Retrieved 2017-08-20. "[...] the Neolithic transition involved increasing sedentism and social complexity, which was usually followed by the gradual adoption of plant and animal domestication. In some cases, however, plant domestication preceded sedentism, particularly in the New World."
"International Stratigraphic Chart". International Commission on Stratigraphy. Archived from the original on 2013-02-12. Retrieved 2012-12-06.
Graeme Barker (2009). The Agricultural Revolution in Prehistory: Why did Foragers become Farmers?. Oxford University Press. ISBN 978-0-19-955995-4.[page needed]
Armelagos, George J. (2014). "Brain Evolution, the Determinates of Food Choice, and the Omnivore's Dilemma". Critical Reviews in Food Science and Nutrition. 54 (10): 1330–1341. ISSN 1040-8398. PMID 24564590. doi:10.1080/10408398.2011.635817.
"Neolithic". Ancient History Encyclopedia. Retrieved 2017-07-21.
"The Slow Birth of Agriculture" Archived 2011-01-01 at the Wayback Machine., Heather Pringle
"Wizard Chemi Shanidar", EMuseum, Minnesota State University
Milton-Edwards, Beverley (May 2003). "Iraq, past, present and future: a thoroughly-modern mandate?". History & Policy. United Kingdom: History & Policy. Retrieved 9 December 2010.
http://www.ancientobjectsformoderndilemmas.com/current-projects.html
Diamond, J.; Bellwood, P. (2003). "Farmers and Their Languages: The First Expansions". Science. 300 (5619): 597–603. Bibcode:2003Sci...300..597D. PMID 12714734. doi:10.1126/science.1078208.
Thissen, L. "Appendix I, The CANeW 14C databases, Anatolia 10,000-5000 cal. BC." in: F. Gérard and L. Thissen (eds.), The Neolithic of Central Anatolia. Internal developments and external relations during the 9th–6th millennia cal BC, Proc. Int. CANeW Round Table, Istanbul 23–24 November 2001, (2002)
Denham, Tim P.; Haberle, S. G.; et al. (2003). "Origins of Agriculture at Kuk Swamp in the Highlands of New Guinea". Science. 301 (5630): 189–193. PMID 12817084. doi:10.1126/science.1085255.
The Kuk Early Agricultural Site
Kealhofer, Lisa (2003). "Looking into the gap: land use and the tropical forests of southern Thailand". Asian Perspectives. 42 (1): 72–95. doi:10.1353/asi.2003.0022.
Gordon Childe (1936). Man Makes Himself. Oxford university press.
Scarre, Chris (2005). "The World Transformed: From Foragers and Farmers to States and Empires" in The Human Past: World Prehistory and the Development of Human Societies (Ed: Chris Scarre). London: Thames and Hudson. Page 188. ISBN 0-500-28531-4
Charles E. Redman (1978). Rise of Civilization: From Early Hunters to Urban Society in the Ancient Near East. San Francisco: Freeman.
Hayden, Brian (1992). "Models of Domestication". In Anne Birgitte Gebauer and T. Douglas Price. Transitions to Agriculture in Prehistory. Madison: Prehistory Press. pp. 11–18.
Sauer, Carl O. (1952). Agricultural origins and dispersals. Cambridge, MA: MIT Press.
Binford, Lewis R. (1968). "Post-Pleistocene Adaptations". In Sally R. Binford and Lewis R. Binford. New Perspectives in Archaeology. Chicago: Aldine Publishing Company. pp. 313–342.
Rindos, David (December 1987). The Origins of Agriculture: An Evolutionary Perspective. Academic Press. ISBN 978-0-12-589281-0.
Richerson, Peter J.; Boyd, Robert; et al. (2001). "Was Agriculture Impossible during the Pleistocene but Mandatory during the Holocene?". American Antiquity. 66 (3): 387–411. JSTOR 2694241. doi:10.2307/2694241.
Wright, Ronald (2004). A Short History of Progress. Anansi. ISBN 0-88784-706-4.
Anderson, David G; Albert C. Goodyear; James Kennett; Allen West (2011). "Multiple lines of evidence for possible Human population decline/settlement reorganization during the early Younger Dryas". Quaternary International. 242 (2): 570–583. doi:10.1016/j.quaint.2011.04.020.
Grinin L.E. Production Revolutions and Periodization of History: A Comparative and Theoretic-mathematical Approach. / Social Evolution & History. Volume 6, Number 2 / September 2007 [1]
Hole, Frank., A Reassessment of the Neolithic Revolution, Paléorient, Volume 10, Issue 10-2, pp. 49-60, 1984.
Zohary, D., The mode of domestication of the founder crops of Southwest Asian agriculture. pp. 142-158 in D. R. Harris (ed.) The Origins and Spread of Agriculture and Pastoralism in Eurasia. UCL Press Ltd, London, 1996
Zohary, D., Monophyletic vs. polyphyletic origin of the crops on which agriculture was founded in the Near East. Genetic Resources and Crop Evolution 46 (2) pp. 133-142
Hillman, G. C. and M. S. Davies., Domestication rate in wild wheats and barley under primitive cultivation: preliminary results and archaeological implications of field measurements of selection coefficient, pp. 124-132 in P. Anderson-Gerfaud (ed.) Préhistoire de l'agriculture: nouvelles approches expérimentales et ethnographiques. Monographie du CRA 6, Éditions Centre Nationale Recherches Scientifiques: Paris, 1992
Weiss, Ehud; Kislev, Mordechai E.; Hartmann, Anat (2006). "Autonomous Cultivation Before Domestication". Science. 312 (5780): 1608–1610. PMID 16778044. doi:10.1126/science.1127235.
"Tamed 11,400 Years Ago, Figs Were Likely First Domesticated Crop".
Brown, T. A.; Jones, M. K.; Powell, W.; Allaby, R. G. (2009). "The complex origins of domesticated crops in the Fertile Crescent". Trends in Ecology & Evolution. 24 (2): 103. doi:10.1016/j.tree.2008.09.008.
Mithen, Steven (2006). After the ice : a global human history, 20.000 - 5.000 BC (1. paperback ed.). Cambridge, Mass.: Harvard Univ. Press. p. 517. ISBN 0-674-01570-3.
Compiled largely with reference to: Weiss, E., Mordechai, E., Simchoni, O., Nadel, D., & Tschauner, H. (2008). Plant-food preparation area on an Upper Paleolithic brush hut floor at Ohalo II, Israel. Journal of Archaeological Science, 35 (8), 2400-2414.
Ozkan, H; Brandolini, A; Schäfer-Pregl, R; Salamini, F (October 2002). "AFLP analysis of a collection of tetraploid wheats indicates the origin of emmer and hard wheat domestication in southeast Turkey". Molecular Biology and Evolution. 19 (10): 1797–801. PMID 12270906. doi:10.1093/oxfordjournals.molbev.a004002.
van Zeist, W. Bakker-Heeres, J.A.H., Archaeobotanical Studies in the Levant 1. Neolithic Sites in the Damascus Basin: Aswad, Ghoraifé, Ramad., Palaeohistoria, 24, 165-256, 1982.
Hopf, Maria., "Jericho plant remains" in Kathleen M. Kenyon and T. A. Holland (eds.) Excavations at Jericho 5, pp. 576-621, British School of Archaeology at Jerusalem, London, 1983.
Jacques Cauvin (27 July 2000). The Birth of the Gods and the Origins of Agriculture, p. 53. Cambridge University Press. ISBN 978-0-521-65135-6. Retrieved 15 August 2012.
E. J. Peltenburg; Alexander Wasse; Council for British Research in the Levant (2004). Maya Haïdar Boustani, Flint workshops of the Southern Beqa' valley (Lebanon): preliminary results from Qar'oun* in Neolithic revolution: new perspectives on southwest Asia in light of recent discoveries on Cyprus. Oxbow Books. ISBN 978-1-84217-132-5. Retrieved 18 January 2012.
L. Copeland; P. Wescombe (1966). Inventory of Stone-Age Sites in Lebanon: North, South and East-Central Lebanon, p. 89. Impr. Catholique. Retrieved 3 March 2011.
Fuller, D. Q. (2007). "Contrasting Patterns in Crop Domestication and Domestication Rates: Recent Archaeobotanical Insights from the Old World". Annals of Botany. 100 (5): 903–924. PMC 2759199 Freely accessible. PMID 17495986. doi:10.1093/aob/mcm048.
Siddiqi, Mohammad Rafiq. Tylenchida: Parasites of Plants and Insects. New York: CABI Pub. 389. p. (2001).
Thacker, Christopher (1985). The history of gardens. Berkeley: University of California Press. p. 57. ISBN 978-0-520-05629-9.
Webber, Herbert John (1967–1989). Chapter I. History and Development of the Citrus Industry Archived 2016-05-23 at the Portuguese Web Archive in ORIGIN OF CITRUS, Vol. 1. University of California
Diamond, Jared (1999). Guns, Germs, and Steel. New York: Norton Press. ISBN 0-393-31755-2.
The Cambridge History of Africa
Smith, Philip E.L., Stone Age Man on the Nile, Scientific American Vol. 235 No. 2, August 1976: "With the benefit of hindsight we can now see that many Late Paleolithic peoples in the Old World were poised on the brink of plant cultivation and animal husbandry as an alternative to the hunter-gatherer's way of life".
Graeme Barker (25 March 2009). The Agricultural Revolution in Prehistory: Why Did Foragers Become Farmers?, p. 252. Oxford University Press. ISBN 978-0-19-955995-4. Retrieved 4 January 2012.
Denham, Tim et al. (received July 2005) "Early and mid Holocene tool-use and processing of taro (Colocasia esculenta), yam (Dioscorea sp.) and other plants at Kuk Swamp in the highlands of Papua New Guinea" (Journal of Archaeological Science, Volume 33, Issue 5, May 2006)
Hoy, Thomas & Matthew Springs (1992), " Direct evidence for human use of plants 28,000 years ago: starch residues on stone artefacts from the northern Solomon Islands" (Antiquity Volume: 66 Number: 253 Page: 898–912)
"The Development of Agriculture". Genographic Project. Retrieved 2017-07-21.
McGourty, Christine (2002-11-22). "Origin of dogs traced". BBC News. Retrieved 2006-11-29.
Fleisch, Henri., Notes de Préhistoire Libanaise : 1) Ard es Saoude. 2) La Bekaa Nord. 3) Un polissoir en plein air. BSPF, vol. 63.
Guns, Germs, and Steel: The Fates of Human Societies. Jared Diamond (1997).
James C. Scott,Against the Grain: a Deep History of the Earliest States, NJ:Yale UP, (2017), "The world's population in 10 000 BC, according to a careful estimate was roughly 4 million. A full five thousand years later it has risen only to 5 million...One likely explanation for this apparent human progress in subsistance techniques together with a long period of demographic stagnation is that epidemologically this was perhaps the most lethal period in human history".
Hermanussen, Michael; Poustka, Fritz (July–September 2003). "Stature of early Europeans". Hormones (Athens). 2 (3): 175–8. PMID 17003019. doi:10.1159/000079404.
Shermer, Michael (2001) The Borderlands of Science, Oxford University Press p.250
Eagly, Alice H. & Wood, Wendy (June 1999). "The Origins of Sex Differences in Human Behavior: Evolved Dispositions Versus Social Roles". American Psychologist. 54 (6): 408–423. doi:10.1037/0003-066x.54.6.408.
Jared Diamond: "The Worst Mistake in the History of the Human Race," Discover Magazine, May 1987, pp. 64-66.
Sherratt 1981
Furuse, Y.; Suzuki, A.; Oshitani, H. (2010). "Origin of measles virus: Divergence from rinderpest virus between the 11th and 12th centuries". Virology Journal. 7: 52. PMC 2838858 Freely accessible. PMID 20202190. doi:10.1186/1743-422X-7-52.
Guns, Germs, and Steel: The Fates of Human Societies - Jared Diamond, 1997
Halcrow, S.; E., Harris, N. J., Tayles, N., Ikehara‐Quebral, R., & Pietrusewsky, M. (2013). "From the mouths of babes: Dental caries in infants and children and the intensification of agriculture in mainland Southeast Asia". American Journal of Physical Anthropology. 150 (3): 409–420. PMID 23359102. doi:10.1002/ajpa.22215.
"BBC - History - Ancient History in depth: Overview: From Neolithic to Bronze Age, 8000 - 800 BC". Retrieved 2017-07-21.
Semino, O; et al. (2004). "Origin, Diffusion, and Differentiation of Y-Chromosome Haplogroups E and J: Inferences on the Neolithization of Europe and Later Migratory Events in the Mediterranean Area". American Journal of Human Genetics. 74 (5): 1023–34. PMC 1181965 Freely accessible. PMID 15069642. doi:10.1086/386295.

    Lancaster, Andrew (2009). "Y Haplogroups, Archaeological Cultures and Language Families: a Review of the Multidisciplinary Comparisons using the case of E-M35" (PDF). Journal of Genetic Genealogy. 5 (1).

Bibliography

    Bailey, Douglass. (2001). Balkan Prehistory: Exclusions, Incorporation and Identity. Routledge Publishers. ISBN 0-415-21598-6.
    Bailey, Douglass. (2005). Prehistoric Figurines: Representation and Corporeality in the Neolithic. Routledge Publishers. ISBN 0-415-33152-8.
    Balter, Michael (2005). The Goddess and the Bull: Catalhoyuk, An Archaeological Journey to the Dawn of Civilization. New York: Free Press. ISBN 0-7432-4360-9.
    Bellwood, Peter. (2004). First Farmers: The Origins of Agricultural Societies. Blackwell Publishers. ISBN 0-631-20566-7
    Bocquet-Appel, Jean-Pierre, editor and Ofer Bar-Yosef, editor, The Neolithic Demographic Transition and its Consequences, Springer (October 21, 2008), hardcover, 544 pages, ISBN 978-1402085383, trade paperback and Kindle editions are also available.
    Cohen, Mark Nathan (1977)The Food Crisis in Prehistory: Overpopulation and the Origins of Agriculture. New Haven and London: Yale University Press. ISBN 0-300-02016-3.
    Diamond, Jared (1997). Guns, germs and steel. A short history of everybody for the last 13,000 years.
    Diamond, Jared (2002). "Evolution, Consequences and Future of Plant and Animal Domestication". Nature, Vol 418.
    Harlan, Jack R. (1992). Crops & Man: Views on Agricultural Origins ASA, CSA, Madison, WI. http://www.hort.purdue.edu/newcrop/history/lecture03/r_3-1.html
    Wright, Gary A. (1971). "Origins of Food Production in Southwestern Asia: A Survey of Ideas" Current Anthropology, Vol. 12, No. 4/5 (Oct.–Dec., 1971), pp. 447–477
    Kuijt, Ian; Finlayson, Bill. (2009). "Evidence for food storage and predomestication granaries 11,000 years ago in the Jordan Valley". PNAS, Vol. 106, No. 27, pp. 10966 –10970.
	
Page protected with pending changes level 1
Agriculture
From Wikipedia, the free encyclopedia
"Farming" redirects here. For other uses, see Farming (disambiguation).
Fields in Záhorie (Slovakia) – a typical Central European agricultural region
Domestic sheep and a cow (heifer) pastured together in South Africa
Maler der Grabkammer des Sennudem 001.jpg
Agriculture
History

    History of organic farming
    Neolithic Revolution
    Arab Agricultural Revolution
    British Agricultural Revolution
    Green Revolution

On land

    Animal husbandry
        cattle pig poultry sheep Dairy Dryland Extensive Free-range Grazing Hobby Intensive
        animal crop Natural Orchard Organic Ranching Sharecropping Slash-and-burn 

In water

    Aquaculture Aquaponics Hydroponics 

Related

    Agribusiness Agricultural engineering Agricultural science Agroecology Agroforestry Agronomy Animal-free Crop diversity Ecology Livestock Mechanisation Permaculture Sustainable Urban 

Lists

    Government ministries
    Universities and colleges

Categories

    Agriculture
        by country companies Biotechnology Livestock Meat industry Poultry farming 

Veranotrigo.jpg Agriculture and agronomy portal

    v t e 

Agriculture is the cultivation and breeding of animals, plants and fungi for food, fiber, biofuel, medicinal plants and other products used to sustain and enhance human life.[1] Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. Industrial agriculture based on large-scale monoculture farming has become the dominant agricultural method.

Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have in many cases sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.

The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are also produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the service sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.

Contents

    1 Etymology and terminology
    2 History
    3 Agriculture and civilization
    4 Types of agriculture
    5 Contemporary agriculture
    6 Workforce
        6.1 Safety
    7 Agricultural production systems
        7.1 Crop cultivation systems
            7.1.1 Crop statistics
        7.2 Livestock production systems
    8 Production practices
    9 Crop alteration and biotechnology
        9.1 Genetic engineering
    10 Environmental impact
        10.1 Livestock issues
        10.2 Land and water issues
        10.3 Pesticides
        10.4 Climate change
        10.5 Sustainability
    11 Agricultural economics
    12 Agricultural science
    13 List of countries by agricultural output
    14 Energy and agriculture
        14.1 Mitigation of effects of petroleum shortages
    15 Policy
    16 See also
    17 References
    18 Further reading
    19 External links

Etymology and terminology

The word agriculture is a late Middle English adaptation of Latin agricultūra, from ager, "field", and cultūra, "cultivation" or "growing".[2] Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle.[3] To practice agriculture means to use natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services."[4] This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and forestry.[4] A distinction is sometimes made between forestry and agriculture, based on the former's longer management rotations, extensive versus intensive management practices and development mainly by nature, rather than by man. Even then, it is acknowledged that there is a large amount of knowledge transfer and overlap between silviculture (the management of forests) and agriculture.[5] In traditional farming, the two are often combined even on small landholdings, leading to the term agroforestry.[6]
History
Main article: History of agriculture
A Sumerian harvester's sickle made from baked clay (c. 3000 BC)

Agriculture began independently in different parts of the globe, and included a diverse range of taxa. At least 11 separate regions of the Old and New World were involved as independent centers of origin.[7] Wild grains were collected and eaten from at least 105,000 years ago.[8] Pigs were domesticated in Mesopotamia around 15,000 years ago.[9] Rice was domesticated in China between 13,500 and 8,200 years ago, followed by mung, soy and azuki beans. Sheep were domesticated in Mesopotamia between 13,000 and 11,000 years ago.[10] From around 11,500 years ago, the eight Neolithic founder crops, emmer and einkorn wheat, hulled barley, peas, lentils, bitter vetch, chick peas and flax were cultivated in the Levant. Cattle were domesticated from the wild aurochs in the areas of modern Turkey and Pakistan some 10,500 years ago.[11] In the Andes of South America, the potato was domesticated between 10,000 and 7,000 years ago, along with beans, coca, llamas, alpacas, and guinea pigs. Sugarcane and some root vegetables were domesticated in New Guinea around 9,000 years ago. Sorghum was domesticated in the Sahel region of Africa by 7,000 years ago. Cotton was domesticated in Peru by 5,600 years ago,[12] and was independently domesticated in Eurasia at an unknown time. In Mesoamerica, wild teosinte was domesticated to maize by 6,000 years ago.[13]

In the Middle Ages, both in the Islamic world and in Europe, agriculture was transformed with improved techniques and the diffusion of crop plants, including the introduction of sugar, rice, cotton and fruit trees such as the orange to Europe by way of Al-Andalus.[14][15] After 1492, the Columbian exchange brought New World crops such as maize, potatoes, sweet potatoes and manioc to Europe, and Old World crops such as wheat, barley, rice and turnips, and livestock including horses, cattle, sheep and goats to the Americas.[16] Irrigation, crop rotation, and fertilizers were introduced soon after the Neolithic Revolution and developed much further in the past 200 years, starting with the British Agricultural Revolution. Since 1900, agriculture in the developed nations, and to a lesser extent in the developing world, has seen large rises in productivity as human labor has been replaced by mechanization, and assisted by synthetic fertilizers, pesticides, and selective breeding. The Haber-Bosch method allowed the synthesis of ammonium nitrate fertilizer on an industrial scale, greatly increasing crop yields.[17][18] Modern agriculture has raised political issues including water pollution, biofuels, genetically modified organisms, tariffs and farm subsidies, leading to alternative approaches such as the organic movement[19][20] and regenerative agriculture.
Agriculture and civilization
Further information: Plants in culture

Civilization was the product of the Agricultural Neolithic Revolution; as H. G. Wells put it, "civilization was the agricultural surplus."[21] In the course of history, civilization coincided in space with fertile areas such as The Fertile Crescent, and states formed mainly in circumscribed agricultural lands. The Great Wall of China and the Roman empire's limes (borders) demarcated the same northern frontier of cereal agriculture. This cereal belt fed the civilizations formed in the Axial Age and connected by the Silk Road.[citation needed]

Ancient Egyptians, whose agriculture depended exclusively on the Nile, deified the river, worshiped, and exalted it in a great hymn.[22] However, the way in which the Nile began to be deified by the people in Egypt was because it played such a crucial role in the formation of civilization within this area. This began because early settlers flocked towards the river bank of the Nile around 6000 BCE which in turn slowly progressed into Egypt becoming the first recognizable nation state around 3150 BCE. Furthermore, it was able to flourish as a nation state due to the many benefits and resources the Nile provided.[23]The Chinese imperial court issued numerous edicts, stating: "Agriculture is the foundation of this Empire."[24] Egyptian, Mesopotamian, Chinese, and Inca Emperors themselves plowed ceremonial fields in order to show personal example to everyone.[25]

Ancient strategists, Chinese Guan Zhong[26] and Shang Yang[27] and Indian Kautilya,[28] drew doctrines linking agriculture with military power. Agriculture defined the limits on how large and for how long an army could be mobilized. Shang Yang called agriculture and war the One.[29] In the vast human pantheon of agricultural deities[30] there are several deities who combined the functions of agriculture and war.[31]

As the Neolithic Agricultural Revolution produced civilization, the modern Agricultural Revolution, begun in Britain (British Agricultural Revolution), made possible the industrial civilization. The first precondition for industry was greater yields by less manpower, resulting in greater percentage of manpower available for non-agricultural sectors.[32]
Types of agriculture
Reindeer herds form the basis of pastoral agriculture for several Arctic and Subarctic peoples.

Pastoralism involves managing domesticated animals. In nomadic pastoralism, herds of livestock are moved from place to place in search of pasture, fodder, and water. This type of farming is practised in arid and semi-arid regions of Sahara, Central Asia and some parts of India.[33]

In shifting cultivation, a small area of a forest is cleared by cutting down all the trees and the area is burned. The land is then used for growing crops for several years. When the soil becomes less fertile, the area is then abandoned. Another patch of land is selected and the process is repeated. This type of farming is practiced mainly in areas with abundant rainfall where the forest regenerates quickly. This practice is used in Northeast India, Southeast Asia, and the Amazon Basin.[34]

Subsistence farming is practiced to satisfy family or local needs alone, with little left over for transport elsewhere. It is intensively practiced in Monsoon Asia and South-East Asia.[35]

In intensive farming, the crops are cultivated for commercial purpose i.e., for selling. The main motive of the farmer is to make profit, with a low fallow ratio and a high use of inputs. This type of farming is mainly practiced in highly developed countries.[36][37]
Contemporary agriculture
Satellite image of farming in Minnesota

In the past century, agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic, regenerative, and sustainable agriculture movements.[19][38] One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies,[39] also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.

In 2007, higher incentives for farmers to grow non-food biofuel crops[40] combined with other factors, such as over development of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth,[41] caused food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe.[42][43] As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes.[44][45][46] The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security. They in part base this on the experience of Vietnam, which went from a food importer to large food exporter and saw a significant drop in poverty, due mainly to the development of smallholder agriculture in the country.[47]

Disease and land degradation are two of the major concerns in agriculture today. For example, an epidemic of stem rust on wheat caused by the Ug99 lineage is currently spreading across Africa and into Asia and is causing major concerns due to crop losses of 70% or more under some conditions.[48] Approximately 40% of the world's agricultural land is seriously degraded.[49] In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to United Nations University's Ghana-based Institute for Natural Resources in Africa.[50]

Agrarian structure is a long-term structure in the Braudelian understanding of the concept. On a larger scale the agrarian structure is more dependent on the regional, social, cultural and historical factors than on the state’s undertaken activities. Like in Poland, where despite running an intense agrarian policy for many years, the agrarian structure in 2002 has much in common with that found in 1921 soon after the partitions period.[51]

In 2009, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States, according to the International Monetary Fund (see below). Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.[52]
Workforce

As of 2011, the International Labour Organization states that approximately one billion people, or over 1/3 of the available work force, are employed in the global agricultural sector. Agriculture constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry.[53] The service sector only overtook the agricultural sector as the largest global employer in 2007. Between 1997 and 2007, the percentage of people employed in agriculture fell by over four percentage points, a trend that is expected to continue.[54] The number of people employed in agriculture varies widely on a per-country basis, ranging from less than 2% in countries like the US and Canada to over 80% in many African nations.[55] In developed countries, these figures are significantly lower than in previous centuries. During the 16th century in Europe, for example, between 55 and 75 percent of the population was engaged in agriculture, depending on the country. By the 19th century in Europe, this had dropped to between 35 and 65 percent.[56] In the same countries today, the figure is less than 10%.[55]
Safety
Rollover protection bar on a Fordson tractor
Main article: Agricultural safety and health

Agriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers.[57] Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects.[58] As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death.[59] Common causes of fatal injuries among young farm workers include drowning, machinery and motor vehicle-related accidents.[59]

The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors."[53] It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported.[60] The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.[53]
Agricultural production systems
Crop cultivation systems
Rice cultivation in Andhra Pradesh, India

Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.[61][62]

Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years.[63] Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.

Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers.[62] Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.[63]

In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.[63]
Crop statistics
See also: List of most important agricultural crops worldwide

Important categories of crops include cereals and pseudocereals, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.
Top agricultural products, by crop types
(million tonnes) 2004 data
Cereals 	2,263
Vegetables and melons 	866
Roots and tubers 	715
Milk 	619
Fruit 	503
Meat 	259
Oilcrops 	133
Fish (2001 estimate) 	130
Eggs 	63
Pulses 	60
Vegetable fiber 	30
Source:
Food and Agriculture Organization (FAO)[64]
Top agricultural products, by individual crops
(million tonnes) 2011 data
Sugar cane 	1794
Maize 	883
Rice 	722
Wheat 	704
Potatoes 	374
Sugar beet 	271
Soybeans 	260
Cassava 	252
Tomatoes 	159
Barley 	134
Source:
Food and Agriculture Organization (FAO)[64]
Livestock production systems
Main article: Livestock
See also: List of domesticated animals
Ploughing rice paddy fields with water buffalo, in Indonesia

Animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.
An ox-pulled plough in India

Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless.[65] As of 2010, 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050.[66] Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.[67]

During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.[68]

Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists.[63] Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.[65]

Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development(OECD) member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution.[65] Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa.[66] Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.[69]
Production practices
Road leading across the farm allows machinery access to the farm for production practices

Farming is the practice of agriculture by specialized labor in an area primarily devoted to agricultural processes, in service of a dislocated population usually in a city.

Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO2, and reduces the abundance and diversity of soil organisms.[70][71]

Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.[72]

Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals.[73] Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period.[74][75] Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.

Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world.[63] Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year.[76] Agriculture represents 70% of freshwater use worldwide.[77]

According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.[78]

"Payment for ecosystem services (PES) can further incentivise efforts to green the agriculture sector. This is an approach that verifies values and rewards the benefits of ecosystem services provided by green agricultural practices."[79] "Innovative PES measures could include reforestation payments made by cities to upstream communities in rural areas of shared watersheds for improved quantities and quality of fresh water for municipal users. Ecoservice payments by farmers to upstream forest stewards for properly managing the flow of soil nutrients, and methods to monetise the carbon sequestration and emission reduction credit benefits of green agriculture practices in order to compensate farmers for their efforts to restore and build SOM and employ other practices."[79]
Crop alteration and biotechnology
Main article: Plant breeding
Tractor and chaser bin

Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.[80]

Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.[81][82]

The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).[83][84][85]
Genetic engineering
Main article: Genetic engineering
See also: Genetically modified food, Genetically modified crops, Regulation of the release of genetic modified organisms, and Genetically modified food controversies

Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering.[86] For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops, which have been put in place due to concerns over potential health issues, declining agricultural diversity and contamination of non-GMO crops.[87] Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.[88]

Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosates. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide.[89] With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides.[90][91] Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.[92]

Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium Bacillus thuringiensis (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects.[93] Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.[94]
Environmental impact
Main article: Environmental issues with agriculture
Water pollution in a rural stream due to runoff from farming activity in New Zealand

Agriculture, as implemented through the method of farming, imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, loss of natural environment and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare.[95] A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million.[96] Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society.[95][96] In 2010, the International Resource Panel of the United Nations Environment Programme published a report assessing the environmental impacts of consumption and production. The study found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions.[97] The 2011 UNEP Green Economy report states that "[a]gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides; (GHG emissions resulting from production of these inputs are included in industrial emissions); and fossil fuel-energy inputs.[79] "On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011–2020 and an average 2 per cent annual expansion after 2020)."[79]
Livestock issues

A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems".[98] Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO2 equivalents. By comparison, all transportation emits 13.5% of the CO2. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO2,) and 37% of all human-induced methane (which is 23 times as warming as CO2.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops.[99] Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the UNEP states that "methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns."[79]
Land and water issues
See also: Environmental impact of irrigation
Water control in field lands, laid connected with together

Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%.[100] Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented.[101] The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).[63]

Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.[102]

Agriculture accounts for 70 percent of withdrawals of freshwater resources.[103] Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia.[104] Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources.[105] Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.[106]
Pesticides
Main article: Environmental impact of pesticides

Pesticide use has increased since 1950 to 2.5 million short tons annually worldwide, yet crop loss from pests has remained relatively constant.[107] The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths.[108] Pesticides select for pesticide resistance in the pest population, leading to a condition termed the "pesticide treadmill" in which pest resistance warrants the development of a new pesticide.[109]

An alternative argument is that the way to "save the environment" and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'.[110][111] However, critics argue that a trade-off between the environment and a need for food is not inevitable,[112] and that pesticides simply replace good agronomic practices such as crop rotation.[109] The UNEP introduces the Push–pull agricultural pest management technique which involves intercropping that uses plant aromas to repel or push away pests while pulling in or attracting the right insects. "The implementation of push-pull in eastern Africa has significantly increased maize yields and the combined cultivation of N-fixing forage crops has enriched the soil and has also provided farmers with feed for livestock. With increased livestock operations, the farmers are able to produce meat, milk and other dairy products and they use the manure as organic fertiliser that returns nutrients to the fields."[79]
Climate change
See also: Climate change and agriculture

Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO2, solar radiation and the interaction of these elements.[63] Extreme events, such as droughts and floods, are forecast to increase as climate change takes hold.[113] Agriculture is among sectors most vulnerable to the impacts of climate change; water supply for example, will be critical to sustain agricultural production and provide the increase in food output required to sustain the world's growing population. Fluctuations in the flow of rivers are likely to increase in the twenty-first century. Based on the experience of countries in the Nile river basin (Ethiopia, Kenya and Sudan) and other developing countries, depletion of water resources during seasons crucial for agriculture can lead to a decline in yield by up to 50%.[114] Transformational approaches will be needed to manage natural resources in the future.[115] For example, policies, practices and tools promoting climate-smart agriculture will be important, as will better use of scientific information on climate for assessing risks and vulnerability. Planners and policy-makers will need to help create suitable policies that encourage funding for such agricultural transformation.[116]

Agriculture in its many forms can both mitigate or worsen global warming. Some of the increase in CO2 in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddy fields,[117] as well as the normal digestive activities of farm animals. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gases nitric oxide and nitrous oxide.[118] Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO2 in the atmosphere.[117] Informed by the UNEP, "[a]griculture also produces about 58 per cent of global nitrous oxide emissions and about 47 per cent of global methane emissions. Cattle and rice farms release methane, fertilized fields release nitrous oxide, and the cutting down of rainforests to grow crops or raise livestock releases carbon dioxide.[119] Both of these gases have a far greater global warming potential per tonne than CO2 (298 times and 25 times respectively)."[79]

There are several factors within the field of agriculture that contribute to the large amount of CO2 emissions. The diversity of the sources ranges from the production of farming tools to the transport of harvested produce. Approximately 8% of the national carbon footprint is due to agricultural sources. Of that, 75% is of the carbon emissions released from the production of crop assisting chemicals.[120] Factories producing insecticides, herbicides, fungicides, and fertilizers are a major culprit of the greenhouse gas. Productivity on the farm itself and the use of machinery is another source of the carbon emission. Almost all the industrial machines used in modern farming are powered by fossil fuels. These instruments are burning fossil fuels from the beginning of the process to the end. Tractors are the root of this source. The tractor is going to burn fuel and release CO2 just to run. The amount of emissions from the machinery increase with the attachment of different units and need for more power. During the soil preparation stage tillers and plows will be used to disrupt the soil. During growth watering pumps and sprayers are used to keep the crops hydrated. And when the crops are ready for picking a forage or combine harvester is used. These types of machinery all require additional energy which leads to increased carbon dioxide emissions from the basic tractors.[121] The final major contribution to CO2 emissions in agriculture is in the final transport of produce. Local farming suffered a decline over the past century due to large amounts of farm subsidies. The majority of crops are shipped hundreds of miles to various processing plants before ending up in the grocery store. These shipments are made using fossil fuel burning modes of transportation. Inevitably these transport adds to carbon dioxide emissions.[122]
Sustainability
See also: List of sustainable agriculture topics

Some major organizations are hailing farming within agroecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and UNEP,[123] there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for more productive farmland, or the preservation of a wetland system that limits fishing rights.[124]

Technological advancements help provide farmers with tools and resources to make farming more sustainable.[125] New technologies have given rise to innovations like conservation tillage, a farming process which helps prevent land loss to erosion, water pollution and enhances carbon sequestration.[126]

According to a report by the International Food Policy Research Institute (IFPRI),[78] agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Agricultural economics
Main article: Agricultural economics
See also: Agricultural subsidy and Rural economics

Agricultural economics refers to economics as it relates to the "production, distribution and consumption of [agricultural] goods and services".[127] Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century.[128] Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States[129] to the European feudal system of manorialism.[130] In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.[131]

National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures.[132] Since at least the 1960s, a combination of import/export restrictions, exchange rate policies and subsidies have affected farmers in both the developing and developed world. In the 1980s, it was clear that non-subsidized farmers in developing countries were experiencing adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements were put into place that limited agricultural tariffs, subsidies and other trade restrictions.[133]

However, as of 2009, there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy.[134] Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers.[135] Unprocessed commodities (i.e. corn, soybeans, cows) are generally graded to indicate quality. The quality affects the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.[136]
Agricultural science
Main article: Agricultural science

Agricultural science is a broad multidisciplinary field of biology that encompasses the parts of exact, natural, economic and social sciences that are used in the practice and understanding of agriculture. (Veterinary science, but not animal science, is often excluded from the definition.)
List of countries by agricultural output
Main article: List of countries by GDP sector composition
See also: List of most important agricultural crops worldwide
Largest countries by agricultural output according to IMF and CIA World Factbook, 2015 Economy 	
Countries by agricultural output in 2015 (billions in USD)
(01)  China 	
1,088
(02)  India 	
413
(—)  European Union 	
333
(03)  United States 	
290
(04)  Indonesia 	
127
(05)  Brazil 	
110
(06)  Nigeria 	
106
(07)  Pakistan 	
63
(08)  Turkey 	
62
(09)  Argentina 	
59
(10)  Japan 	
51
(11)  Egypt 	
47
(12)  Thailand 	
47
(13)  Russia 	
47
(14)  Australia 	
46
(15)  Mexico 	
43
(16)  France 	
42
(17)  Italy 	
41
(18)  Spain 	
39
(19)  Vietnam 	
37
(20)  Iran 	
36

The twenty largest countries by agricultural output in 2015, according to the IMF and CIA World Factbook.
Largest Countries by agricultural output according to UNCTAD at 2005 constant prices and exchange rates, 2015 [137] Economy 	
Countries by agricultural output in 2015 (millions in 2005 constant USD and exchange rates)
(01)  China 	
418,455
(02)  India 	
196,592
(03)  United States 	
149,023
(04)  Nigeria 	
77,113
(05)  Brazil 	
59,977


Energy and agriculture

Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources.[138] Between the 1960–65 measuring cycle and the cycle from 1986 to 1990, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area)[139] as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.[140]
Agriculture and food system share (%) of total energy
consumption by three industrialized nations
Country 	Year 	Agriculture
(direct & indirect) 	Food
system
United Kingdom[141] 	2005 	1.9 	11
United States[142] 	2002 	2.0 	14
Sweden[143] 	2000 	2.5 	13

Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1. direct consumption on the farm and 2. indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gasoline, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1% of the nation's total energy.[140]

Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002.[140] The natural gas and coal consumed by the production of nitrogen fertilizer can account for over half of the agricultural energy usage. China utilizes mostly coal in the production of nitrogen fertilizer, while most of Europe uses large amounts of natural gas and small amounts of coal. According to a 2010 report published by The Royal Society, agriculture is increasingly dependent on the direct and indirect input of fossil fuels. Overall, the fuels used in agriculture vary based on several factors, including crop, production system and location.[144] The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years.[140] Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.[145][142]
Mitigation of effects of petroleum shortages
M. King Hubbert's prediction of world petroleum production rates. Modern agriculture is totally reliant on petroleum energy[146]

In the event of a petroleum shortage (see peak oil for global concerns), organic agriculture can be more attractive than conventional practices that use petroleum-based pesticides, herbicides, or fertilizers. Some studies using modern organic-farming methods have reported yields equal to or higher than those available from conventional farming.[147] In the aftermath of the fall of the Soviet Union, with shortages of conventional petroleum-based inputs, Cuba made use of mostly organic practices, including biopesticides, plant-based pesticides and sustainable cropping practices, to feed its populace.[148] However, organic farming may be more labor-intensive and would require a shift of the workforce from urban to rural areas.[149] The reconditioning of soil to restore organic matter lost during the use of monoculture agriculture techniques is important to provide a reservoir of plant-available nutrients, to maintain texture, and to minimize erosion.[150]

It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural waste to provide charcoal fertilizer, some fuel and food, instead of the normal food vs. fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.[151][152]

It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops.[153] The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds.[154][155] While there has been some research on sustainability using GMO crops, at least one prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop.[156]
Policy
Main article: Agricultural policy
From a Congressional Budget Office report

Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries).[157] Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.[158]

There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities.[159] The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.[160]
See also
Main article: Outline of agriculture

    Aeroponics
    Agricultural engineering
    Agricultural robot
    Agricultural value chain
    Agroecology
    Agroforestry
    Building-integrated agriculture
    Contract farming
    Corporate farming
    Crofting
    Crop
    Ecoagriculture
    Feed additive
    Food security
    Hill farming
    List of documentary films about agriculture
    Pharming (genetics)
    Regenerative agriculture
    Remote sensing
    Subsistence economy
    Vertical farming
    Farmer

    Veranotrigo.jpgAgriculture and agronomy portal Moai Easter Island InvMH-35-61-1.jpgAnthropology portal Stonehenge Closeup.jpgArchaeology portal Goblet Glass (Banquet).svgDrink portal Foodlogo2.svgFood portal 

References

Safety and health in agriculture. International Labour Organization. 1999. pp. 77–. ISBN 978-92-2-111517-5. Retrieved 13 September 2010.
Chantrell, Glynnis, ed. (2002). The Oxford Dictionary of Word Histories. Oxford University Press. p. 14. ISBN 0-19-863121-9.
Ulrich G. Mueller; Nicole M. Gerardo; Duur K. Aanen; Diana L. Six; Ted R. Schultz (December 2005). "The Evolution of Agriculture in Insects". Annual Review of Ecology, Evolution, and Systematics. 36: 563–95. doi:10.1146/annurev.ecolsys.36.102003.152626.
"Definition of Agriculture". State of Maine. Retrieved 6 May 2013.
Committee on Forestry Research, National Research Council (1990). Forestry Research: A Mandate for Change. National Academies Press. pp. 15–16. ISBN 0-309-04248-8.
Budowski, Gerardo (1982). "Applicability of agro-forestry systems". In MacDonald, L.H. Agro-forestry in the African Humid Tropics. United Nations University. ISBN 92-808-0364-6. Retrieved 17 March 2016.
Larson, G.; Piperno, D. R.; Allaby, R. G.; Purugganan, M. D.; Andersson, L.; Arroyo-Kalin, M.; Barton, L.; Climer Vigueira, C.; Denham, T.; Dobney, K.; Doust, A.N.; Gepts, P.; Gilbert, M. T. P.; Gremillion, K. J.; Lucas, L.; Lukens, L.; Marshall, F. B.; Olsen, K.M.; Pires, J.C.; Richerson, P.J.; Rubio De Casas, R.; Sanjur, O.I.; Thomas, M.G.; Fuller, D.Q. (2014). "Current perspectives and the future of domestication studies". Proceedings of the National Academy of Sciences. 111 (17): 6139. PMC 4035915 Freely accessible. PMID 24757054. doi:10.1073/pnas.1323964111 Freely accessible.
Harmon, Katherine (17 December 2009). "Humans feasting on grains for at least 100,000 years". Scientific American. Retrieved 28 August 2016.
Nelson, Sarah M. (1998). Ancestors for the Pigs. Pigs in prehistory. University of Pennsylvania Museum of Archaeology and Anthropology.
Ensminger, M.E.; Parker, R.O. (1986). Sheep and Goat Science (Fifth ed.). Interstate Printers and Publishers. ISBN 0-8134-2464-X.
McTavish, E.J.; Decker, J.E.; Schnabel, R.D.; Taylor, J.F. & Hillis, D.M. (2013). "New World cattle show ancestry from multiple independent domestication events". Proc. Natl. Acad. Sci. U.S.A. 110: E1398–406. PMC 3625352 Freely accessible. PMID 23530234. doi:10.1073/pnas.1303367110.
Broudy, Eric (1979). The Book of Looms: A History of the Handloom from Ancient Times to the Present. UPNE. p. 81. ISBN 978-0-87451-649-4.
S. Johannessen and C. A. Hastorf (eds.) Corn and Culture in the Prehistoric New World, Westview Press, Boulder, Colorado.
Watson, Andrew M. (1974). "The Arab Agricultural Revolution and Its Diffusion, 700–1100". The Journal of Economic History. 34 (1): 8–35. doi:10.1017/s0022050700079602.
National Geographic (2015). Food Journeys of a Lifetime. National Geographic Society. p. 126. ISBN 978-1-4262-1609-1.
Crosby, Alfred. "The Columbian Exchange". The Gilder Lehrman Institute of American History. Retrieved 11 May 2013.
Janick, Jules. "Agricultural Scientific Revolution: Mechanical" (PDF). Purdue University. Retrieved 24 May 2013.
Reid, John F. (2011). "The Impact of Mechanization on Agriculture". The Bridge on Agriculture and Information Technology. 41 (3).
Philpott, Tom (19 April 2013). "A Brief History of Our Deadly Addiction to Nitrogen Fertilizer". Mother Jones. Retrieved 7 May 2013.
"Ten worst famines of the 20th century". Sydney Morning Herald. 15 August 2011.
H. G. Wells (1914). World Set Free. Macmillan. p. 194.
Ancient Egyptian Literature: A Book of Readings, (ed. Miriam Lichtheim, Berkeley & Los Angeles & London: University of California Press, 1975, vol I, pp. 205–09).
"Nile". Ancient History Encyclopedia. Retrieved 2017-10-09.
Han Agriculture: The Foundation of Early Chinese Agrarian Economy, 206 BC – AD 220, (ed. Cho-yun Hsu, Seattle & London: University of Washington Press, pp. 169–70).
Guan Zhong, Guanzi: Economic Dialogues in Ancient China, (trs. Po-fu Tan, & Kuang-wen Wen, Connecticut: New Heaven, 1954, p. 174); Samuel Noah Kramer, History Begins at Sumer: Thirty-Nine Firsts in Man's Recorded History, New York: University of Pennsylvania Press, 1953, pp. 96–97; Rafael Karsten, A Totalitarian State of the Past: The Civilization of the Inca Empire in Ancient Peru, Helsingforce: Academic Bookstore, 1949, pp. 93, 181.
Guanzi: Economic Dialogues in Ancient China, (trs. Po-fu Tan, & Kuang-wen Wen, Connecticut: New Heaven, 1954, pp. 94, 129–30, 156–57, 362).
The Book of the Governor of the Shang Region,( tr. L. S. Perelomov, Moscow: Nauka, 1993, p. 153).
Arthashastra, (tr. T. N. Ramaswamy, London: Asia Publishers, 1962, pp. 126, 128).
One chapter Shang Yang titled "Reflections on the One," The Book of the Governor of the Shang Region, (p. 150).
Category:agricultural deities; Category:agricultural gods
The Mesopotamian Lahmu; the Hittite goddess of Arinna, and the Roman Janus. From Lahmu derive two Hebrew words—warfare (lehima) and bread (lehem).
Barrington Moore, Social Origins of Democracy and Dictatorship, (London: Penguin Books, 1967, p. 429; Shepard B. Clough & Richard T. Rapp, European Economic History: The Economic Development of Western Civilization, (London & Sydney: McGraw-Hill, 1968, p. 258).
Blench, Roger (2001). Pastoralists in the new millennium (PDF). FAO. pp. 11–12.
"Shifting cultivation". Survival International. Retrieved 28 August 2016.
Waters, Tony (2007). The Persistence of Subsistence Agriculture: life beneath the level of the marketplace. Lexington Books.
Encyclopædia Britannica's definition of Intensive Agriculture
BBC School fact sheet on intensive farming
Scheierling, Susanne M. (1995). "Overcoming agricultural pollution of water: the challenge of integrating agricultural and environmental policies in the European Union, Volume 1". The World Bank. Retrieved 15 April 2013.
"CAP Reform". European Commission. 2003. Retrieved 15 April 2013.
Smith, Kate; Edwards, Rob (8 March 2008). "2008: The year of global food crisis". The Herald. Glasgow.
"The global grain bubble". The Christian Science Monitor. 18 January 2008. Retrieved 26 September 2013.
"The cost of food: Facts and figures". BBC. 16 October 2008. Retrieved 26 September 2013.
Walt, Vivienne (27 February 2008). "The World's Growing Food-Price Crisis". Time.
Watts, Jonathan (4 December 2007). "Riots and hunger feared as demand for grain sends food costs soaring", The Guardian (London).
Mortished, Carl (7 March 2008)."Already we have riots, hoarding, panic: the sign of things to come?", The Times (London).
Borger, Julian (26 February 2008). "Feed the world? We are fighting a losing battle, UN admits", The Guardian (London).
"Food prices: smallholder farmers can be part of the solution". International Fund for Agricultural Development. Retrieved 24 April 2013.
"Wheat Stem Rust – UG99 (Race TTKSK)". FAO. Retrieved 6 January 2014.
Sample, Ian (31 August 2007). "Global food crisis looms as climate change and population growth strip fertile land", The Guardian (London).
"Africa may be able to feed only 25% of its population by 2025". Mongabay. 14 December 2006. Archived from the original on 27 November 2011. Retrieved 15 July 2016.
M. Pietrzak, D. Walczak. 2014. The Analysis of the Agrarian Structure in Poland with the Special Consideration of the Years 1921 and 2002, Bulgarian Journal of Agricultural Science, Vol 20, No 5, pp. 1025, 1038.
"Agricultural Productivity in the United States". USDA Economic Research Service. 5 July 2012. Retrieved 22 April 2013.
"Safety and health in agriculture". International Labour Organization. 21 March 2011. Retrieved 24 April 2013.
"Services sector overtakes farming as world's biggest employer: ILO". The Financial Express. Associated Press. 26 January 2007. Retrieved 24 April 2013.
"Labor Force – By Occupation". The World Factbook. Central Intelligence Agency. Retrieved 4 May 2013.
Allen, Robert C. "Economic structure and agricultural productivity in Europe, 1300–1800" (PDF). European Review of Economic History. 3: 1–25. Archived from the original (PDF) on 27 October 2014.
"NIOSH Workplace Safety & Health Topic: Agricultural Injuries". Centers for Disease Control and Prevention. Retrieved 16 April 2013.
"NIOSH Pesticide Poisoning Monitoring Program Protects Farmworkers". Centers for Disease Control and Prevention. Retrieved 15 April 2013.
"NIOSH Workplace Safety & Health Topic: Agriculture". Centers for Disease Control and Prevention. Retrieved 16 April 2013.
"Agriculture: A hazardous work". International Labour Organization. 15 June 2009. Retrieved 24 April 2013.
"Analysis of farming systems". Food and Agriculture Organization. Retrieved 22 May 2013.
Acquaah, G. 2002. Agricultural Production Systems. pp. 283–317 in "Principles of Crop Production, Theories, Techniques and Technology". Prentice Hall, Upper Saddle River, NJ.
Chrispeels, M.J.; Sadava, D.E. 1994. "Farming Systems: Development, Productivity, and Sustainability". pp. 25–57 in Plants, Genes, and Agriculture. Jones and Bartlett, Boston, MA.
"Food and Agriculture Organization of the United Nations (FAOSTAT)". Archived from the original on 18 January 2013. Retrieved 2 February 2013.
Sere, C.; Steinfeld, H.; Groeneweld, J. (1995). "Description of Systems in World Livestock Systems – Current status issues and trends". U.N. Food and Agriculture Organization. Retrieved 8 September 2013.
Thornton, Philip K. (27 September 2010). "Livestock production: recent trends, future prospects". Philosophical Transactions of the Royal Society B. 365 (1554): 2853–67. doi:10.1098/rstb.2010.0134 Freely accessible.
Stier, Ken (19 September 2007). "Fish Farming's Growing Dangers". Time.
P. Ajmone-Marsan (May 2010). "A global view of livestock biodiversity and conservation – Globaldiv". Animal Genetics. 41 (supplement S1): 1–5. doi:10.1111/j.1365-2052.2010.02036.x.
"Growth Promoting Hormones Pose Health Risk to Consumers, Confirms EU Scientific Committee" (PDF). European Union. 23 April 2002. Retrieved 6 April 2013.
Brady, N.C. and R.R. Weil. 2002. Elements of the Nature and Properties of Soils. Pearson Prentice Hall, Upper Saddle River, NJ.
Acquaah, G. 2002. "Land Preparation and Farm Energy" pp. 318–38 in Principles of Crop Production, Theories, Techniques and Technology. Prentice Hall, Upper Saddle River, NJ.
Acquaah, G. 2002. "Pesticide Use in U.S. Crop Production" pp. 240–82 in Principles of Crop Production, Theories, Techniques and Technology. Prentice Hall, Upper Saddle River, NJ.
Acquaah, G. 2002. "Soil and Land" pp. 165–210 in Principles of Crop Production, Theories, Techniques and Technology. Prentice Hall, Upper Saddle River, NJ.
Chrispeels, M.J.; Sadava, D.E. 1994. "Nutrition from the Soil" pp. 187–218 in Plants, Genes, and Agriculture. Jones and Bartlett, Boston, MA.
Brady, N.C.; Weil, R.R. 2002. "Practical Nutrient Management" pp. 472–515 in Elements of the Nature and Properties of Soils. Pearson Prentice Hall, Upper Saddle River, NJ.
Acquaah, G. 2002. "Plants and Soil Water" pp. 211–39 in Principles of Crop Production, Theories, Techniques and Technology. Prentice Hall, Upper Saddle River, NJ.
Pimentel, D.; Berger, D.; Filberto, D.; Newton, M.; et al. (2004). "Water Resources: Agricultural and Environmental Issues". BioScience. 54 (10): 909–18. doi:10.1641/0006-3568(2004)054[0909:WRAAEI]2.0.CO;2 Freely accessible.
International Food Policy Research Institute (2014). "Food Security in a World of Growing Natural Resource Scarcity". CropLife International. Retrieved 1 July 2013.
UNEP, 2011, Towards a Green Economy: Pathways to Sustainable Development and Poverty Eradication, www.unep.org/greeneconomy
"History of Plant Breeding". Colorado State University. 29 January 2004. Retrieved 11 May 2013.
Stadler, L. J.; Sprague, G.F. (15 October 1936). "Genetic Effects of Ultra-Violet Radiation in Maize: I. Unfiltered Radiation" (PDF). Proceedings of the National Academy of Sciences of the United States of America. US Department of Agriculture and Missouri Agricultural Experiment Station. 22 (10): 572–78. PMC 1076819 Freely accessible. PMID 16588111. doi:10.1073/pnas.22.10.572. Archived (PDF) from the original on 24 October 2007. Retrieved 11 October 2007.
Berg, Paul; Singer, Maxine (15 August 2003). George Beadle: An Uncommon Farmer. The Emergence of Genetics in the 20th century. Cold Springs Harbor Laboratory Press. ISBN 978-0-87969-688-7.
Ruttan, Vernon W. (December 1999). "Biotechnology and Agriculture: A Skeptical Perspective" (PDF). AgBioForum. 2 (1): 54–60.
Cassman, K. (5 December 1998). "Ecological intensification of cereal production systems: The Challenge of increasing crop yield potential and precision agriculture". Proceedings of a National Academy of Sciences Colloquium, Irvine, California. University of Nebraska. Archived from the original on 24 October 2007. Retrieved 11 October 2007.
Conversion note: 1 bushel of wheat = 60 pounds (lb) ≈ 27.215 kg. 1 bushel of maize = 56 pounds ≈ 25.401 kg
"20 Questions on Genetically Modified Foods". World Health Organization. Retrieved 16 April 2013.
Whiteside, Stephanie (28 November 2012). "Peru bans genetically modified foods as US lags". Current TV. Archived from the original on 24 March 2013. Retrieved 7 May 2013.
Shiva, Vandana (2005). Earth Democracy: Justice, Sustainability, and Peace. Cambridge, MA: South End Press.
Kathrine Hauge Madsen; Jens Carl Streibig. "Benefits and risks of the use of herbicide-resistant crops". Weed Management for Developing Countries. FAO. Retrieved 4 May 2013.
"Farmers Guide to GMOs" (PDF). Rural Advancement Foundation International. Retrieved 16 April 2013.
Brian Hindo (13 February 2008). "Report Raises Alarm over 'Super-weeds'". Bloomberg BusinessWeek.
Ozturk; et al. (2008). "Glyphosate inhibition of ferric reductase activity in iron deficient sunflower roots". New Phytologist. 177: 899–906. PMID 18179601. doi:10.1111/j.1469-8137.2007.02340.x.
"Insect-resistant Crops Through Genetic Engineering". University of Illinois. Retrieved 4 May 2013.
Kimbrell, A. (2002). Fatal Harvest: The Tragedy of Industrial Agriculture. Washington: Island Press.
Pretty, J; et al. (2000). "An assessment of the total external costs of UK agriculture". Agricultural Systems. 65 (2): 113–36. doi:10.1016/S0308-521X(00)00031-7.
Tegtmeier, E.M.; Duffy, M. (2005). "External Costs of Agricultural Production in the United States" (PDF). The Earthscan Reader in Sustainable Agriculture.
International Resource Panel (2010). "Priority products and materials: assessing the environmental impacts of consumption and production". United Nations Environment Programme. Archived from the original on 24 December 2012. Retrieved 7 May 2013.
"Livestock a major threat to environment". UN Food and Agriculture Organization. 29 November 2006. Archived from the original on 28 March 2008. Retrieved 24 April 2013.
Steinfeld, H.; Gerber, P.; Wassenaar, T.; Castel, V.; Rosales, M.; de Haan, C. (2006). "Livestock's Long Shadow – Environmental issues and options" (PDF). Rome: U.N. Food and Agriculture Organization. Archived from the original (PDF) on 25 June 2008. Retrieved 5 December 2008.
Vitousek, P.M.; Mooney, H.A.; Lubchenco, J.; Melillo, J.M. (1997). "Human Domination of Earth's Ecosystems". Science. 277 (5325): 494–99. doi:10.1126/science.277.5325.494.
Bai, Z.G.; D.L. Dent; L. Olsson & M.E. Schaepman (November 2008). "Global assessment of land degradation and improvement: 1. identification by remote sensing" (PDF). FAO/ISRIC. Archived from the original (PDF) on 13 December 2013. Retrieved 24 May 2013.
Carpenter, S.R.; N.F. Caraco; D.L. Correll; R.W. Howarth; A.N. Sharpley & V.H. Smith (1998). "Nonpoint Pollution of Surface Waters with Phosphorus and Nitrogen". Ecological Applications. 8 (3): 559–68. doi:10.1890/1051-0761(1998)008[0559:NPOSWW]2.0.CO;2. hdl:1808/16724 Freely accessible.
Molden, D. (ed.). "Findings of the Comprehensive Assessment of Water Management in Agriculture" (PDF). Annual Report 2006/2007. International Water Management Institute. Retrieved 6 January 2014.
Li, Sophia (13 August 2012). "Stressed Aquifers Around the Globe". New York Times. Retrieved 7 May 2013.
"Water Use in Agriculture". FAO. November 2005. Archived from the original on 15 June 2013. Retrieved 7 May 2013.
"Water Management: Towards 2030". FAO. March 2003. Archived from the original on 10 May 2013. Retrieved 7 May 2013.
Pimentel, D.; T.W. Culliney; T. Bashore (1996). "Public health risks associated with pesticides and natural toxins in foods". Radcliffe's IPM World Textbook. Archived from the original on 18 February 1999. Retrieved 7 May 2013.
WHO. 1992. Our planet, our health: Report of the WHO commission on health and environment. Geneva: World Health Organization.
Chrispeels, M.J. and D.E. Sadava. 1994. "Strategies for Pest Control" pp. 355–83 in Plants, Genes, and Agriculture. Jones and Bartlett, Boston, MA.
Avery, D.T. (2000). Saving the Planet with Pesticides and Plastic: The Environmental Triumph of High-Yield Farming. Indianapolis, IN: Hudson Institute.
"Center for Global Food Issues". Center for Global Food Issues. Retrieved 14 July 2016.
Lappe, F.M., J. Collins, and P. Rosset. 1998. "Myth 4: Food vs. Our Environment" pp. 42–57 in World Hunger, Twelve Myths, Grove Press, New York.
Harvey, Fiona (18 November 2011). "Extreme weather will strike as climate change takes hold, IPCC warns". The Guardian.
"Report: Blue Peace for the Nile" (PDF). Strategic Foresight Group. Archived from the original (PDF) on 8 September 2013. Retrieved 20 August 2013.
"World: Pessimism about future grows in agribusiness". Archived from the original on 10 November 2013.
"SREX: Lessons for the agricultural sector". Climate & Development Knowledge Network. Retrieved 24 May 2013.
Brady, N.C. and R.R. Weil. 2002. "Soil Organic Matter" pp. 353–85 in Elements of the Nature and Properties of Soils. Pearson Prentice Hall, Upper Saddle River, NJ.
Brady, N.C. and R.R. Weil. 2002. "Nitrogen and Sulfur Economy of Soils" pp. 386–421 in Elements of the Nature and Properties of Soils. Pearson Prentice Hall, Upper Saddle River, NJ.
Foley, Jonathan (May 2014). "A Five-Step Plan to Feed The World". National Geographic.
Hillier, Jonathon; C. Hawes; G. Squire; A. Hilton (2009). "The carbon footprints of food crop production". International Journal of Agricultural Sustainability. 7 (2): 107–18. doi:10.3763/ijas.2009.0419.
Lal, Rattan (2004). "Carbon emission from farm operations". Environmental International. 30 (7): 981–90. doi:10.1016/j.envint.2004.03.005.
West, T.O.; G. Marland (2002). "Net carbon flux from agricultural ecosystems: methodology for full carbon cycle analyses". Environmental Pollution. 116 (3): 439–44. doi:10.1016/s0269-7491(01)00221-4.
Boelee, E., ed. (2011). "Ecosystems for water and food security". IWMI/UNEP. Retrieved 24 May 2013.
Molden, D. "Opinion: The Water Deficit" (PDF). The Scientist. Retrieved 23 August 2011.
Safefood Consulting, Inc. (2005). "Benefits of Crop Protection Technologies on Canadian Food Production, Nutrition, Economy and the Environment". CropLife International. Retrieved 24 May 2013.
Trewavas, Anthony (2004). "A critical assessment of organic farming-and-food assertions with particular respect to the UK and the potential environmental benefits of no-till agriculture". Crop Protection. 23 (9): 757–81. doi:10.1016/j.cropro.2004.01.009.
"Agricultural Economics". University of Idaho. Archived from the original on 1 April 2013. Retrieved 16 April 2013.
Runge, C. Ford (June 2006). "Agricultural Economics: A Brief Intellectual History" (PDF). Center for International Food and Agriculture Policy. p. 4. Retrieved 16 September 2013.
Conrad, David E. "Tenant Farming and Sharecropping". Encyclopedia of Oklahoma History and Culture. Oklahoma Historical Society. Retrieved 16 September 2013.
Stokstad, Marilyn (2005). Medieval Castles. Greenwood Publishing Group. ISBN 0-313-32525-1. Retrieved 17 March 2016.
Sexton, R.J. (2000). "Industrialization and Consolidation in the US Food Sector: Implications for Competition and Welfare". American Journal of Agricultural Economics. 82 (5): 1087–104. doi:10.1111/0002-9092.00106.
Peter J. Lloyd; Johanna L. Croser; Kym Anderson (March 2009). "How Do Agricultural Policy Restrictions to Global Trade and Welfare Differ across Commodities?" (PDF). Policy Research Working Paper #4864. The World Bank. pp. 2–3. Retrieved 16 April 2013.
Kym Anderson; Ernesto Valenzuela (April 2006). "Do Global Trade Distortions Still Harm Developing Country Farmers?" (PDF). World Bank Policy Research Working Paper 3901. World Bank. pp. 1–2. Retrieved 16 April 2013.
Peter J. Lloyd; Johanna L. Croser; Kym Anderson (March 2009). "How Do Agricultural Policy Restrictions to Global Trade and Welfare Differ across Commodities?" (PDF). Policy Research Working Paper #4864. The World Bank. p. 21. Retrieved 16 April 2013.
Glenys Kinnock (24 May 2011). "America's $24bn subsidy damages developing world cotton farmers". The Guardian. Retrieved 16 April 2013.
"Agriculture's Bounty" (PDF). May 2013. Retrieved 19 August 2013.
http://unctadstat.unctad.org/wds/TableViewer/tableView.aspx?ReportId=95
"World oil supplies are set to run out faster than expected, warn scientists". The Independent. 14 June 2007. Archived from the original on 21 October 2010. Retrieved 14 July 2016.
Robert W. Herdt (30 May 1997). "The Future of the Green Revolution: Implications for International Grain Markets" (PDF). The Rockefeller Foundation. p. 2. Retrieved 16 April 2013.
Schnepf, Randy (19 November 2004). "Energy use in Agriculture: Background and Issues" (PDF). CRS Report for Congress. Congressional Research Service. Retrieved 26 September 2013.
Rebecca White (2007). "Carbon governance from a systems perspective: an investigation of food production and consumption in the UK" (PDF). Oxford University Center for the Environment. Archived from the original (PDF) on 19 July 2011.
Patrick Canning; Ainsley Charles; Sonya Huang; Karen R. Polenske; Arnold Waters (2010). "Energy Use in the U.S. Food System". USDA Economic Research Service Report No. ERR-94. United States Department of Agriculture.
Wallgren, Christine; Höjer, Mattias (2009). "Eating energy—Identifying possibilities for reduced energy use in the future food supply system". Energy Policy. 37 (12): 5803–13. ISSN 0301-4215. doi:10.1016/j.enpol.2009.08.046.
Jeremy Woods; Adrian Williams; John K. Hughes; Mairi Black; Richard Murphy (August 2010). "Energy and the food system". Philosophical Transactions of the Royal Society. 365 (1554): 2991–3006. doi:10.1098/rstb.2010.0172 Freely accessible.
Martin Heller; Gregory Keoleian (2000). "Life Cycle-Based Sustainability Indicators for Assessment of the U.S. Food System" (PDF). University of Michigan Center for Sustainable Food Systems. Archived from the original (PDF) on 14 March 2016. Retrieved 17 March 2016.
"World oil supplies are set to run out faster than expected, warn scientists". The Independent. 14 June 2007.
Barry Estabrook (December 5, 2011). "Organic Can Feed the World". The Atlantic. Archived from the original on 25 April 2016. Retrieved 14 July 2016.
"Cuban Organic Farming Experiment". Harvard School of Public Health. Retrieved 15 April 2013.
Strochlic, R.; Sierra, L. (2007). "Conventional, Mixed, and "Deregistered" Organic Farmers: Entry Barriers and Reasons for Exiting Organic Production in California" (PDF). California Institute for Rural Studies. Retrieved 15 April 2013.
Alexandra Bot and José Benites (2005), The importance of soil organic matter: Key to drought-resistant soil and sustained food production, FAO Soils Bulletin, 80, Food and Agriculture Organization of the United Nations
P. Read (2005). "Carbon cycle management with increased photo-synthesis and long-term sinks" (PDF). Geophysical Research Abstracts. 7: 11082.
Greene, Nathanael (December 2004). "How biofuels can help end America's energy dependence". Biotechnology Industry Organization.
Srinivas (June 2008). "Reviewing The Methodologies For Sustainable Living". 7. The Electronic Journal of Environmental, Agricultural and Food Chemistry.
R. Pillarisetti; Kylie Radel (June 2004). "Economic and Environmental Issues in International Trade and Production of Genetically Modified Foods and Crops and the WTO". Journal of Economic Integration. 19 (2): 332–52. doi:10.11130/jei.2004.19.2.332.
Conway, G. (2000). "Genetically modified crops: risks and promise". 4 (1). Conservation Ecology: 2.
"Monsanto failure". New Scientist. 181 (2433). London. 7 February 2004. Retrieved 18 April 2008.
Lindsay Hogan; Paul Morris (October 2010). "Agricultural and food policy choices in Australia" (PDF). Sustainable agriculture and food policy in the 21st century: challenges and solutions. Australian Bureau of Agricultural and Resource Economics – Bureau of Rural Sciences: 13. Archived from the original (PDF) on 21 March 2012. Retrieved 22 April 2013.
"Agriculture: Not Just Farming ...". European Union. Retrieved 22 April 2013.
Ikerd, John (2010). "Corporatization of Agricultural Policy". Small Farm Today Magazine.

    Jowit, Juliette (22 September 2010). "Corporate Lobbying Is Blocking Food Reforms, Senior UN Official Warns: Farming Summit Told of Delaying Tactics by Large Agribusiness and Food Producers on Decisions that Would Improve Human Health and the Environment". The Guardian. London.

Further reading

    Alvarez, Robert A (2007). "The March of Empire: Mangos, Avocados, and the Politics of Transfer". Gastronomica. 7 (3): 28–33. doi:10.1525/gfc.2007.7.3.28.
    Bolens, L. (1997). "Agriculture" in Selin, Helaine (ed.), Encyclopedia of the History of Science, Technology, and Medicine in Non-Western Cultures. Kluwer Academic Publishers, Dordrecht/Boston/London, pp. 20–22.
    Collinson, M. (ed.) A History of Farming Systems Research. CABI Publishing, 2000. ISBN 978-0-85199-405-5
    Jared Diamond, Guns, germs and steel. A short history of everybody for the last 13,000 years, 1997.
    Mazoyer, Marcel; Roudart, Laurence (2006). A history of world agriculture: from the Neolithic Age to the current crisis. Monthly Review Press, New York. ISBN 978-1-58367-121-4
    Watson, A.M. (1983). Agricultural Innovation in the Early Islamic World, Cambridge University Press.

External links
Find more about
Agriculture
at Wikipedia's sister projects

    Definitions from Wiktionary
    Media from Commons
    News from Wikinews
    Quotations from Wikiquote
    Texts from Wikisource
    Textbooks from Wikibooks
    Learning resources from Wikiversity
    Data from Wikidata

    Official website of the Food and Agriculture Organization (FAO) of the United Nations
    Official website of the United States Department of Agriculture (USDA)
        Official website of the USDA's Agricultural Research Service
    Agriculture Research Guide from the Government Information Library of the University of Colorado, Boulder
    Agriculture material from the World Bank Group
    "Agriculture collected news and commentary". The New York Times.
    "Agriculture collected news and commentary". The Guardian.

	
Horticulture
From Wikipedia, the free encyclopedia

Horticulture is the science and art of growing (plants) - fruits, vegetables, flowers, and any other cultivar. It also includes plant conservation, landscape restoration, soil management, landscape and garden design, construction, and maintenance, and arboriculture. In contrast to agriculture, horticulture does not include large-scale crop production or animal husbandry.

Horticulturists apply their knowledge, skills, and technologies used to grow intensively produced plants for human food and non-food uses and for personal or social needs. Their work involves plant propagation and cultivation with the aim of improving plant growth, yields, quality, nutritional value, and resistance to insects, diseases, and environmental stresses. They work as gardeners, growers, therapists, designers, and technical advisors in the food and non-food sectors of horticulture. Horticulture even refers to the growing of plants in a field or garden.

Contents

    1 Etymology
    2 Scope
    3 Anthropology
    4 Organizations
    5 See also
    6 References
    7 Further reading
    8 External links

Etymology

The word horticulture is modeled after agriculture, and comes from the Greek χόρτος, which in Latin became hortus "garden"[1] and cultūra "cultivation", from cultus, the perfect passive participle of the verb colō "I cultivate".[2] Hortus is cognate with the native English word yard (in the meaning of land associated with a building) and also the borrowed word garden.[3]
Scope

Horticulture involves nine areas of study,[citation needed] which can be grouped into two broad sections: ornamentals and edibles:

    Arboriculture is the study of, and the selection, plant, care, and removal of, individual trees, shrubs, vines, and other perennial woody plants.
    Turf management includes all aspects of the production and maintenance of turf grass for sports, leisure use or amenity use.
    Floriculture includes the production and marketing of floral crops.
    Landscape horticulture includes the production, marketing and maintenance of landscape plants.
    Olericulture includes the production and marketing of vegetables.
    Pomology includes the production and marketing of pome fruits.
    Viticulture includes the production and marketing of grapes.
    Oenology includes all aspects of wine and winemaking.
    Postharvest physiology involves maintaining the quality of and preventing the spoilage of plants and animals.

Anthropology

Horticulture has a very long history.[4] The study and science of horticulture dates all the way back to the times of Cyrus the Great of ancient Persia, and has been going on ever since, with present-day horticulturists such as Freeman S. Howlett and Luther Burbank. The practice of horticulture can be retraced for many thousands of years. The cultivation of taro and yam in Papua New Guinea dates back to at least 6950–6440 cal BP.[5] The origins of horticulture lie in the transition of human communities from nomadic hunter-gatherers to sedentary or semi-sedentary horticultural communities, cultivating a variety of crops on a small scale around their dwellings or in specialized plots visited occasionally during migrations from one area to the next (such as the "milpa" or maize field of Mesoamerican cultures).[6] In the Pre-Columbian Amazon Rainforest, natives are believed to have used biochar to enhance soil productivity by smoldering plant waste.[7] European settlers called it Terra Preta de Indio.[8] In forest areas such horticulture is often carried out in swiddens ("slash and burn" areas).[9] A characteristic of horticultural communities is that useful trees are often to be found planted around communities or specially retained from the natural ecosystem.

Horticulture primarily differs from agriculture in two ways. First, it generally encompasses a smaller scale of cultivation, using small plots of mixed crops rather than large fields of single crops. Secondly, horticultural cultivations generally include a wide variety of crops, even including fruit trees with ground crops. Agricultural cultivations however as a rule focus on one primary crop. In pre-contact North America the semi-sedentary horticultural communities of the Eastern Woodlands (growing maize, squash and sunflower) contrasted markedly with the mobile hunter-gatherer communities of the Plains people. In Central America, Maya horticulture involved augmentation of the forest with useful trees such as papaya, avocado, cacao, ceiba and sapodilla. In the cornfields, multiple crops were grown such as beans (using cornstalks as supports), squash, pumpkins and chilli peppers, in some cultures tended mainly or exclusively by women.[10]
Organizations

The oldest Horticultural society in the world, was found in 1768, is the Ancient Society of York Florists. They still have four shows a year in York, UK.[11] They have a large archive of records, including the original members book dating back to 1768.

Desh Bhagat School of Horticulture Sciences at Desh Bhagat University, Punjab is a prestigious institution in India to encourage the research and promote horticulture techniques throughout the country. The Royal Horticultural Society is a UK charity which oversees several major shows and gardens.[12]

The professional body representing horticulturists in Great Britain and Ireland is the Institute of Horticulture (IOH).[13] Also, the IOH has an international branch for members outside of these islands.

The International Society for Horticultural Science[14] promotes and encourages research and education in all branches of horticultural science.

The American Society of Horticultural Science[15] promotes and encourages research and education in all branches of horticultural science in the Americas.

The Australian Society of Horticultural Science was established in 1990 as a professional society for the promotion and enhancement of Australian horticultural science and industry.[16]

The National Junior Horticultural Association (NJHA) was established in 1934 and was the first organization in the world dedicated solely to youth and horticulture. NJHA programs are designed to help young people obtain a basic understanding of, and develop skills in, the ever-expanding art and science of horticulture.[17]

The New Zealand Horticulture Institute.[18]

The Global Horticulture Initiative (GlobalHort) fosters more efficient and effective partnerships and collective action among different stakeholders in horticulture. The organization has a special focus on horticulture for development (H4D), i.e. using horticulture to reduce poverty and improve nutrition worldwide. To be efficient, GlobalHort is organized in a consortium of national and international organizations to collaborate in research, training, and technology-generating activities designed to meet mutually-agreed-upon objectives. GlobalHort is a not-for-profit organization registered in Belgium.[19]
See also

    iconAgriculture and Agronomy portal iconGardening portal 

    Floriculture
    Forest gardening
    Gardening
    Genetically modified trees
    Genomics of domestication
    Hoe-farming
    Horticultural botany
    Horticultural flora
    Horticultural oil
    Horticultural therapy
    Indigenous horticulture
    Landscaping
    Permaculture
    Plant nutrition
    Plug (horticulture)
    Tropical horticulture
    Turf management
    Vertical farming

References

hortus. Charlton T. Lewis and Charles Short. A Latin Dictionary on Perseus Project.
Harper, Douglas. "horticulture". Online Etymology Dictionary.
Entry for yard Dictionary.com (presenting information supposedly from Random House Dictionary)
"Archived copy". Archived from the original on September 10, 2012. Retrieved September 21, 2012.
Fullagar, Richard, Judith Field, Tim Denham, and Carol Lentfer (2006) Early and mid Holocene tool-use and processing of taro (Colocasia esculenta), yam (Dioscorea sp.) and other plants at Kuk Swamp in the highlands of Papua New Guinea Journal of Archaeological Science 33: 595–614
von Hagen, V.W. (1957) The Ancient Sun Kingdoms Of The Americas. Ohio: The World Publishing Company
Solomon, Dawit, Johannes Lehmann, Janice Thies, Thorsten Schafer, Biqing Liang, James Kinyangi, Eduardo Neves, James Petersen, Flavio Luizao, and Jan Skjemstad, Molecular signature and sources of biochemical recalcitrance of organic carbone in Amazonian Dark Earths, Geochemica et cosmochemica ACTA 71.9 2285-2286 (2007) ("Amazonian Dark Earths (ADE) are a unique type of soils apparently developed between 500 and 9000 years B.P. through intense anthropogenic activities such as biomass-burning and high-intensity nutrient depositions on pre-Columbian Amerindian settlements that transformed the original soils into Fimic Anthrosols throughout the Brazilian Amazon Basin.") (internal citations omitted)
Glaser, Bruno, Johannes Lehmann, and Wolfgang Zech, Ameliorating physical and chemical properties of highly weathered soils in the tropics with charcoal – a review, Biology and Fertility of Soils 35.4 219-220 (2002) ("These so called Terra Preta do Indio (Terra Preta) characterize the settlements of pre-Columbian Indios. In Terra Preta soils large amounts of black C indicate a high and prolonged input of carbonized organic matter probably due to the production of charcoal in hearths, whereas only low amounts of charcoal are added to soils as a result of forest fires and slash-and-burn techniques.") (internal citations omitted)
McGee, J.R. and Kruse, M. (1986) Swidden horticulture among the Lacandon Maya [videorecording (29 mins.)]. University of California, Berkeley: Extension Media Center
Thompson, S.I. (1977) Women, Horticulture, and Society in Tropical America. American Anthropologist, N.S., 79: 908–10
Wilson, Simon. "Ancient society of York Florists,oldest horticultural society in world,longest running horticultural show in world established 1768, flower shows in york yorkshire uk,horticultural shows in york yorkshire uk, vegetable shows in york yorkshire uk, fruit shows in york yorkshire uk, floral art shows in york yorkshire uk,handicrafts and baking shows in york uk,dahlia shows in york yorkshire uk,gladioli shows in york yorkshire uk,chrysanthemum shows in york yorkshire uk, auricula shows in york yorkshire uk, sweet pea shows in york yorkshire uk,".
https://www.rhs.org.uk/
IOH
ISHS Archived September 22, 2012, at the Wayback Machine.
"ASHS".
"Australian Society of Horticultural Science - Australian Society of Horticultural Science".
"Home - NJHA".
"RNZIH - Royal New Zealand Institute of Horticulture - Home Page".

    "The Global Horticulture Initiative".

Further reading

    C.R. Adams, Principles of Horticulture Butterworth-Heinemann; 5th edition (11 Aug 2008), ISBN 0-7506-8694-4.
    David Oluwatosin Samuel. I actually don't know I can make a contribution on here. This is great. : )

External links

    The Institute of Horticulture
    ISHS – International Society for Horticultural Science
    The Royal Horticultural Society
    British Library – finding information on the horticulture industry
    History of Horticulture
    HORTIVAR – The FAO Horticulture Cultivars Performance Database
    Global Horticulture Initiative – GlobalHort
    Horticulture Information & Resource Library
    Plant and Soil Sciences eLibrary


Horticulture and gardening
Gardening 	

    History Design
        computer-aided Garden tool Green wall Arboretum Allotment Butterfly Community Forest French intensive Guerrilla Garden Historic conservation Landscape Native Parterre Raised bed Square foot Sustainable Xeriscaping 

	
RegaderaMetalica.jpg
Types of gardens 	

    Back Biblical Botanical Butterfly Byzantine Cactus Cantonese Chinese Color Container Cottage Dutch English Fernery Floating Flower French
        formal landscape Renaissance Front Greek Greenhouse Hanging Hügelkultur Islamic Italian Japanese Kitchen Knot Korean Market Mary Monastic Mughal Orangery Orchard Persian
        Bagh Charbagh Paradise Philosophical Pleasure Roman Spanish Rain Rose Roof Sacred Scottish Sculpture Sensory Shade Therapeutic Trial Tropical Victory Vineyard Walled Water Wildlife Winter Zen Zoological 

Horticulture 	

    Agriculture
        stock-free sustainable urban Arboriculture Botany Companion planting Crop
        most valuable Flora Floriculture Fruticulture Genetically modified tree Hydroculture Indigenous Intercropping Landscape architecture Oenology Olericulture Plant
        breeding propagation drought tolerance hardiness Pomology Postharvest physiology Tropical Urban
        agriculture horticulture forestry reforestation Viticulture 

Organic 	

    Biodynamic agriculture List of organic gardening and farming topics Vegan organic gardening 

Plant protection 	

    Fungicide Herbicide Index of pesticide articles List of fungicides Pesticide Plant disease forecasting Pruning Weed control 

    Agriculture and agronomy portal Gardening portal Commons page Commons 



